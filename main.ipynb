{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt2\n",
    "import pandas as pd\n",
    "from pandas import datetime\n",
    "import math, time\n",
    "import itertools\n",
    "from sklearn import preprocessing\n",
    "import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import load_model\n",
    "import keras\n",
    "import h5py\n",
    "import os\n",
    "from statistics import mean\n",
    "from keras import backend as K\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D\n",
    "from keras.layers.core import Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Initialize Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 22\n",
    "shape = [seq_len, 9, 1]\n",
    "neurons = [256, 256, 32, 1]\n",
    "dropout = 0.3\n",
    "decay = 0.5\n",
    "epochs = 90\n",
    "os.chdir(\"/Users/youssefberrada/Dropbox (MIT)/15.961 Independant Study/Data\")\n",
    "#os.chdir(\"/Users/michelcassard/Dropbox (MIT)/15.960 Independant Study/Data\")\n",
    "file = 'FX-5.xlsx'\n",
    "# Load spreadsheet\n",
    "xl = pd.ExcelFile(file)\n",
    "close = pd.ExcelFile('close.xlsx')\n",
    "df_close=np.array(close.parse(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_data(stock_name, ma=[]):\n",
    "    \"\"\"\n",
    "    Return a dataframe of that stock and normalize all the values. \n",
    "    (Optional: create moving average)\n",
    "    \n",
    "    \"\"\"\n",
    "    df = xl.parse(stock_name)\n",
    "    df.drop(['VOLUME'], 1, inplace=True)\n",
    "    df.set_index('Date', inplace=True)\n",
    "    \n",
    "    # Renaming all the columns so that we can use the old version code\n",
    "    df.rename(columns={'OPEN': 'Open', 'HIGH': 'High', 'LOW': 'Low', 'NUMBER_TICKS': 'Volume', 'LAST_PRICE': 'Adj Close'}, inplace=True)\n",
    "     # Percentage change\n",
    "    df['Pct'] = df['Adj Close'].pct_change()\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    # Moving Average    \n",
    "    if ma != []:\n",
    "        for moving in ma:\n",
    "            df['{}ma'.format(moving)] = df['Adj Close'].rolling(window=moving).mean()\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "  \n",
    "    # Move Adj Close to the rightmost for the ease of training\n",
    "    adj_close = df['Adj Close']\n",
    "    df.drop(labels=['Adj Close'], axis=1, inplace=True)\n",
    "    df = pd.concat([df, adj_close], axis=1)\n",
    "      \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_stock(df):\n",
    "    print(df.head())\n",
    "    plt.subplot(211)\n",
    "    plt.plot(df['Adj Close'], color='red', label='Adj Close')\n",
    "    plt.legend(loc='best')\n",
    "    plt.subplot(212)\n",
    "    plt.plot(df['Pct'], color='blue', label='Percentage change')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Training/Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(stock,normalize,seq_len,split,ma):\n",
    "    amount_of_features = len(stock.columns)\n",
    "    print (\"Amount of features = {}\".format(amount_of_features))\n",
    "    sequence_length = seq_len\n",
    "    result_train = []\n",
    "    result_test= []\n",
    "    row = round(split * stock.shape[0]) \n",
    "    df_train=stock[0:row].copy()\n",
    "    print (\"Amount of training data = {}\".format(df_train.shape[0]))\n",
    "    df_test=stock[row:len(stock)].copy()\n",
    "    print (\"Amount of testing data = {}\".format(df_test.shape[0]))\n",
    "\n",
    "    \n",
    "    if normalize:\n",
    "        #Training\n",
    "        min_max_scaler = preprocessing.MinMaxScaler()\n",
    "        df_train['Open'] = min_max_scaler.fit_transform(df_train.Open.values.reshape(-1,1))\n",
    "        df_train['High'] = min_max_scaler.fit_transform(df_train.High.values.reshape(-1,1))\n",
    "        df_train['Low'] = min_max_scaler.fit_transform(df_train.Low.values.reshape(-1,1))\n",
    "        df_train['Volume'] = min_max_scaler.fit_transform(df_train.Volume.values.reshape(-1,1))\n",
    "        df_train['Adj Close'] = min_max_scaler.fit_transform(df_train['Adj Close'].values.reshape(-1,1))\n",
    "        df_train['Pct'] = min_max_scaler.fit_transform(df_train['Pct'].values.reshape(-1,1))\n",
    "        if ma != []:\n",
    "            for moving in ma:\n",
    "                df_train['{}ma'.format(moving)] = min_max_scaler.fit_transform(df_train['{}ma'.format(moving)].values.reshape(-1,1))  \n",
    "        #Test\n",
    "        df_test['Open'] = min_max_scaler.fit_transform(df_test.Open.values.reshape(-1,1))\n",
    "        df_test['High'] = min_max_scaler.fit_transform(df_test.High.values.reshape(-1,1))\n",
    "        df_test['Low'] = min_max_scaler.fit_transform(df_test.Low.values.reshape(-1,1))\n",
    "        df_test['Volume'] = min_max_scaler.fit_transform(df_test.Volume.values.reshape(-1,1))\n",
    "        df_test['Adj Close'] = min_max_scaler.fit_transform(df_test['Adj Close'].values.reshape(-1,1))\n",
    "        df_test['Pct'] = min_max_scaler.fit_transform(df_test['Pct'].values.reshape(-1,1))\n",
    "        if ma != []:\n",
    "            for moving in ma:\n",
    "                df_test['{}ma'.format(moving)] = min_max_scaler.fit_transform(df_test['{}ma'.format(moving)].values.reshape(-1,1))  \n",
    "     \n",
    "    #Training\n",
    "    data_train = df_train.as_matrix()\n",
    "    for index in range(len(data_train) - sequence_length): \n",
    "        result_train.append(data_train[index: index + sequence_length]) \n",
    "    train = np.array(result_train)\n",
    "    X_train = train[:, :-1].copy() # all data until day m\n",
    "    y_train = train[:, -1][:,-1].copy() # day m + 1 adjusted close price\n",
    "\n",
    "    #Test\n",
    "    data_test = df_test.as_matrix()\n",
    "    for index in range(len(data_test) - sequence_length): \n",
    "        result_test.append(data_test[index: index + sequence_length]) \n",
    "    test = np.array(result_train)\n",
    "    X_test = test[:, :-1].copy()\n",
    "    y_test = test[:, -1][:,-1].copy()\n",
    "\n",
    "    \n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], amount_of_features))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], amount_of_features))\n",
    "\n",
    "    return [X_train, y_train, X_test, y_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(shape, neurons, dropout, decay):\n",
    "    model = Sequential()\n",
    "\n",
    "    #model.add(Dense(neurons[0],activation=\"relu\", input_shape=(shape[0], shape[1])))\n",
    "    \n",
    "    model.add(LSTM(neurons[0], input_shape=(shape[0], shape[1]), return_sequences=True))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(LSTM(neurons[1], input_shape=(shape[0], shape[1]), return_sequences=False))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Dense(neurons[2],kernel_initializer=\"uniform\",activation='relu'))\n",
    "    model.add(Dense(neurons[3],kernel_initializer=\"uniform\",activation='linear'))\n",
    "    \n",
    "    adam = keras.optimizers.Adam(decay=decay)\n",
    "    model.compile(loss='mse',optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model_CNN(shape, neurons, dropout, decay):\n",
    "    model = Sequential()\n",
    "    model.add(Convolution1D(input_shape = (shape[0], shape[1]), \n",
    "                        nb_filter=64,\n",
    "                        filter_length=2,\n",
    "                        border_mode='valid',\n",
    "                        activation='relu',\n",
    "                        subsample_length=1))\n",
    "    model.add(MaxPooling1D(pool_length=2))\n",
    "\n",
    "    model.add(Convolution1D(input_shape = (shape[0], shape[1]), \n",
    "                        nb_filter=64,\n",
    "                        filter_length=2,\n",
    "                        border_mode='valid',\n",
    "                        activation='relu',\n",
    "                        subsample_length=1))\n",
    "    model.add(MaxPooling1D(pool_length=2))\n",
    "\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(250))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('linear'))\n",
    "    adam = keras.optimizers.Adam(decay=decay)\n",
    "    model.compile(loss='mse',optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michelcassard/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(input_shape=(22, 9), activation=\"relu\", filters=64, kernel_size=2, strides=1, padding=\"valid\")`\n",
      "  \n",
      "/Users/michelcassard/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=2)`\n",
      "  if __name__ == '__main__':\n",
      "/Users/michelcassard/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(input_shape=(22, 9), activation=\"relu\", filters=64, kernel_size=2, strides=1, padding=\"valid\")`\n",
      "  app.launch_new_instance()\n",
      "/Users/michelcassard/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=2)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 21, 64)            1216      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 10, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 9, 64)             8256      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 4, 64)             0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 4, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 250)               64250     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 251       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 73,973\n",
      "Trainable params: 73,973\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model_CNN(shape, neurons, dropout, decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9782 samples, validate on 4193 samples\n",
      "Epoch 1/90\n",
      "9782/9782 [==============================] - 2s 157us/step - loss: 0.0300 - acc: 1.0223e-04 - val_loss: 0.0027 - val_acc: 2.3849e-04\n",
      "Epoch 2/90\n",
      "9782/9782 [==============================] - 0s 45us/step - loss: 0.0062 - acc: 1.0223e-04 - val_loss: 8.3929e-04 - val_acc: 2.3849e-04\n",
      "Epoch 3/90\n",
      "9782/9782 [==============================] - 0s 46us/step - loss: 0.0040 - acc: 1.0223e-04 - val_loss: 0.0019 - val_acc: 2.3849e-04\n",
      "Epoch 4/90\n",
      "9782/9782 [==============================] - 1s 53us/step - loss: 0.0031 - acc: 1.0223e-04 - val_loss: 0.0017 - val_acc: 2.3849e-04\n",
      "Epoch 5/90\n",
      "9782/9782 [==============================] - 0s 44us/step - loss: 0.0025 - acc: 1.0223e-04 - val_loss: 0.0021 - val_acc: 2.3849e-04\n",
      "Epoch 6/90\n",
      "9782/9782 [==============================] - 1s 53us/step - loss: 0.0023 - acc: 1.0223e-04 - val_loss: 0.0019 - val_acc: 2.3849e-04\n",
      "Epoch 7/90\n",
      "9782/9782 [==============================] - 1s 57us/step - loss: 0.0019 - acc: 1.0223e-04 - val_loss: 0.0018 - val_acc: 2.3849e-04\n",
      "Epoch 8/90\n",
      "9782/9782 [==============================] - 1s 53us/step - loss: 0.0018 - acc: 1.0223e-04 - val_loss: 0.0011 - val_acc: 2.3849e-04\n",
      "Epoch 9/90\n",
      "9782/9782 [==============================] - 1s 62us/step - loss: 0.0017 - acc: 1.0223e-04 - val_loss: 6.8893e-04 - val_acc: 2.3849e-04\n",
      "Epoch 10/90\n",
      "9782/9782 [==============================] - 0s 48us/step - loss: 0.0016 - acc: 1.0223e-04 - val_loss: 0.0015 - val_acc: 2.3849e-04\n",
      "Epoch 11/90\n",
      "9782/9782 [==============================] - 1s 52us/step - loss: 0.0015 - acc: 1.0223e-04 - val_loss: 0.0015 - val_acc: 2.3849e-04\n",
      "Epoch 12/90\n",
      "9782/9782 [==============================] - 0s 47us/step - loss: 0.0014 - acc: 1.0223e-04 - val_loss: 9.8716e-04 - val_acc: 2.3849e-04\n",
      "Epoch 13/90\n",
      "9782/9782 [==============================] - 0s 48us/step - loss: 0.0014 - acc: 1.0223e-04 - val_loss: 0.0020 - val_acc: 2.3849e-04\n",
      "Epoch 14/90\n",
      "9782/9782 [==============================] - 0s 46us/step - loss: 0.0013 - acc: 1.0223e-04 - val_loss: 0.0014 - val_acc: 2.3849e-04\n",
      "Epoch 15/90\n",
      "9782/9782 [==============================] - 0s 46us/step - loss: 0.0013 - acc: 1.0223e-04 - val_loss: 0.0011 - val_acc: 2.3849e-04\n",
      "Epoch 16/90\n",
      "9782/9782 [==============================] - 0s 49us/step - loss: 0.0013 - acc: 1.0223e-04 - val_loss: 0.0020 - val_acc: 2.3849e-04\n",
      "Epoch 17/90\n",
      "9782/9782 [==============================] - 0s 49us/step - loss: 0.0012 - acc: 1.0223e-04 - val_loss: 9.2508e-04 - val_acc: 2.3849e-04\n",
      "Epoch 18/90\n",
      "9782/9782 [==============================] - 0s 51us/step - loss: 0.0012 - acc: 1.0223e-04 - val_loss: 0.0014 - val_acc: 2.3849e-04\n",
      "Epoch 19/90\n",
      "9782/9782 [==============================] - 1s 53us/step - loss: 0.0012 - acc: 1.0223e-04 - val_loss: 0.0019 - val_acc: 2.3849e-04\n",
      "Epoch 20/90\n",
      "9782/9782 [==============================] - 1s 51us/step - loss: 0.0012 - acc: 1.0223e-04 - val_loss: 0.0015 - val_acc: 2.3849e-04\n",
      "Epoch 21/90\n",
      "9782/9782 [==============================] - 1s 51us/step - loss: 0.0011 - acc: 1.0223e-04 - val_loss: 0.0015 - val_acc: 2.3849e-04\n",
      "Epoch 22/90\n",
      "9782/9782 [==============================] - 1s 53us/step - loss: 0.0011 - acc: 1.0223e-04 - val_loss: 0.0014 - val_acc: 2.3849e-04\n",
      "Epoch 23/90\n",
      "9782/9782 [==============================] - 1s 53us/step - loss: 0.0011 - acc: 1.0223e-04 - val_loss: 0.0014 - val_acc: 2.3849e-04\n",
      "Epoch 24/90\n",
      "9782/9782 [==============================] - 0s 50us/step - loss: 0.0010 - acc: 1.0223e-04 - val_loss: 0.0013 - val_acc: 2.3849e-04\n",
      "Epoch 25/90\n",
      "9782/9782 [==============================] - 1s 53us/step - loss: 0.0010 - acc: 1.0223e-04 - val_loss: 6.0320e-04 - val_acc: 2.3849e-04\n",
      "Epoch 26/90\n",
      "9782/9782 [==============================] - 1s 54us/step - loss: 0.0010 - acc: 1.0223e-04 - val_loss: 0.0018 - val_acc: 2.3849e-04\n",
      "Epoch 27/90\n",
      "9782/9782 [==============================] - 1s 52us/step - loss: 0.0010 - acc: 1.0223e-04 - val_loss: 7.0059e-04 - val_acc: 2.3849e-04\n",
      "Epoch 28/90\n",
      "9782/9782 [==============================] - 0s 46us/step - loss: 0.0011 - acc: 1.0223e-04 - val_loss: 0.0011 - val_acc: 2.3849e-04\n",
      "Epoch 29/90\n",
      "9782/9782 [==============================] - 0s 47us/step - loss: 0.0010 - acc: 1.0223e-04 - val_loss: 6.3347e-04 - val_acc: 2.3849e-04\n",
      "Epoch 30/90\n",
      "9782/9782 [==============================] - 0s 47us/step - loss: 9.6083e-04 - acc: 1.0223e-04 - val_loss: 8.8266e-04 - val_acc: 2.3849e-04\n",
      "Epoch 31/90\n",
      "9782/9782 [==============================] - 0s 50us/step - loss: 9.6896e-04 - acc: 1.0223e-04 - val_loss: 8.0560e-04 - val_acc: 2.3849e-04\n",
      "Epoch 32/90\n",
      "9782/9782 [==============================] - 1s 54us/step - loss: 9.8041e-04 - acc: 1.0223e-04 - val_loss: 7.3745e-04 - val_acc: 2.3849e-04\n",
      "Epoch 33/90\n",
      "9782/9782 [==============================] - 0s 47us/step - loss: 9.4081e-04 - acc: 1.0223e-04 - val_loss: 4.8212e-04 - val_acc: 2.3849e-04\n",
      "Epoch 34/90\n",
      "9782/9782 [==============================] - 0s 48us/step - loss: 9.8479e-04 - acc: 1.0223e-04 - val_loss: 0.0012 - val_acc: 2.3849e-04\n",
      "Epoch 35/90\n",
      "9782/9782 [==============================] - 0s 49us/step - loss: 9.5071e-04 - acc: 1.0223e-04 - val_loss: 6.9842e-04 - val_acc: 2.3849e-04\n",
      "Epoch 36/90\n",
      "9782/9782 [==============================] - 1s 52us/step - loss: 9.1280e-04 - acc: 1.0223e-04 - val_loss: 6.7218e-04 - val_acc: 2.3849e-04\n",
      "Epoch 37/90\n",
      "9782/9782 [==============================] - 0s 45us/step - loss: 9.3024e-04 - acc: 1.0223e-04 - val_loss: 0.0011 - val_acc: 2.3849e-04\n",
      "Epoch 38/90\n",
      "9782/9782 [==============================] - 0s 50us/step - loss: 9.2309e-04 - acc: 1.0223e-04 - val_loss: 8.3252e-04 - val_acc: 2.3849e-04\n",
      "Epoch 39/90\n",
      "9782/9782 [==============================] - 0s 48us/step - loss: 9.2595e-04 - acc: 1.0223e-04 - val_loss: 5.4402e-04 - val_acc: 2.3849e-04\n",
      "Epoch 40/90\n",
      "9782/9782 [==============================] - 1s 55us/step - loss: 8.9355e-04 - acc: 1.0223e-04 - val_loss: 5.4410e-04 - val_acc: 2.3849e-04\n",
      "Epoch 41/90\n",
      "9782/9782 [==============================] - 1s 55us/step - loss: 9.0858e-04 - acc: 1.0223e-04 - val_loss: 0.0011 - val_acc: 2.3849e-04\n",
      "Epoch 42/90\n",
      "9782/9782 [==============================] - 1s 61us/step - loss: 9.5210e-04 - acc: 1.0223e-04 - val_loss: 9.5326e-04 - val_acc: 2.3849e-04\n",
      "Epoch 43/90\n",
      "9782/9782 [==============================] - 1s 68us/step - loss: 8.8722e-04 - acc: 1.0223e-04 - val_loss: 5.1590e-04 - val_acc: 2.3849e-04\n",
      "Epoch 44/90\n",
      "9782/9782 [==============================] - 1s 58us/step - loss: 8.5629e-04 - acc: 1.0223e-04 - val_loss: 0.0013 - val_acc: 2.3849e-04\n",
      "Epoch 45/90\n",
      "9782/9782 [==============================] - 1s 71us/step - loss: 8.7284e-04 - acc: 1.0223e-04 - val_loss: 5.8075e-04 - val_acc: 2.3849e-04\n",
      "Epoch 46/90\n",
      "9782/9782 [==============================] - 1s 80us/step - loss: 8.3667e-04 - acc: 1.0223e-04 - val_loss: 6.8614e-04 - val_acc: 2.3849e-04\n",
      "Epoch 47/90\n",
      "9782/9782 [==============================] - 1s 59us/step - loss: 8.4454e-04 - acc: 1.0223e-04 - val_loss: 8.0890e-04 - val_acc: 2.3849e-04\n",
      "Epoch 48/90\n",
      "9782/9782 [==============================] - 0s 50us/step - loss: 8.3100e-04 - acc: 1.0223e-04 - val_loss: 8.6068e-04 - val_acc: 2.3849e-04\n",
      "Epoch 49/90\n",
      "9782/9782 [==============================] - 0s 47us/step - loss: 8.3631e-04 - acc: 1.0223e-04 - val_loss: 4.6337e-04 - val_acc: 2.3849e-04\n",
      "Epoch 50/90\n",
      "9782/9782 [==============================] - 0s 51us/step - loss: 8.6489e-04 - acc: 1.0223e-04 - val_loss: 5.4392e-04 - val_acc: 2.3849e-04\n",
      "Epoch 51/90\n",
      "9782/9782 [==============================] - 1s 58us/step - loss: 8.2384e-04 - acc: 1.0223e-04 - val_loss: 5.1553e-04 - val_acc: 2.3849e-04\n",
      "Epoch 52/90\n",
      "9782/9782 [==============================] - 1s 66us/step - loss: 8.1945e-04 - acc: 1.0223e-04 - val_loss: 4.5196e-04 - val_acc: 2.3849e-04\n",
      "Epoch 53/90\n",
      "9782/9782 [==============================] - 1s 61us/step - loss: 8.3795e-04 - acc: 1.0223e-04 - val_loss: 5.3157e-04 - val_acc: 2.3849e-04\n",
      "Epoch 54/90\n",
      "9782/9782 [==============================] - 1s 56us/step - loss: 7.8482e-04 - acc: 1.0223e-04 - val_loss: 6.6991e-04 - val_acc: 2.3849e-04\n",
      "Epoch 55/90\n",
      "9782/9782 [==============================] - 0s 49us/step - loss: 8.0100e-04 - acc: 1.0223e-04 - val_loss: 5.0156e-04 - val_acc: 2.3849e-04\n",
      "Epoch 56/90\n",
      "9782/9782 [==============================] - 0s 50us/step - loss: 7.9817e-04 - acc: 1.0223e-04 - val_loss: 4.1211e-04 - val_acc: 2.3849e-04\n",
      "Epoch 57/90\n",
      "9782/9782 [==============================] - 0s 46us/step - loss: 8.6557e-04 - acc: 1.0223e-04 - val_loss: 4.0639e-04 - val_acc: 2.3849e-04\n",
      "Epoch 58/90\n",
      "9782/9782 [==============================] - 0s 45us/step - loss: 8.4588e-04 - acc: 1.0223e-04 - val_loss: 8.6013e-04 - val_acc: 2.3849e-04\n",
      "Epoch 59/90\n",
      "9782/9782 [==============================] - 0s 45us/step - loss: 8.4204e-04 - acc: 1.0223e-04 - val_loss: 7.0024e-04 - val_acc: 2.3849e-04\n",
      "Epoch 60/90\n",
      "9782/9782 [==============================] - 1s 57us/step - loss: 7.9460e-04 - acc: 1.0223e-04 - val_loss: 0.0012 - val_acc: 2.3849e-04\n",
      "Epoch 61/90\n",
      "9782/9782 [==============================] - 1s 54us/step - loss: 8.2483e-04 - acc: 1.0223e-04 - val_loss: 7.1989e-04 - val_acc: 2.3849e-04\n",
      "Epoch 62/90\n",
      "9782/9782 [==============================] - 1s 52us/step - loss: 9.9973e-04 - acc: 1.0223e-04 - val_loss: 7.3191e-04 - val_acc: 2.3849e-04\n",
      "Epoch 63/90\n",
      "9782/9782 [==============================] - 0s 48us/step - loss: 8.6027e-04 - acc: 1.0223e-04 - val_loss: 5.9269e-04 - val_acc: 2.3849e-04\n",
      "Epoch 64/90\n",
      "9782/9782 [==============================] - 0s 49us/step - loss: 7.6959e-04 - acc: 1.0223e-04 - val_loss: 7.2461e-04 - val_acc: 2.3849e-04\n",
      "Epoch 65/90\n",
      "9782/9782 [==============================] - 0s 46us/step - loss: 7.8893e-04 - acc: 1.0223e-04 - val_loss: 7.7115e-04 - val_acc: 2.3849e-04\n",
      "Epoch 66/90\n",
      "9782/9782 [==============================] - 0s 44us/step - loss: 7.8180e-04 - acc: 1.0223e-04 - val_loss: 0.0011 - val_acc: 2.3849e-04\n",
      "Epoch 67/90\n",
      "9782/9782 [==============================] - 0s 43us/step - loss: 7.9688e-04 - acc: 1.0223e-04 - val_loss: 7.5313e-04 - val_acc: 2.3849e-04\n",
      "Epoch 68/90\n",
      "9782/9782 [==============================] - 0s 46us/step - loss: 7.4693e-04 - acc: 1.0223e-04 - val_loss: 8.0424e-04 - val_acc: 2.3849e-04\n",
      "Epoch 69/90\n",
      "9782/9782 [==============================] - 0s 43us/step - loss: 7.4213e-04 - acc: 1.0223e-04 - val_loss: 7.6073e-04 - val_acc: 2.3849e-04\n",
      "Epoch 70/90\n",
      "9782/9782 [==============================] - 0s 45us/step - loss: 7.4883e-04 - acc: 1.0223e-04 - val_loss: 8.7098e-04 - val_acc: 2.3849e-04\n",
      "Epoch 71/90\n",
      "9782/9782 [==============================] - 0s 45us/step - loss: 7.5040e-04 - acc: 1.0223e-04 - val_loss: 4.4049e-04 - val_acc: 2.3849e-04\n",
      "Epoch 72/90\n",
      "9782/9782 [==============================] - 0s 48us/step - loss: 7.6684e-04 - acc: 1.0223e-04 - val_loss: 7.5038e-04 - val_acc: 2.3849e-04\n",
      "Epoch 73/90\n",
      "9782/9782 [==============================] - 0s 48us/step - loss: 7.5701e-04 - acc: 1.0223e-04 - val_loss: 8.0990e-04 - val_acc: 2.3849e-04\n",
      "Epoch 74/90\n",
      "9782/9782 [==============================] - 0s 48us/step - loss: 7.6470e-04 - acc: 1.0223e-04 - val_loss: 0.0013 - val_acc: 2.3849e-04\n",
      "Epoch 75/90\n",
      "9782/9782 [==============================] - 0s 48us/step - loss: 8.0393e-04 - acc: 1.0223e-04 - val_loss: 5.5986e-04 - val_acc: 2.3849e-04\n",
      "Epoch 76/90\n",
      "9782/9782 [==============================] - 0s 46us/step - loss: 7.6198e-04 - acc: 1.0223e-04 - val_loss: 5.0854e-04 - val_acc: 2.3849e-04\n",
      "Epoch 77/90\n",
      "9782/9782 [==============================] - 0s 45us/step - loss: 7.7522e-04 - acc: 1.0223e-04 - val_loss: 0.0020 - val_acc: 2.3849e-04\n",
      "Epoch 78/90\n",
      "9782/9782 [==============================] - 0s 46us/step - loss: 7.9246e-04 - acc: 1.0223e-04 - val_loss: 5.7807e-04 - val_acc: 2.3849e-04\n",
      "Epoch 79/90\n",
      "9782/9782 [==============================] - 0s 45us/step - loss: 7.0724e-04 - acc: 1.0223e-04 - val_loss: 4.9251e-04 - val_acc: 2.3849e-04\n",
      "Epoch 80/90\n",
      "9782/9782 [==============================] - 0s 50us/step - loss: 7.0103e-04 - acc: 1.0223e-04 - val_loss: 5.0446e-04 - val_acc: 2.3849e-04\n",
      "Epoch 81/90\n",
      "9782/9782 [==============================] - 0s 49us/step - loss: 7.1523e-04 - acc: 1.0223e-04 - val_loss: 3.9632e-04 - val_acc: 2.3849e-04\n",
      "Epoch 82/90\n",
      "9782/9782 [==============================] - 1s 54us/step - loss: 6.8915e-04 - acc: 1.0223e-04 - val_loss: 6.0674e-04 - val_acc: 2.3849e-04\n",
      "Epoch 83/90\n",
      "9782/9782 [==============================] - 1s 58us/step - loss: 6.9093e-04 - acc: 1.0223e-04 - val_loss: 7.3123e-04 - val_acc: 2.3849e-04\n",
      "Epoch 84/90\n",
      "9782/9782 [==============================] - 1s 64us/step - loss: 6.9002e-04 - acc: 1.0223e-04 - val_loss: 7.3097e-04 - val_acc: 2.3849e-04\n",
      "Epoch 85/90\n",
      "9782/9782 [==============================] - 1s 61us/step - loss: 6.9586e-04 - acc: 1.0223e-04 - val_loss: 4.0390e-04 - val_acc: 2.3849e-04\n",
      "Epoch 86/90\n",
      "9782/9782 [==============================] - 1s 59us/step - loss: 7.0406e-04 - acc: 1.0223e-04 - val_loss: 0.0011 - val_acc: 2.3849e-04\n",
      "Epoch 87/90\n",
      "9782/9782 [==============================] - 1s 63us/step - loss: 7.0171e-04 - acc: 1.0223e-04 - val_loss: 0.0010 - val_acc: 2.3849e-04\n",
      "Epoch 88/90\n",
      "9782/9782 [==============================] - 1s 55us/step - loss: 6.9965e-04 - acc: 1.0223e-04 - val_loss: 4.2269e-04 - val_acc: 2.3849e-04\n",
      "Epoch 89/90\n",
      "9782/9782 [==============================] - 1s 62us/step - loss: 7.1589e-04 - acc: 1.0223e-04 - val_loss: 5.9269e-04 - val_acc: 2.3849e-04\n",
      "Epoch 90/90\n",
      "9782/9782 [==============================] - 1s 54us/step - loss: 6.8733e-04 - acc: 1.0223e-04 - val_loss: 4.2963e-04 - val_acc: 2.3849e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a1ec1fcc0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train,batch_size=512,epochs=epochs,validation_split=0.3,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_score(model, X_train, y_train, X_test, y_test):\n",
    "    trainScore = model.evaluate(X_train, y_train, verbose=0)\n",
    "    print('Train Score: %.5f MSE (%.2f RMSE)' % (trainScore[0], math.sqrt(trainScore[0])))\n",
    "\n",
    "    testScore = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print('Test Score: %.5f MSE (%.2f RMSE)' % (testScore[0], math.sqrt(testScore[0])))\n",
    "    return trainScore[0], testScore[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.00037 MSE (0.02 RMSE)\n",
      "Test Score: 0.00037 MSE (0.02 RMSE)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0003710565636182714, 0.0003710565636182714)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_score(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def percentage_difference(model, X_test, y_test):\n",
    "    percentage_diff=[]\n",
    "\n",
    "    p = model.predict(X_test)\n",
    "    for u in range(len(y_test)): # for each data index in test data\n",
    "        pr = p[u][0] # pr = prediction on day u\n",
    "\n",
    "        percentage_diff.append((pr-y_test[u]/pr)*100)\n",
    "    print(mean(percentage_diff))\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-57.38379361472257\n"
     ]
    }
   ],
   "source": [
    "p = percentage_difference(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_result_norm(stock_name, normalized_value_p, normalized_value_y_test):\n",
    "    newp=normalized_value_p\n",
    "    newy_test=normalized_value_y_test\n",
    "    plt2.plot(newp, color='red', label='Prediction')\n",
    "    plt2.plot(newy_test,color='blue', label='Actual')\n",
    "    plt2.legend(loc='best')\n",
    "    plt2.title('The test result for {}'.format(stock_name))\n",
    "    plt2.xlabel('5 Min ahead Forecast')\n",
    "    plt2.ylabel('Price')\n",
    "    plt2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def denormalize(stock_name, normalized_value,split=0.7,predict=True):\n",
    "    \"\"\"\n",
    "    Return a dataframe of that stock and normalize all the values. \n",
    "    (Optional: create moving average)\n",
    "    \"\"\"\n",
    "    df = xl.parse(stock_name)\n",
    "    df.drop(['VOLUME'], 1, inplace=True)\n",
    "    df.set_index('Date', inplace=True)\n",
    "    \n",
    "    # Renaming all the columns so that we can use the old version code\n",
    "    df.rename(columns={'OPEN': 'Open', 'HIGH': 'High', 'LOW': 'Low', 'NUMBER_TICKS': 'Volume', 'LAST_PRICE': 'Adj Close'}, inplace=True)\n",
    "\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "    df = df['Adj Close'].values.reshape(-1,1)\n",
    "    normalized_value = normalized_value.reshape(-1,1)\n",
    "    \n",
    "    row = round(split * df.shape[0]) \n",
    "    if predict:\n",
    "        df_p=df[0:row].copy()\n",
    "    else:\n",
    "        df_p=df[row:len(df)].copy()\n",
    "    \n",
    "    #return df.shape, p.shape\n",
    "    mean_df=np.mean(df_p)\n",
    "    std_df=np.std(df_p)\n",
    "    new=normalized_value*mean_df+std_df\n",
    "      \n",
    "    return new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Portfolio construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def portfolio(currency_list,file = 'FX-5.xlsx',seq_len = 22,shape = [seq_len, 9, 1],neurons = [256, 256, 32, 1],dropout = 0.3,decay = 0.5,\n",
    "              epochs = 90,ma=[50, 100, 200],split=0.7):\n",
    "    i=0\n",
    "    mini=99999999\n",
    "    for currency in currency_list:\n",
    "        df=get_stock_data(currency,  ma)\n",
    "        X_train, y_train, X_test, y_test = load_data(df,True,seq_len,split,ma)\n",
    "        model = build_model_CNN(shape, neurons, dropout, decay)\n",
    "        model.fit(X_train,y_train,batch_size=512,epochs=epochs,validation_split=0.3,verbose=1)\n",
    "        p = percentage_difference(model, X_test, y_test)\n",
    "        newp = denormalize(currency, p,predict=True)\n",
    "        if mini>p.size:\n",
    "            mini=p.size\n",
    "        if i==0:\n",
    "            predict=newp.copy()\n",
    "        else:\n",
    "            predict=np.hstack((predict[0:mini],newp[0:mini]))\n",
    "        i+=1\n",
    "    return predict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "currency_list=[ 'GBP Curncy',\n",
    " 'JPY Curncy',\n",
    " 'EUR Curncy',\n",
    " 'CAD Curncy',\n",
    " 'NZD Curncy',\n",
    " 'SEK Curncy',\n",
    " 'AUD Curncy',\n",
    " 'CHF Curncy',\n",
    " 'NOK Curncy',\n",
    " 'ZAR Curncy']\n",
    "#currency_list=['JPY Curncy']\n",
    "predictcur=portfolio(currency_list,file = 'FX-5.xlsx',seq_len = 22,shape = [seq_len, 9, 1],neurons = [256, 256, 32, 1],dropout = 0.3,decay = 0.5,\n",
    "              epochs = 1,ma=[50, 100, 200],split=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebalance(n,previous_prices,x0,w,mu,gamma=1):\n",
    "    GT=np.cov(previous_prices)\n",
    "    f = n*[0]\n",
    "    g = n*[0.0005]\n",
    "    _,weights=MarkowitzWithTransactionsCost(n,mu,GT,x0,w,gamma,f,g)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_diff(data):\n",
    "    return np.diff(np.log(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtest(prices, predictions, initial_weights):\n",
    "    t_prices = len(prices[1,:])\n",
    "    t_predictions = len(predictions[:,1])\n",
    "    length_past = t_prices - t_predictions\n",
    "    returns = np.apply_along_axis(log_diff, 1, prices)\n",
    "    prediction_return = []\n",
    "    for k in range(t_predictions):\n",
    "        prediction_return.append(np.log(predictions[k]/prices[:,length_past+k]))\n",
    "    weights = initial_weights\n",
    "    portfolio_return = []\n",
    "    prev_weight = weights\n",
    "    for i in range(0,t_predictions-1)):\n",
    "        print(i)\n",
    "        predicted_return = prediction_return[i]\n",
    "        previous_return = returns[:,length_past+i]\n",
    "        previous_returns = returns[:,0:length_past+i]\n",
    "        new_weight = rebalance(10,previous_returns,mu=predicted_return.tolist(),x0=prev_weight,w=1,gamma=0.5)\n",
    "        period_return = new_weight*np.log(prices[:,length_past+i+1]/prices[:,length_past+i])\n",
    "        portfolio_return.append(np.sum(period_return))\n",
    "        prev_weight = new_weight\n",
    "        print(new_weight)\n",
    "    return portfolio_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=backtest(dq.T, predictcur, np.repeat(1/10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_result(stock_name, normalized_value_p, normalized_value_y_test):\n",
    "    newp = denormalize(stock_name, normalized_value_p,predict=True)\n",
    "    newy_test = denormalize(stock_name, normalized_value_y_test,predict=False)\n",
    "    plt2.plot(newp, color='red', label='Prediction')\n",
    "    plt2.plot(newy_test,color='blue', label='Actual')\n",
    "    plt2.legend(loc='best')\n",
    "    plt2.title('The test result for {}'.format(stock_name))\n",
    "    plt2.xlabel('5 Min ahead Forecast')\n",
    "    plt2.ylabel('Price')\n",
    "    plt2.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
