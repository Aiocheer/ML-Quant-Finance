{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt2\n",
    "import pandas as pd\n",
    "from pandas import datetime\n",
    "import math, time\n",
    "import itertools\n",
    "from sklearn import preprocessing\n",
    "import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import load_model\n",
    "from keras.utils import plot_model\n",
    "from keras import backend as K\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D\n",
    "from keras.layers.core import Flatten\n",
    "import keras\n",
    "import h5py\n",
    "import os\n",
    "import graphviz\n",
    "import pydot\n",
    "from statistics import mean\n",
    "import matplotlib.pyplot as plt\n",
    "from mosek.fusion import *\n",
    "from keras.utils.vis_utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Initialize Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 22\n",
    "shape = [seq_len, 9, 1]\n",
    "neurons = [256, 256, 32, 1]\n",
    "dropout = 0.3\n",
    "decay = 0.5\n",
    "epochs = 90\n",
    "os.chdir(\"/Users/youssefberrada/Dropbox (MIT)/15.961 Independant Study/Data\")\n",
    "#os.chdir(\"/Users/michelcassard/Dropbox (MIT)/15.960 Independant Study/Data\")\n",
    "file = 'FX-5.xlsx'\n",
    "# Load spreadsheet\n",
    "xl = pd.ExcelFile(file)\n",
    "close = pd.ExcelFile('close.xlsx')\n",
    "df_close=np.array(close.parse(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_data(stock_name, ma=[],bollinger=[],exp_ma=[],ma_conv=[]):\n",
    "    \"\"\"\n",
    "    Return a dataframe of that stock and normalize all the values. \n",
    "    (Optional: create moving average)\n",
    "    \n",
    "    \"\"\"\n",
    "    df = xl.parse(stock_name)\n",
    "    df.drop(['VOLUME'], 1, inplace=True)\n",
    "    df.set_index('Date', inplace=True)\n",
    "    \n",
    "    # Renaming all the columns so that we can use the old version code\n",
    "    df.rename(columns={'OPEN': 'Open', 'HIGH': 'High', 'LOW': 'Low', 'NUMBER_TICKS': 'Volume', 'LAST_PRICE': 'Adj Close'}, inplace=True)\n",
    "     # Percentage change\n",
    "    df['Pct'] = df['Adj Close'].pct_change()\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    # Moving Average    \n",
    "    if ma != []:\n",
    "        for moving in ma:\n",
    "            df['{}ma'.format(moving)] = df['Adj Close'].rolling(window=moving).mean()\n",
    "    # Bollinger   \n",
    "    if bollinger != []:\n",
    "        for moving in bollinger:\n",
    "            df['{}bollinger'.format(moving)] = df['Adj Close'].rolling(window=moving, center=False).mean()\n",
    "        \n",
    "    # Exponential Moving Average    \n",
    "    if exp_ma != []:\n",
    "        for moving in exp_ma:\n",
    "            df['{}exp_ma'.format(moving)] = df['Adj Close'].ewm(min_periods=moving, adjust=True, span=span, ignore_na=ignore_na).mean()\n",
    "   \n",
    "    # Moving Average Convergence   \n",
    "    if ma_conv!= []:\n",
    "        for moving in ma_conv:\n",
    "            df['{}ma_conv'.format(moving)] = df['Adj Close'].ewm(min_periods=moving[1], adjust=True, span=span, ignore_na=ignore_na).mean()-df['Adj Close'].ewm(min_periods=moving[2], adjust=True, span=span, ignore_na=ignore_na).mean()\n",
    "    \n",
    "    \n",
    "    \n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "  \n",
    "    # Move Adj Close to the rightmost for the ease of training\n",
    "    adj_close = df['Adj Close']\n",
    "    df.drop(labels=['Adj Close'], axis=1, inplace=True)\n",
    "    df = pd.concat([df, adj_close], axis=1)\n",
    "      \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stock(df):\n",
    "    print(df.head())\n",
    "    plt.subplot(211)\n",
    "    plt.plot(df['Adj Close'], color='red', label='Adj Close')\n",
    "    plt.legend(loc='best')\n",
    "    plt.subplot(212)\n",
    "    plt.plot(df['Pct'], color='blue', label='Percentage change')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Training/Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(stock,normalize,seq_len,split,ma=[],bollinger=[],exp_ma=[],ma_conv=[]):\n",
    "    amount_of_features = len(stock.columns)\n",
    "    print (\"Amount of features = {}\".format(amount_of_features))\n",
    "    sequence_length = seq_len+1\n",
    "    result_train = []\n",
    "    result_test= []\n",
    "    row = round(split * stock.shape[0])\n",
    "    df_train=stock[0:row].copy()\n",
    "    print (\"Amount of training data = {}\".format(df_train.shape[0]))\n",
    "    df_test=stock[row:len(stock)].copy()\n",
    "    print (\"Amount of testing data = {}\".format(df_test.shape[0]))\n",
    "\n",
    "\n",
    "    if normalize:\n",
    "        #Training\n",
    "        min_max_scaler = preprocessing.MinMaxScaler()\n",
    "        min_max_scaler.fit(df_train['Adj Close'].values.reshape(-1,1))\n",
    "        df_train['Open'] = min_max_scaler.transform(df_train['Open'].values.reshape(-1,1))\n",
    "        df_train['High'] = min_max_scaler.transform(df_train['High'].values.reshape(-1,1))\n",
    "        df_train['Low'] = min_max_scaler.transform(df_train['Low'].values.reshape(-1,1))\n",
    "        df_train['Adj Close'] = min_max_scaler.transform(df_train['Adj Close'].values.reshape(-1,1))\n",
    "        df_train['Volume'] = min_max_scaler.fit_transform(df_train.Volume.values.reshape(-1,1))\n",
    "        df_train['Pct'] = min_max_scaler.fit_transform(df_train['Pct'].values.reshape(-1,1))\n",
    "        if ma != []:\n",
    "            for moving in ma:\n",
    "                df_train['{}ma'.format(moving)] = min_max_scaler.fit_transform(df_train['{}ma'.format(moving)].values.reshape(-1,1))\n",
    "        if bollinger != []:\n",
    "            for moving in bollinger:\n",
    "                df_train['{}bollinger'.format(moving)] = min_max_scaler.fit_transform(df_train['{}bollinger'.format(moving)].values.reshape(-1,1))\n",
    "        if exp_ma != []:\n",
    "            for moving in exp_ma:\n",
    "                df_train['{}exp_ma'.format(moving)] = min_max_scaler.fit_transform(df_train['{}exp_ma'.format(moving)].values.reshape(-1,1))\n",
    "        if ma_conv!= []:\n",
    "            for moving in ma_conv:\n",
    "                df_train['{}ma_conv'.format(moving)] = min_max_scaler.fit_transform(df_train['{}ma_conv'.format(moving)].values.reshape(-1,1))\n",
    "                \n",
    "        #Test\n",
    "        min_max_scaler.fit(df_test['Adj Close'].values.reshape(-1,1))\n",
    "        df_test['Open'] = min_max_scaler.transform(df_test['Open'].values.reshape(-1,1))\n",
    "        df_test['High'] = min_max_scaler.transform(df_test['High'].values.reshape(-1,1))\n",
    "        df_test['Low'] = min_max_scaler.transform(df_test['Low'].values.reshape(-1,1))\n",
    "        df_test['Adj Close'] = min_max_scaler.transform(df_test['Adj Close'].values.reshape(-1,1))\n",
    "        df_test['Volume'] = min_max_scaler.fit_transform(df_test.Volume.values.reshape(-1,1))\n",
    "        df_test['Pct'] = min_max_scaler.fit_transform(df_test['Pct'].values.reshape(-1,1))\n",
    "        if ma != []:\n",
    "            for moving in ma:\n",
    "                df_test['{}ma'.format(moving)] = min_max_scaler.fit_transform(df_test['{}ma'.format(moving)].values.reshape(-1,1))\n",
    "        if bollinger != []:\n",
    "            for moving in bollinger:\n",
    "                df_test['{}bollinger'.format(moving)] = min_max_scaler.fit_transform(df_test['{}bollinger'.format(moving)].values.reshape(-1,1))\n",
    "        if exp_ma != []:\n",
    "            for moving in exp_ma:\n",
    "                df_test['{}exp_ma'.format(moving)] = min_max_scaler.fit_transform(df_test['{}exp_ma'.format(moving)].values.reshape(-1,1))\n",
    "        if ma_conv!= []:\n",
    "            for moving in ma_conv:\n",
    "                df_test['{}ma_conv'.format(moving)] = min_max_scaler.fit_transform(df_test['{}ma_conv'.format(moving)].values.reshape(-1,1))\n",
    "                \n",
    "    #Training\n",
    "    data_train = df_train.as_matrix()\n",
    "    for index in range(len(data_train) - sequence_length):\n",
    "        result_train.append(data_train[index: index + sequence_length])\n",
    "    train = np.array(result_train)\n",
    "    X_train = train[:, :-1].copy() # all data until day m\n",
    "    y_train = train[:, -1][:,-1].copy() # day m + 1 adjusted close price\n",
    "\n",
    "    #Test\n",
    "    data_test = df_test.as_matrix()\n",
    "    for index in range(len(data_test) - sequence_length):\n",
    "        result_test.append(data_test[index: index + sequence_length])\n",
    "    test = np.array(result_train)\n",
    "    X_test = test[:, :-1].copy()\n",
    "    y_test = test[:, -1][:,-1].copy()\n",
    "\n",
    "\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], amount_of_features))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], amount_of_features))\n",
    "\n",
    "    return [X_train, y_train, X_test, y_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(shape, neurons, dropout, decay):\n",
    "    model = Sequential()\n",
    "\n",
    "    #model.add(Dense(neurons[0],activation=\"relu\", input_shape=(shape[0], shape[1])))\n",
    "    \n",
    "    model.add(LSTM(neurons[0], input_shape=(shape[0], shape[1]), return_sequences=True))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(LSTM(neurons[1], input_shape=(shape[0], shape[1]), return_sequences=False))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Dense(neurons[2],kernel_initializer=\"uniform\",activation='relu'))\n",
    "    model.add(Dense(neurons[3],kernel_initializer=\"uniform\",activation='linear'))\n",
    "    \n",
    "    adam = keras.optimizers.Adam(decay=decay)\n",
    "    model.compile(loss='mse',optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_CNN(shape, neurons, dropout, decay):\n",
    "    model = Sequential()\n",
    "    model.add(Convolution1D(input_shape = (shape[0], shape[1]), \n",
    "                        nb_filter=64,\n",
    "                        filter_length=2,\n",
    "                        border_mode='valid',\n",
    "                        activation='relu',\n",
    "                        subsample_length=1))\n",
    "    model.add(MaxPooling1D(pool_length=2))\n",
    "\n",
    "    model.add(Convolution1D(input_shape = (shape[0], shape[1]), \n",
    "                        nb_filter=64,\n",
    "                        filter_length=2,\n",
    "                        border_mode='valid',\n",
    "                        activation='relu',\n",
    "                        subsample_length=1))\n",
    "    model.add(MaxPooling1D(pool_length=2))\n",
    "\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(250))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('linear'))\n",
    "    adam = keras.optimizers.Adam(decay=decay)\n",
    "    model.compile(loss='mse',optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(input_shape=(22, 9), activation=\"relu\", filters=64, kernel_size=2, strides=1, padding=\"valid\")`\n",
      "  \n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=2)`\n",
      "  if __name__ == '__main__':\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(input_shape=(22, 9), activation=\"relu\", filters=64, kernel_size=2, strides=1, padding=\"valid\")`\n",
      "  app.launch_new_instance()\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=2)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 21, 64)            1216      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 10, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 9, 64)             8256      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 4, 64)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 250)               64250     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 251       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 73,973\n",
      "Trainable params: 73,973\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model_CNN(shape, neurons, dropout, decay)\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9782 samples, validate on 4193 samples\n",
      "Epoch 1/90\n",
      "9782/9782 [==============================] - 2s 157us/step - loss: 0.0300 - acc: 1.0223e-04 - val_loss: 0.0027 - val_acc: 2.3849e-04\n",
      "Epoch 2/90\n",
      "9782/9782 [==============================] - 0s 45us/step - loss: 0.0062 - acc: 1.0223e-04 - val_loss: 8.3929e-04 - val_acc: 2.3849e-04\n",
      "Epoch 3/90\n",
      "9782/9782 [==============================] - 0s 46us/step - loss: 0.0040 - acc: 1.0223e-04 - val_loss: 0.0019 - val_acc: 2.3849e-04\n",
      "Epoch 4/90\n",
      "9782/9782 [==============================] - 1s 53us/step - loss: 0.0031 - acc: 1.0223e-04 - val_loss: 0.0017 - val_acc: 2.3849e-04\n",
      "Epoch 5/90\n",
      "9782/9782 [==============================] - 0s 44us/step - loss: 0.0025 - acc: 1.0223e-04 - val_loss: 0.0021 - val_acc: 2.3849e-04\n",
      "Epoch 6/90\n",
      "9782/9782 [==============================] - 1s 53us/step - loss: 0.0023 - acc: 1.0223e-04 - val_loss: 0.0019 - val_acc: 2.3849e-04\n",
      "Epoch 7/90\n",
      "9782/9782 [==============================] - 1s 57us/step - loss: 0.0019 - acc: 1.0223e-04 - val_loss: 0.0018 - val_acc: 2.3849e-04\n",
      "Epoch 8/90\n",
      "9782/9782 [==============================] - 1s 53us/step - loss: 0.0018 - acc: 1.0223e-04 - val_loss: 0.0011 - val_acc: 2.3849e-04\n",
      "Epoch 9/90\n",
      "9782/9782 [==============================] - 1s 62us/step - loss: 0.0017 - acc: 1.0223e-04 - val_loss: 6.8893e-04 - val_acc: 2.3849e-04\n",
      "Epoch 10/90\n",
      "9782/9782 [==============================] - 0s 48us/step - loss: 0.0016 - acc: 1.0223e-04 - val_loss: 0.0015 - val_acc: 2.3849e-04\n",
      "Epoch 11/90\n",
      "9782/9782 [==============================] - 1s 52us/step - loss: 0.0015 - acc: 1.0223e-04 - val_loss: 0.0015 - val_acc: 2.3849e-04\n",
      "Epoch 12/90\n",
      "9782/9782 [==============================] - 0s 47us/step - loss: 0.0014 - acc: 1.0223e-04 - val_loss: 9.8716e-04 - val_acc: 2.3849e-04\n",
      "Epoch 13/90\n",
      "9782/9782 [==============================] - 0s 48us/step - loss: 0.0014 - acc: 1.0223e-04 - val_loss: 0.0020 - val_acc: 2.3849e-04\n",
      "Epoch 14/90\n",
      "9782/9782 [==============================] - 0s 46us/step - loss: 0.0013 - acc: 1.0223e-04 - val_loss: 0.0014 - val_acc: 2.3849e-04\n",
      "Epoch 15/90\n",
      "9782/9782 [==============================] - 0s 46us/step - loss: 0.0013 - acc: 1.0223e-04 - val_loss: 0.0011 - val_acc: 2.3849e-04\n",
      "Epoch 16/90\n",
      "9782/9782 [==============================] - 0s 49us/step - loss: 0.0013 - acc: 1.0223e-04 - val_loss: 0.0020 - val_acc: 2.3849e-04\n",
      "Epoch 17/90\n",
      "9782/9782 [==============================] - 0s 49us/step - loss: 0.0012 - acc: 1.0223e-04 - val_loss: 9.2508e-04 - val_acc: 2.3849e-04\n",
      "Epoch 18/90\n",
      "9782/9782 [==============================] - 0s 51us/step - loss: 0.0012 - acc: 1.0223e-04 - val_loss: 0.0014 - val_acc: 2.3849e-04\n",
      "Epoch 19/90\n",
      "9782/9782 [==============================] - 1s 53us/step - loss: 0.0012 - acc: 1.0223e-04 - val_loss: 0.0019 - val_acc: 2.3849e-04\n",
      "Epoch 20/90\n",
      "9782/9782 [==============================] - 1s 51us/step - loss: 0.0012 - acc: 1.0223e-04 - val_loss: 0.0015 - val_acc: 2.3849e-04\n",
      "Epoch 21/90\n",
      "9782/9782 [==============================] - 1s 51us/step - loss: 0.0011 - acc: 1.0223e-04 - val_loss: 0.0015 - val_acc: 2.3849e-04\n",
      "Epoch 22/90\n",
      "9782/9782 [==============================] - 1s 53us/step - loss: 0.0011 - acc: 1.0223e-04 - val_loss: 0.0014 - val_acc: 2.3849e-04\n",
      "Epoch 23/90\n",
      "9782/9782 [==============================] - 1s 53us/step - loss: 0.0011 - acc: 1.0223e-04 - val_loss: 0.0014 - val_acc: 2.3849e-04\n",
      "Epoch 24/90\n",
      "9782/9782 [==============================] - 0s 50us/step - loss: 0.0010 - acc: 1.0223e-04 - val_loss: 0.0013 - val_acc: 2.3849e-04\n",
      "Epoch 25/90\n",
      "9782/9782 [==============================] - 1s 53us/step - loss: 0.0010 - acc: 1.0223e-04 - val_loss: 6.0320e-04 - val_acc: 2.3849e-04\n",
      "Epoch 26/90\n",
      "9782/9782 [==============================] - 1s 54us/step - loss: 0.0010 - acc: 1.0223e-04 - val_loss: 0.0018 - val_acc: 2.3849e-04\n",
      "Epoch 27/90\n",
      "9782/9782 [==============================] - 1s 52us/step - loss: 0.0010 - acc: 1.0223e-04 - val_loss: 7.0059e-04 - val_acc: 2.3849e-04\n",
      "Epoch 28/90\n",
      "9782/9782 [==============================] - 0s 46us/step - loss: 0.0011 - acc: 1.0223e-04 - val_loss: 0.0011 - val_acc: 2.3849e-04\n",
      "Epoch 29/90\n",
      "9782/9782 [==============================] - 0s 47us/step - loss: 0.0010 - acc: 1.0223e-04 - val_loss: 6.3347e-04 - val_acc: 2.3849e-04\n",
      "Epoch 30/90\n",
      "9782/9782 [==============================] - 0s 47us/step - loss: 9.6083e-04 - acc: 1.0223e-04 - val_loss: 8.8266e-04 - val_acc: 2.3849e-04\n",
      "Epoch 31/90\n",
      "9782/9782 [==============================] - 0s 50us/step - loss: 9.6896e-04 - acc: 1.0223e-04 - val_loss: 8.0560e-04 - val_acc: 2.3849e-04\n",
      "Epoch 32/90\n",
      "9782/9782 [==============================] - 1s 54us/step - loss: 9.8041e-04 - acc: 1.0223e-04 - val_loss: 7.3745e-04 - val_acc: 2.3849e-04\n",
      "Epoch 33/90\n",
      "9782/9782 [==============================] - 0s 47us/step - loss: 9.4081e-04 - acc: 1.0223e-04 - val_loss: 4.8212e-04 - val_acc: 2.3849e-04\n",
      "Epoch 34/90\n",
      "9782/9782 [==============================] - 0s 48us/step - loss: 9.8479e-04 - acc: 1.0223e-04 - val_loss: 0.0012 - val_acc: 2.3849e-04\n",
      "Epoch 35/90\n",
      "9782/9782 [==============================] - 0s 49us/step - loss: 9.5071e-04 - acc: 1.0223e-04 - val_loss: 6.9842e-04 - val_acc: 2.3849e-04\n",
      "Epoch 36/90\n",
      "9782/9782 [==============================] - 1s 52us/step - loss: 9.1280e-04 - acc: 1.0223e-04 - val_loss: 6.7218e-04 - val_acc: 2.3849e-04\n",
      "Epoch 37/90\n",
      "9782/9782 [==============================] - 0s 45us/step - loss: 9.3024e-04 - acc: 1.0223e-04 - val_loss: 0.0011 - val_acc: 2.3849e-04\n",
      "Epoch 38/90\n",
      "9782/9782 [==============================] - 0s 50us/step - loss: 9.2309e-04 - acc: 1.0223e-04 - val_loss: 8.3252e-04 - val_acc: 2.3849e-04\n",
      "Epoch 39/90\n",
      "9782/9782 [==============================] - 0s 48us/step - loss: 9.2595e-04 - acc: 1.0223e-04 - val_loss: 5.4402e-04 - val_acc: 2.3849e-04\n",
      "Epoch 40/90\n",
      "9782/9782 [==============================] - 1s 55us/step - loss: 8.9355e-04 - acc: 1.0223e-04 - val_loss: 5.4410e-04 - val_acc: 2.3849e-04\n",
      "Epoch 41/90\n",
      "9782/9782 [==============================] - 1s 55us/step - loss: 9.0858e-04 - acc: 1.0223e-04 - val_loss: 0.0011 - val_acc: 2.3849e-04\n",
      "Epoch 42/90\n",
      "9782/9782 [==============================] - 1s 61us/step - loss: 9.5210e-04 - acc: 1.0223e-04 - val_loss: 9.5326e-04 - val_acc: 2.3849e-04\n",
      "Epoch 43/90\n",
      "9782/9782 [==============================] - 1s 68us/step - loss: 8.8722e-04 - acc: 1.0223e-04 - val_loss: 5.1590e-04 - val_acc: 2.3849e-04\n",
      "Epoch 44/90\n",
      "9782/9782 [==============================] - 1s 58us/step - loss: 8.5629e-04 - acc: 1.0223e-04 - val_loss: 0.0013 - val_acc: 2.3849e-04\n",
      "Epoch 45/90\n",
      "9782/9782 [==============================] - 1s 71us/step - loss: 8.7284e-04 - acc: 1.0223e-04 - val_loss: 5.8075e-04 - val_acc: 2.3849e-04\n",
      "Epoch 46/90\n",
      "9782/9782 [==============================] - 1s 80us/step - loss: 8.3667e-04 - acc: 1.0223e-04 - val_loss: 6.8614e-04 - val_acc: 2.3849e-04\n",
      "Epoch 47/90\n",
      "9782/9782 [==============================] - 1s 59us/step - loss: 8.4454e-04 - acc: 1.0223e-04 - val_loss: 8.0890e-04 - val_acc: 2.3849e-04\n",
      "Epoch 48/90\n",
      "9782/9782 [==============================] - 0s 50us/step - loss: 8.3100e-04 - acc: 1.0223e-04 - val_loss: 8.6068e-04 - val_acc: 2.3849e-04\n",
      "Epoch 49/90\n",
      "9782/9782 [==============================] - 0s 47us/step - loss: 8.3631e-04 - acc: 1.0223e-04 - val_loss: 4.6337e-04 - val_acc: 2.3849e-04\n",
      "Epoch 50/90\n",
      "9782/9782 [==============================] - 0s 51us/step - loss: 8.6489e-04 - acc: 1.0223e-04 - val_loss: 5.4392e-04 - val_acc: 2.3849e-04\n",
      "Epoch 51/90\n",
      "9782/9782 [==============================] - 1s 58us/step - loss: 8.2384e-04 - acc: 1.0223e-04 - val_loss: 5.1553e-04 - val_acc: 2.3849e-04\n",
      "Epoch 52/90\n",
      "9782/9782 [==============================] - 1s 66us/step - loss: 8.1945e-04 - acc: 1.0223e-04 - val_loss: 4.5196e-04 - val_acc: 2.3849e-04\n",
      "Epoch 53/90\n",
      "9782/9782 [==============================] - 1s 61us/step - loss: 8.3795e-04 - acc: 1.0223e-04 - val_loss: 5.3157e-04 - val_acc: 2.3849e-04\n",
      "Epoch 54/90\n",
      "9782/9782 [==============================] - 1s 56us/step - loss: 7.8482e-04 - acc: 1.0223e-04 - val_loss: 6.6991e-04 - val_acc: 2.3849e-04\n",
      "Epoch 55/90\n",
      "9782/9782 [==============================] - 0s 49us/step - loss: 8.0100e-04 - acc: 1.0223e-04 - val_loss: 5.0156e-04 - val_acc: 2.3849e-04\n",
      "Epoch 56/90\n",
      "9782/9782 [==============================] - 0s 50us/step - loss: 7.9817e-04 - acc: 1.0223e-04 - val_loss: 4.1211e-04 - val_acc: 2.3849e-04\n",
      "Epoch 57/90\n",
      "9782/9782 [==============================] - 0s 46us/step - loss: 8.6557e-04 - acc: 1.0223e-04 - val_loss: 4.0639e-04 - val_acc: 2.3849e-04\n",
      "Epoch 58/90\n",
      "9782/9782 [==============================] - 0s 45us/step - loss: 8.4588e-04 - acc: 1.0223e-04 - val_loss: 8.6013e-04 - val_acc: 2.3849e-04\n",
      "Epoch 59/90\n",
      "9782/9782 [==============================] - 0s 45us/step - loss: 8.4204e-04 - acc: 1.0223e-04 - val_loss: 7.0024e-04 - val_acc: 2.3849e-04\n",
      "Epoch 60/90\n",
      "9782/9782 [==============================] - 1s 57us/step - loss: 7.9460e-04 - acc: 1.0223e-04 - val_loss: 0.0012 - val_acc: 2.3849e-04\n",
      "Epoch 61/90\n",
      "9782/9782 [==============================] - 1s 54us/step - loss: 8.2483e-04 - acc: 1.0223e-04 - val_loss: 7.1989e-04 - val_acc: 2.3849e-04\n",
      "Epoch 62/90\n",
      "9782/9782 [==============================] - 1s 52us/step - loss: 9.9973e-04 - acc: 1.0223e-04 - val_loss: 7.3191e-04 - val_acc: 2.3849e-04\n",
      "Epoch 63/90\n",
      "9782/9782 [==============================] - 0s 48us/step - loss: 8.6027e-04 - acc: 1.0223e-04 - val_loss: 5.9269e-04 - val_acc: 2.3849e-04\n",
      "Epoch 64/90\n",
      "9782/9782 [==============================] - 0s 49us/step - loss: 7.6959e-04 - acc: 1.0223e-04 - val_loss: 7.2461e-04 - val_acc: 2.3849e-04\n",
      "Epoch 65/90\n",
      "9782/9782 [==============================] - 0s 46us/step - loss: 7.8893e-04 - acc: 1.0223e-04 - val_loss: 7.7115e-04 - val_acc: 2.3849e-04\n",
      "Epoch 66/90\n",
      "9782/9782 [==============================] - 0s 44us/step - loss: 7.8180e-04 - acc: 1.0223e-04 - val_loss: 0.0011 - val_acc: 2.3849e-04\n",
      "Epoch 67/90\n",
      "9782/9782 [==============================] - 0s 43us/step - loss: 7.9688e-04 - acc: 1.0223e-04 - val_loss: 7.5313e-04 - val_acc: 2.3849e-04\n",
      "Epoch 68/90\n",
      "9782/9782 [==============================] - 0s 46us/step - loss: 7.4693e-04 - acc: 1.0223e-04 - val_loss: 8.0424e-04 - val_acc: 2.3849e-04\n",
      "Epoch 69/90\n",
      "9782/9782 [==============================] - 0s 43us/step - loss: 7.4213e-04 - acc: 1.0223e-04 - val_loss: 7.6073e-04 - val_acc: 2.3849e-04\n",
      "Epoch 70/90\n",
      "9782/9782 [==============================] - 0s 45us/step - loss: 7.4883e-04 - acc: 1.0223e-04 - val_loss: 8.7098e-04 - val_acc: 2.3849e-04\n",
      "Epoch 71/90\n",
      "9782/9782 [==============================] - 0s 45us/step - loss: 7.5040e-04 - acc: 1.0223e-04 - val_loss: 4.4049e-04 - val_acc: 2.3849e-04\n",
      "Epoch 72/90\n",
      "9782/9782 [==============================] - 0s 48us/step - loss: 7.6684e-04 - acc: 1.0223e-04 - val_loss: 7.5038e-04 - val_acc: 2.3849e-04\n",
      "Epoch 73/90\n",
      "9782/9782 [==============================] - 0s 48us/step - loss: 7.5701e-04 - acc: 1.0223e-04 - val_loss: 8.0990e-04 - val_acc: 2.3849e-04\n",
      "Epoch 74/90\n",
      "9782/9782 [==============================] - 0s 48us/step - loss: 7.6470e-04 - acc: 1.0223e-04 - val_loss: 0.0013 - val_acc: 2.3849e-04\n",
      "Epoch 75/90\n",
      "9782/9782 [==============================] - 0s 48us/step - loss: 8.0393e-04 - acc: 1.0223e-04 - val_loss: 5.5986e-04 - val_acc: 2.3849e-04\n",
      "Epoch 76/90\n",
      "9782/9782 [==============================] - 0s 46us/step - loss: 7.6198e-04 - acc: 1.0223e-04 - val_loss: 5.0854e-04 - val_acc: 2.3849e-04\n",
      "Epoch 77/90\n",
      "9782/9782 [==============================] - 0s 45us/step - loss: 7.7522e-04 - acc: 1.0223e-04 - val_loss: 0.0020 - val_acc: 2.3849e-04\n",
      "Epoch 78/90\n",
      "9782/9782 [==============================] - 0s 46us/step - loss: 7.9246e-04 - acc: 1.0223e-04 - val_loss: 5.7807e-04 - val_acc: 2.3849e-04\n",
      "Epoch 79/90\n",
      "9782/9782 [==============================] - 0s 45us/step - loss: 7.0724e-04 - acc: 1.0223e-04 - val_loss: 4.9251e-04 - val_acc: 2.3849e-04\n",
      "Epoch 80/90\n",
      "9782/9782 [==============================] - 0s 50us/step - loss: 7.0103e-04 - acc: 1.0223e-04 - val_loss: 5.0446e-04 - val_acc: 2.3849e-04\n",
      "Epoch 81/90\n",
      "9782/9782 [==============================] - 0s 49us/step - loss: 7.1523e-04 - acc: 1.0223e-04 - val_loss: 3.9632e-04 - val_acc: 2.3849e-04\n",
      "Epoch 82/90\n",
      "9782/9782 [==============================] - 1s 54us/step - loss: 6.8915e-04 - acc: 1.0223e-04 - val_loss: 6.0674e-04 - val_acc: 2.3849e-04\n",
      "Epoch 83/90\n",
      "9782/9782 [==============================] - 1s 58us/step - loss: 6.9093e-04 - acc: 1.0223e-04 - val_loss: 7.3123e-04 - val_acc: 2.3849e-04\n",
      "Epoch 84/90\n",
      "9782/9782 [==============================] - 1s 64us/step - loss: 6.9002e-04 - acc: 1.0223e-04 - val_loss: 7.3097e-04 - val_acc: 2.3849e-04\n",
      "Epoch 85/90\n",
      "9782/9782 [==============================] - 1s 61us/step - loss: 6.9586e-04 - acc: 1.0223e-04 - val_loss: 4.0390e-04 - val_acc: 2.3849e-04\n",
      "Epoch 86/90\n",
      "9782/9782 [==============================] - 1s 59us/step - loss: 7.0406e-04 - acc: 1.0223e-04 - val_loss: 0.0011 - val_acc: 2.3849e-04\n",
      "Epoch 87/90\n",
      "9782/9782 [==============================] - 1s 63us/step - loss: 7.0171e-04 - acc: 1.0223e-04 - val_loss: 0.0010 - val_acc: 2.3849e-04\n",
      "Epoch 88/90\n",
      "9782/9782 [==============================] - 1s 55us/step - loss: 6.9965e-04 - acc: 1.0223e-04 - val_loss: 4.2269e-04 - val_acc: 2.3849e-04\n",
      "Epoch 89/90\n",
      "9782/9782 [==============================] - 1s 62us/step - loss: 7.1589e-04 - acc: 1.0223e-04 - val_loss: 5.9269e-04 - val_acc: 2.3849e-04\n",
      "Epoch 90/90\n",
      "9782/9782 [==============================] - 1s 54us/step - loss: 6.8733e-04 - acc: 1.0223e-04 - val_loss: 4.2963e-04 - val_acc: 2.3849e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a1ec1fcc0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train,batch_size=512,epochs=epochs,validation_split=0.3,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_score(model, X_train, y_train, X_test, y_test):\n",
    "    trainScore = model.evaluate(X_train, y_train, verbose=0)\n",
    "    print('Train Score: %.5f MSE (%.2f RMSE)' % (trainScore[0], math.sqrt(trainScore[0])))\n",
    "\n",
    "    testScore = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print('Test Score: %.5f MSE (%.2f RMSE)' % (testScore[0], math.sqrt(testScore[0])))\n",
    "    return trainScore[0], testScore[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.00037 MSE (0.02 RMSE)\n",
      "Test Score: 0.00037 MSE (0.02 RMSE)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0003710565636182714, 0.0003710565636182714)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_score(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage_difference(model, X_test, y_test):\n",
    "    percentage_diff=[]\n",
    "\n",
    "    p = model.predict(X_test)\n",
    "    for u in range(len(y_test)): # for each data index in test data\n",
    "        pr = p[u][0] # pr = prediction on day u\n",
    "\n",
    "        percentage_diff.append((pr-y_test[u]/pr)*100)\n",
    "    print(mean(percentage_diff))\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-a5a88dcc240f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpercentage_difference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "p = percentage_difference(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result_norm(stock_name, normalized_value_p, normalized_value_y_test):\n",
    "    newp=normalized_value_p\n",
    "    newy_test=normalized_value_y_test\n",
    "    plt2.plot(newp, color='red', label='Prediction')\n",
    "    plt2.plot(newy_test,color='blue', label='Actual')\n",
    "    plt2.legend(loc='best')\n",
    "    plt2.title('The test result for {}'.format(stock_name))\n",
    "    plt2.xlabel('5 Min ahead Forecast')\n",
    "    plt2.ylabel('Price')\n",
    "    plt2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(stock_name, normalized_value,split=0.7,predict=True):\n",
    "    \"\"\"\n",
    "    Return a dataframe of that stock and normalize all the values. \n",
    "    (Optional: create moving average)\n",
    "    \"\"\"\n",
    "    df = xl.parse(stock_name)\n",
    "    df.drop(['VOLUME'], 1, inplace=True)\n",
    "    df.set_index('Date', inplace=True)\n",
    "    \n",
    "    # Renaming all the columns so that we can use the old version code\n",
    "    df.rename(columns={'OPEN': 'Open', 'HIGH': 'High', 'LOW': 'Low', 'NUMBER_TICKS': 'Volume', 'LAST_PRICE': 'Adj Close'}, inplace=True)\n",
    "\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "    df = df['Adj Close'].values.reshape(-1,1)\n",
    "    normalized_value = normalized_value.reshape(-1,1)\n",
    "    \n",
    "    row = round(split * df.shape[0]) \n",
    "    if predict:\n",
    "        df_p=df[0:row].copy()\n",
    "    else:\n",
    "        df_p=df[row:len(df)].copy()\n",
    "    \n",
    "    #return df.shape, p.shape\n",
    "    max_df=np.max(df_p)\n",
    "    min_df=np.min(df_p)\n",
    "    new=normalized_value*(max_df-min_df)+min_df\n",
    "      \n",
    "    return new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Portfolio construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def portfolio(currency_list,file = 'FX-5.xlsx',seq_len = 22,shape = [seq_len, 9, 1],neurons = [256, 256, 32, 1],dropout = 0.3,decay = 0.5,\n",
    "              epochs = 90,ma=[50, 100, 200],split=0.7):\n",
    "    i=0\n",
    "    mini=99999999\n",
    "    for currency in currency_list:\n",
    "        df=get_stock_data(currency,  ma)\n",
    "        X_train, y_train, X_test, y_test = load_data(df,True,seq_len,split,ma)\n",
    "        model = build_model_CNN(shape, neurons, dropout, decay)\n",
    "        model.fit(X_train,y_train,batch_size=512,epochs=epochs,validation_split=0.3,verbose=1)\n",
    "        p = percentage_difference(model, X_test, y_test)\n",
    "        newp = denormalize(currency, p,predict=True)\n",
    "        if mini>p.size:\n",
    "            mini=p.size\n",
    "        if i==0:\n",
    "            predict=newp.copy()\n",
    "        else:\n",
    "            predict=np.hstack((predict[0:mini],newp[0:mini]))\n",
    "        i+=1\n",
    "    return predict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of features = 9\n",
      "Amount of training data = 13998\n",
      "Amount of testing data = 5999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(input_shape=(22, 9), activation=\"relu\", filters=64, kernel_size=2, strides=1, padding=\"valid\")`\n",
      "  \n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=2)`\n",
      "  if __name__ == '__main__':\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(input_shape=(22, 9), activation=\"relu\", filters=64, kernel_size=2, strides=1, padding=\"valid\")`\n",
      "  app.launch_new_instance()\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=2)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 21, 64)            1216      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 10, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 9, 64)             8256      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 4, 64)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 250)               64250     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 251       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 73,973\n",
      "Trainable params: 73,973\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 9782 samples, validate on 4193 samples\n",
      "Epoch 1/90\n",
      "9782/9782 [==============================] - 1s 106us/step - loss: 0.0167 - acc: 1.0223e-04 - val_loss: 0.0083 - val_acc: 2.3849e-04\n",
      "Epoch 2/90\n",
      "9782/9782 [==============================] - 0s 48us/step - loss: 0.0048 - acc: 1.0223e-04 - val_loss: 8.7412e-04 - val_acc: 2.3849e-04\n",
      "Epoch 3/90\n",
      "9782/9782 [==============================] - 0s 46us/step - loss: 0.0031 - acc: 1.0223e-04 - val_loss: 0.0022 - val_acc: 2.3849e-04\n",
      "Epoch 4/90\n",
      "9782/9782 [==============================] - 0s 43us/step - loss: 0.0024 - acc: 1.0223e-04 - val_loss: 0.0017 - val_acc: 2.3849e-04\n",
      "Epoch 5/90\n",
      "9782/9782 [==============================] - 0s 44us/step - loss: 0.0021 - acc: 1.0223e-04 - val_loss: 0.0023 - val_acc: 2.3849e-04\n",
      "Epoch 6/90\n",
      "9782/9782 [==============================] - 0s 44us/step - loss: 0.0018 - acc: 1.0223e-04 - val_loss: 0.0018 - val_acc: 2.3849e-04\n",
      "Epoch 7/90\n",
      "9782/9782 [==============================] - 0s 41us/step - loss: 0.0016 - acc: 1.0223e-04 - val_loss: 0.0010 - val_acc: 2.3849e-04\n",
      "Epoch 8/90\n",
      "9782/9782 [==============================] - 0s 44us/step - loss: 0.0016 - acc: 1.0223e-04 - val_loss: 0.0016 - val_acc: 2.3849e-04\n",
      "Epoch 9/90\n",
      "9782/9782 [==============================] - 0s 45us/step - loss: 0.0014 - acc: 1.0223e-04 - val_loss: 0.0018 - val_acc: 2.3849e-04\n",
      "Epoch 10/90\n",
      "9782/9782 [==============================] - 0s 44us/step - loss: 0.0014 - acc: 1.0223e-04 - val_loss: 0.0011 - val_acc: 2.3849e-04\n",
      "Epoch 11/90\n",
      "9782/9782 [==============================] - 0s 42us/step - loss: 0.0013 - acc: 1.0223e-04 - val_loss: 0.0015 - val_acc: 2.3849e-04\n",
      "Epoch 12/90\n",
      "9782/9782 [==============================] - 0s 43us/step - loss: 0.0013 - acc: 1.0223e-04 - val_loss: 6.6988e-04 - val_acc: 2.3849e-04\n",
      "Epoch 13/90\n",
      "9782/9782 [==============================] - 0s 44us/step - loss: 0.0012 - acc: 1.0223e-04 - val_loss: 0.0025 - val_acc: 2.3849e-04\n",
      "Epoch 14/90\n",
      "9782/9782 [==============================] - 0s 44us/step - loss: 0.0012 - acc: 1.0223e-04 - val_loss: 0.0012 - val_acc: 2.3849e-04\n",
      "Epoch 15/90\n",
      "9782/9782 [==============================] - 0s 43us/step - loss: 0.0011 - acc: 1.0223e-04 - val_loss: 7.5721e-04 - val_acc: 2.3849e-04\n",
      "Epoch 16/90\n",
      "9782/9782 [==============================] - 0s 44us/step - loss: 0.0011 - acc: 1.0223e-04 - val_loss: 0.0014 - val_acc: 2.3849e-04\n",
      "Epoch 17/90\n",
      "9782/9782 [==============================] - 0s 43us/step - loss: 0.0010 - acc: 1.0223e-04 - val_loss: 0.0021 - val_acc: 2.3849e-04\n",
      "Epoch 18/90\n",
      "9782/9782 [==============================] - 0s 45us/step - loss: 0.0010 - acc: 1.0223e-04 - val_loss: 0.0015 - val_acc: 2.3849e-04\n",
      "Epoch 19/90\n",
      "9782/9782 [==============================] - 0s 44us/step - loss: 9.9453e-04 - acc: 1.0223e-04 - val_loss: 7.3263e-04 - val_acc: 2.3849e-04\n",
      "Epoch 20/90\n",
      "9782/9782 [==============================] - 0s 42us/step - loss: 0.0010 - acc: 1.0223e-04 - val_loss: 0.0012 - val_acc: 2.3849e-04\n",
      "Epoch 21/90\n",
      "9782/9782 [==============================] - 0s 44us/step - loss: 9.7170e-04 - acc: 1.0223e-04 - val_loss: 0.0020 - val_acc: 2.3849e-04\n",
      "Epoch 22/90\n",
      "9782/9782 [==============================] - 0s 44us/step - loss: 9.5648e-04 - acc: 1.0223e-04 - val_loss: 0.0018 - val_acc: 2.3849e-04\n",
      "Epoch 23/90\n",
      "9782/9782 [==============================] - 0s 45us/step - loss: 9.4898e-04 - acc: 1.0223e-04 - val_loss: 7.4365e-04 - val_acc: 2.3849e-04\n",
      "Epoch 24/90\n",
      "9782/9782 [==============================] - 0s 44us/step - loss: 9.7956e-04 - acc: 1.0223e-04 - val_loss: 8.9687e-04 - val_acc: 2.3849e-04\n",
      "Epoch 25/90\n",
      "9782/9782 [==============================] - 0s 44us/step - loss: 9.1873e-04 - acc: 1.0223e-04 - val_loss: 5.4347e-04 - val_acc: 2.3849e-04\n",
      "Epoch 26/90\n",
      "9782/9782 [==============================] - 0s 44us/step - loss: 8.9184e-04 - acc: 1.0223e-04 - val_loss: 5.9332e-04 - val_acc: 2.3849e-04\n",
      "Epoch 27/90\n",
      "9782/9782 [==============================] - 0s 44us/step - loss: 9.9819e-04 - acc: 1.0223e-04 - val_loss: 0.0017 - val_acc: 2.3849e-04\n",
      "Epoch 28/90\n",
      "9782/9782 [==============================] - 0s 44us/step - loss: 9.3998e-04 - acc: 1.0223e-04 - val_loss: 0.0017 - val_acc: 2.3849e-04\n",
      "Epoch 29/90\n",
      "9782/9782 [==============================] - 0s 43us/step - loss: 9.1790e-04 - acc: 1.0223e-04 - val_loss: 0.0012 - val_acc: 2.3849e-04\n",
      "Epoch 30/90\n",
      "9782/9782 [==============================] - 0s 44us/step - loss: 8.5890e-04 - acc: 1.0223e-04 - val_loss: 7.4638e-04 - val_acc: 2.3849e-04\n",
      "Epoch 31/90\n",
      "9782/9782 [==============================] - 0s 43us/step - loss: 8.9756e-04 - acc: 1.0223e-04 - val_loss: 0.0017 - val_acc: 2.3849e-04\n",
      "Epoch 32/90\n",
      "9782/9782 [==============================] - 0s 44us/step - loss: 8.8354e-04 - acc: 1.0223e-04 - val_loss: 0.0016 - val_acc: 2.3849e-04\n",
      "Epoch 33/90\n",
      "9782/9782 [==============================] - 0s 44us/step - loss: 8.7781e-04 - acc: 1.0223e-04 - val_loss: 0.0025 - val_acc: 2.3849e-04\n",
      "Epoch 34/90\n",
      "9782/9782 [==============================] - 0s 43us/step - loss: 8.9775e-04 - acc: 1.0223e-04 - val_loss: 0.0014 - val_acc: 2.3849e-04\n",
      "Epoch 35/90\n",
      "9782/9782 [==============================] - 0s 44us/step - loss: 8.6401e-04 - acc: 1.0223e-04 - val_loss: 8.5464e-04 - val_acc: 2.3849e-04\n",
      "Epoch 36/90\n",
      "9782/9782 [==============================] - 0s 45us/step - loss: 8.4147e-04 - acc: 1.0223e-04 - val_loss: 0.0010 - val_acc: 2.3849e-04\n",
      "Epoch 37/90\n",
      "9782/9782 [==============================] - 0s 44us/step - loss: 8.0594e-04 - acc: 1.0223e-04 - val_loss: 7.6020e-04 - val_acc: 2.3849e-04\n",
      "Epoch 38/90\n",
      "9782/9782 [==============================] - 0s 45us/step - loss: 8.2846e-04 - acc: 1.0223e-04 - val_loss: 0.0013 - val_acc: 2.3849e-04\n",
      "Epoch 39/90\n",
      "9782/9782 [==============================] - 0s 43us/step - loss: 8.3402e-04 - acc: 1.0223e-04 - val_loss: 0.0010 - val_acc: 2.3849e-04\n",
      "Epoch 40/90\n",
      "9782/9782 [==============================] - 0s 44us/step - loss: 7.8118e-04 - acc: 1.0223e-04 - val_loss: 6.9451e-04 - val_acc: 2.3849e-04\n",
      "Epoch 41/90\n",
      "9782/9782 [==============================] - 0s 44us/step - loss: 7.8912e-04 - acc: 1.0223e-04 - val_loss: 0.0013 - val_acc: 2.3849e-04\n",
      "Epoch 42/90\n",
      "9782/9782 [==============================] - 0s 43us/step - loss: 8.2790e-04 - acc: 1.0223e-04 - val_loss: 0.0023 - val_acc: 2.3849e-04\n",
      "Epoch 43/90\n",
      "9782/9782 [==============================] - 0s 45us/step - loss: 9.6656e-04 - acc: 1.0223e-04 - val_loss: 0.0012 - val_acc: 2.3849e-04\n",
      "Epoch 44/90\n",
      "9782/9782 [==============================] - 0s 42us/step - loss: 8.0003e-04 - acc: 1.0223e-04 - val_loss: 4.8328e-04 - val_acc: 2.3849e-04\n",
      "Epoch 45/90\n",
      "9782/9782 [==============================] - 0s 43us/step - loss: 8.1265e-04 - acc: 1.0223e-04 - val_loss: 0.0011 - val_acc: 2.3849e-04\n",
      "Epoch 46/90\n",
      "9782/9782 [==============================] - 0s 43us/step - loss: 7.7815e-04 - acc: 1.0223e-04 - val_loss: 7.9064e-04 - val_acc: 2.3849e-04\n",
      "Epoch 47/90\n",
      "9782/9782 [==============================] - 0s 42us/step - loss: 7.6931e-04 - acc: 1.0223e-04 - val_loss: 8.4764e-04 - val_acc: 2.3849e-04\n",
      "Epoch 48/90\n",
      "9782/9782 [==============================] - 0s 43us/step - loss: 7.7613e-04 - acc: 1.0223e-04 - val_loss: 6.1479e-04 - val_acc: 2.3849e-04\n",
      "Epoch 49/90\n",
      "9782/9782 [==============================] - 0s 46us/step - loss: 8.1436e-04 - acc: 1.0223e-04 - val_loss: 8.2772e-04 - val_acc: 2.3849e-04\n",
      "Epoch 50/90\n",
      "9782/9782 [==============================] - 0s 45us/step - loss: 7.5186e-04 - acc: 1.0223e-04 - val_loss: 0.0011 - val_acc: 2.3849e-04\n",
      "Epoch 51/90\n",
      "9782/9782 [==============================] - 0s 43us/step - loss: 7.5790e-04 - acc: 1.0223e-04 - val_loss: 7.6279e-04 - val_acc: 2.3849e-04\n",
      "Epoch 52/90\n",
      "9782/9782 [==============================] - 0s 43us/step - loss: 7.6614e-04 - acc: 1.0223e-04 - val_loss: 8.9220e-04 - val_acc: 2.3849e-04\n",
      "Epoch 53/90\n",
      "9782/9782 [==============================] - 0s 43us/step - loss: 7.6335e-04 - acc: 1.0223e-04 - val_loss: 9.4296e-04 - val_acc: 2.3849e-04\n",
      "Epoch 54/90\n",
      "9782/9782 [==============================] - 0s 43us/step - loss: 8.0750e-04 - acc: 1.0223e-04 - val_loss: 4.4232e-04 - val_acc: 2.3849e-04\n",
      "Epoch 55/90\n",
      "9782/9782 [==============================] - 0s 44us/step - loss: 7.3192e-04 - acc: 1.0223e-04 - val_loss: 6.2567e-04 - val_acc: 2.3849e-04\n",
      "Epoch 56/90\n",
      "9782/9782 [==============================] - 0s 43us/step - loss: 7.5797e-04 - acc: 1.0223e-04 - val_loss: 4.8461e-04 - val_acc: 2.3849e-04\n",
      "Epoch 57/90\n",
      "9782/9782 [==============================] - 0s 43us/step - loss: 7.3890e-04 - acc: 1.0223e-04 - val_loss: 7.2658e-04 - val_acc: 2.3849e-04\n",
      "Epoch 58/90\n",
      "9782/9782 [==============================] - 0s 43us/step - loss: 7.2269e-04 - acc: 1.0223e-04 - val_loss: 9.3467e-04 - val_acc: 2.3849e-04\n",
      "Epoch 59/90\n",
      "9782/9782 [==============================] - 0s 42us/step - loss: 7.0058e-04 - acc: 1.0223e-04 - val_loss: 3.9042e-04 - val_acc: 2.3849e-04\n",
      "Epoch 60/90\n",
      "9782/9782 [==============================] - 0s 43us/step - loss: 7.3425e-04 - acc: 1.0223e-04 - val_loss: 4.7461e-04 - val_acc: 2.3849e-04\n",
      "Epoch 61/90\n",
      "9782/9782 [==============================] - 0s 43us/step - loss: 7.2509e-04 - acc: 1.0223e-04 - val_loss: 7.7506e-04 - val_acc: 2.3849e-04\n",
      "Epoch 62/90\n",
      "9782/9782 [==============================] - 0s 43us/step - loss: 7.2820e-04 - acc: 1.0223e-04 - val_loss: 8.1403e-04 - val_acc: 2.3849e-04\n",
      "Epoch 63/90\n",
      "9782/9782 [==============================] - 0s 44us/step - loss: 7.0441e-04 - acc: 1.0223e-04 - val_loss: 4.6976e-04 - val_acc: 2.3849e-04\n",
      "Epoch 64/90\n",
      "9782/9782 [==============================] - 0s 44us/step - loss: 7.2309e-04 - acc: 1.0223e-04 - val_loss: 4.3128e-04 - val_acc: 2.3849e-04\n",
      "Epoch 65/90\n",
      "9782/9782 [==============================] - 0s 42us/step - loss: 7.5768e-04 - acc: 1.0223e-04 - val_loss: 5.1158e-04 - val_acc: 2.3849e-04\n",
      "Epoch 66/90\n",
      "9782/9782 [==============================] - 0s 42us/step - loss: 7.1914e-04 - acc: 1.0223e-04 - val_loss: 5.9838e-04 - val_acc: 2.3849e-04\n",
      "Epoch 67/90\n",
      "9782/9782 [==============================] - 0s 43us/step - loss: 6.7229e-04 - acc: 1.0223e-04 - val_loss: 0.0011 - val_acc: 2.3849e-04\n",
      "Epoch 68/90\n",
      "9782/9782 [==============================] - 0s 42us/step - loss: 7.1336e-04 - acc: 1.0223e-04 - val_loss: 6.7305e-04 - val_acc: 2.3849e-04\n",
      "Epoch 69/90\n",
      "9782/9782 [==============================] - 0s 43us/step - loss: 7.0668e-04 - acc: 1.0223e-04 - val_loss: 9.2136e-04 - val_acc: 2.3849e-04\n",
      "Epoch 70/90\n",
      "9782/9782 [==============================] - 0s 43us/step - loss: 6.8091e-04 - acc: 1.0223e-04 - val_loss: 4.7792e-04 - val_acc: 2.3849e-04\n",
      "Epoch 71/90\n",
      "9782/9782 [==============================] - 0s 43us/step - loss: 6.7287e-04 - acc: 1.0223e-04 - val_loss: 4.2899e-04 - val_acc: 2.3849e-04\n",
      "Epoch 72/90\n",
      "9782/9782 [==============================] - 0s 43us/step - loss: 6.6028e-04 - acc: 1.0223e-04 - val_loss: 7.5918e-04 - val_acc: 2.3849e-04\n",
      "Epoch 73/90\n",
      "9782/9782 [==============================] - 0s 43us/step - loss: 6.9957e-04 - acc: 1.0223e-04 - val_loss: 0.0010 - val_acc: 2.3849e-04\n",
      "Epoch 74/90\n",
      "9782/9782 [==============================] - 0s 43us/step - loss: 6.7026e-04 - acc: 1.0223e-04 - val_loss: 8.1524e-04 - val_acc: 2.3849e-04\n",
      "Epoch 75/90\n",
      "9782/9782 [==============================] - 0s 43us/step - loss: 6.4473e-04 - acc: 1.0223e-04 - val_loss: 8.0886e-04 - val_acc: 2.3849e-04\n",
      "Epoch 76/90\n",
      "9782/9782 [==============================] - 0s 43us/step - loss: 6.6413e-04 - acc: 1.0223e-04 - val_loss: 4.1428e-04 - val_acc: 2.3849e-04\n",
      "Epoch 77/90\n",
      "9782/9782 [==============================] - 0s 44us/step - loss: 6.5270e-04 - acc: 1.0223e-04 - val_loss: 7.7475e-04 - val_acc: 2.3849e-04\n",
      "Epoch 78/90\n",
      "9782/9782 [==============================] - 0s 43us/step - loss: 6.4655e-04 - acc: 1.0223e-04 - val_loss: 4.7037e-04 - val_acc: 2.3849e-04\n",
      "Epoch 79/90\n",
      "9782/9782 [==============================] - 0s 43us/step - loss: 7.2496e-04 - acc: 1.0223e-04 - val_loss: 5.2517e-04 - val_acc: 2.3849e-04\n",
      "Epoch 80/90\n",
      "9782/9782 [==============================] - 0s 43us/step - loss: 6.7273e-04 - acc: 1.0223e-04 - val_loss: 7.8260e-04 - val_acc: 2.3849e-04\n",
      "Epoch 81/90\n",
      "9782/9782 [==============================] - 0s 43us/step - loss: 6.4133e-04 - acc: 1.0223e-04 - val_loss: 5.1440e-04 - val_acc: 2.3849e-04\n",
      "Epoch 82/90\n",
      "9782/9782 [==============================] - 0s 42us/step - loss: 7.0942e-04 - acc: 1.0223e-04 - val_loss: 9.2948e-04 - val_acc: 2.3849e-04\n",
      "Epoch 83/90\n",
      "9782/9782 [==============================] - 0s 44us/step - loss: 6.6823e-04 - acc: 1.0223e-04 - val_loss: 7.4472e-04 - val_acc: 2.3849e-04\n",
      "Epoch 84/90\n",
      "9782/9782 [==============================] - 0s 43us/step - loss: 6.4006e-04 - acc: 1.0223e-04 - val_loss: 9.3205e-04 - val_acc: 2.3849e-04\n",
      "Epoch 85/90\n",
      "9782/9782 [==============================] - 0s 43us/step - loss: 6.3604e-04 - acc: 1.0223e-04 - val_loss: 4.9532e-04 - val_acc: 2.3849e-04\n",
      "Epoch 86/90\n",
      "9782/9782 [==============================] - 0s 44us/step - loss: 6.7990e-04 - acc: 1.0223e-04 - val_loss: 0.0014 - val_acc: 2.3849e-04\n",
      "Epoch 87/90\n",
      "9782/9782 [==============================] - 0s 43us/step - loss: 6.4775e-04 - acc: 1.0223e-04 - val_loss: 0.0013 - val_acc: 2.3849e-04\n",
      "Epoch 88/90\n",
      "9782/9782 [==============================] - 0s 42us/step - loss: 6.5884e-04 - acc: 1.0223e-04 - val_loss: 9.0395e-04 - val_acc: 2.3849e-04\n",
      "Epoch 89/90\n",
      "9782/9782 [==============================] - 0s 43us/step - loss: 6.0293e-04 - acc: 1.0223e-04 - val_loss: 6.4769e-04 - val_acc: 2.3849e-04\n",
      "Epoch 90/90\n",
      "9782/9782 [==============================] - 0s 43us/step - loss: 6.1869e-04 - acc: 1.0223e-04 - val_loss: 7.4638e-04 - val_acc: 2.3849e-04\n",
      "-61.15915835589804\n",
      "Amount of features = 9\n",
      "Amount of training data = 14003\n",
      "Amount of testing data = 6001\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_5 (Conv1D)            (None, 21, 64)            1216      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 10, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 9, 64)             8256      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 4, 64)             0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 4, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 250)               64250     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 251       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 73,973\n",
      "Trainable params: 73,973\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9786 samples, validate on 4194 samples\n",
      "Epoch 1/90\n",
      "9786/9786 [==============================] - 1s 91us/step - loss: 0.0658 - acc: 0.0000e+00 - val_loss: 0.0031 - val_acc: 2.3844e-04\n",
      "Epoch 2/90\n",
      "9786/9786 [==============================] - 0s 42us/step - loss: 0.0108 - acc: 1.0219e-04 - val_loss: 0.0028 - val_acc: 2.3844e-04\n",
      "Epoch 3/90\n",
      "9786/9786 [==============================] - 0s 42us/step - loss: 0.0075 - acc: 1.0219e-04 - val_loss: 0.0027 - val_acc: 2.3844e-04\n",
      "Epoch 4/90\n",
      "9786/9786 [==============================] - 0s 42us/step - loss: 0.0058 - acc: 1.0219e-04 - val_loss: 0.0026 - val_acc: 2.3844e-04\n",
      "Epoch 5/90\n",
      "9786/9786 [==============================] - 0s 42us/step - loss: 0.0051 - acc: 1.0219e-04 - val_loss: 0.0033 - val_acc: 2.3844e-04\n",
      "Epoch 6/90\n",
      "9786/9786 [==============================] - 0s 42us/step - loss: 0.0043 - acc: 1.0219e-04 - val_loss: 0.0045 - val_acc: 2.3844e-04\n",
      "Epoch 7/90\n",
      "9786/9786 [==============================] - 0s 42us/step - loss: 0.0039 - acc: 1.0219e-04 - val_loss: 0.0039 - val_acc: 2.3844e-04\n",
      "Epoch 8/90\n",
      "9786/9786 [==============================] - 0s 42us/step - loss: 0.0035 - acc: 1.0219e-04 - val_loss: 0.0028 - val_acc: 2.3844e-04\n",
      "Epoch 9/90\n",
      "9786/9786 [==============================] - 0s 44us/step - loss: 0.0032 - acc: 1.0219e-04 - val_loss: 0.0031 - val_acc: 2.3844e-04\n",
      "Epoch 10/90\n",
      "9786/9786 [==============================] - 0s 43us/step - loss: 0.0031 - acc: 1.0219e-04 - val_loss: 0.0029 - val_acc: 2.3844e-04\n",
      "Epoch 11/90\n",
      "9786/9786 [==============================] - 0s 46us/step - loss: 0.0029 - acc: 1.0219e-04 - val_loss: 0.0027 - val_acc: 2.3844e-04\n",
      "Epoch 12/90\n",
      "9786/9786 [==============================] - 0s 44us/step - loss: 0.0028 - acc: 1.0219e-04 - val_loss: 0.0025 - val_acc: 2.3844e-04\n",
      "Epoch 13/90\n",
      "9786/9786 [==============================] - 0s 42us/step - loss: 0.0027 - acc: 1.0219e-04 - val_loss: 0.0029 - val_acc: 2.3844e-04\n",
      "Epoch 14/90\n",
      "9786/9786 [==============================] - 0s 44us/step - loss: 0.0027 - acc: 1.0219e-04 - val_loss: 0.0020 - val_acc: 2.3844e-04\n",
      "Epoch 15/90\n",
      "9786/9786 [==============================] - 0s 43us/step - loss: 0.0025 - acc: 1.0219e-04 - val_loss: 0.0023 - val_acc: 2.3844e-04\n",
      "Epoch 16/90\n",
      "9786/9786 [==============================] - 0s 44us/step - loss: 0.0024 - acc: 1.0219e-04 - val_loss: 0.0028 - val_acc: 2.3844e-04\n",
      "Epoch 17/90\n",
      "9786/9786 [==============================] - 0s 43us/step - loss: 0.0023 - acc: 1.0219e-04 - val_loss: 0.0029 - val_acc: 2.3844e-04\n",
      "Epoch 18/90\n",
      "9786/9786 [==============================] - 0s 43us/step - loss: 0.0023 - acc: 1.0219e-04 - val_loss: 0.0025 - val_acc: 2.3844e-04\n",
      "Epoch 19/90\n",
      "9786/9786 [==============================] - 0s 44us/step - loss: 0.0023 - acc: 1.0219e-04 - val_loss: 0.0032 - val_acc: 2.3844e-04\n",
      "Epoch 20/90\n",
      "9786/9786 [==============================] - 0s 42us/step - loss: 0.0024 - acc: 1.0219e-04 - val_loss: 0.0030 - val_acc: 2.3844e-04\n",
      "Epoch 21/90\n",
      "9786/9786 [==============================] - 0s 43us/step - loss: 0.0023 - acc: 1.0219e-04 - val_loss: 0.0030 - val_acc: 2.3844e-04\n",
      "Epoch 22/90\n",
      "9786/9786 [==============================] - 0s 44us/step - loss: 0.0023 - acc: 1.0219e-04 - val_loss: 0.0027 - val_acc: 2.3844e-04\n",
      "Epoch 23/90\n",
      "9786/9786 [==============================] - 0s 43us/step - loss: 0.0022 - acc: 1.0219e-04 - val_loss: 0.0032 - val_acc: 2.3844e-04\n",
      "Epoch 24/90\n",
      "9786/9786 [==============================] - 0s 43us/step - loss: 0.0022 - acc: 1.0219e-04 - val_loss: 0.0032 - val_acc: 2.3844e-04\n",
      "Epoch 25/90\n",
      "9786/9786 [==============================] - 0s 44us/step - loss: 0.0022 - acc: 1.0219e-04 - val_loss: 0.0042 - val_acc: 2.3844e-04\n",
      "Epoch 26/90\n",
      "9786/9786 [==============================] - 0s 43us/step - loss: 0.0021 - acc: 1.0219e-04 - val_loss: 0.0042 - val_acc: 2.3844e-04\n",
      "Epoch 27/90\n",
      "9786/9786 [==============================] - 0s 43us/step - loss: 0.0022 - acc: 1.0219e-04 - val_loss: 0.0043 - val_acc: 2.3844e-04\n",
      "Epoch 28/90\n",
      "9786/9786 [==============================] - 0s 42us/step - loss: 0.0022 - acc: 1.0219e-04 - val_loss: 0.0045 - val_acc: 2.3844e-04\n",
      "Epoch 29/90\n",
      "9786/9786 [==============================] - 0s 43us/step - loss: 0.0020 - acc: 1.0219e-04 - val_loss: 0.0045 - val_acc: 2.3844e-04\n",
      "Epoch 30/90\n",
      "9786/9786 [==============================] - 0s 43us/step - loss: 0.0020 - acc: 1.0219e-04 - val_loss: 0.0047 - val_acc: 2.3844e-04\n",
      "Epoch 31/90\n",
      "9786/9786 [==============================] - 0s 44us/step - loss: 0.0020 - acc: 1.0219e-04 - val_loss: 0.0046 - val_acc: 2.3844e-04\n",
      "Epoch 32/90\n",
      "9786/9786 [==============================] - 0s 43us/step - loss: 0.0020 - acc: 1.0219e-04 - val_loss: 0.0042 - val_acc: 2.3844e-04\n",
      "Epoch 33/90\n",
      "9786/9786 [==============================] - 0s 42us/step - loss: 0.0022 - acc: 1.0219e-04 - val_loss: 0.0049 - val_acc: 2.3844e-04\n",
      "Epoch 34/90\n",
      "9786/9786 [==============================] - 0s 44us/step - loss: 0.0020 - acc: 1.0219e-04 - val_loss: 0.0044 - val_acc: 2.3844e-04\n",
      "Epoch 35/90\n",
      "9786/9786 [==============================] - 0s 42us/step - loss: 0.0019 - acc: 1.0219e-04 - val_loss: 0.0040 - val_acc: 2.3844e-04\n",
      "Epoch 36/90\n",
      "9786/9786 [==============================] - 0s 43us/step - loss: 0.0019 - acc: 1.0219e-04 - val_loss: 0.0042 - val_acc: 2.3844e-04\n",
      "Epoch 37/90\n",
      "9786/9786 [==============================] - 0s 43us/step - loss: 0.0019 - acc: 1.0219e-04 - val_loss: 0.0055 - val_acc: 2.3844e-04\n",
      "Epoch 38/90\n",
      "9786/9786 [==============================] - 0s 43us/step - loss: 0.0019 - acc: 1.0219e-04 - val_loss: 0.0056 - val_acc: 2.3844e-04\n",
      "Epoch 39/90\n",
      "9786/9786 [==============================] - 0s 43us/step - loss: 0.0018 - acc: 1.0219e-04 - val_loss: 0.0062 - val_acc: 2.3844e-04\n",
      "Epoch 40/90\n",
      "9786/9786 [==============================] - 0s 43us/step - loss: 0.0018 - acc: 1.0219e-04 - val_loss: 0.0053 - val_acc: 2.3844e-04\n",
      "Epoch 41/90\n",
      "9786/9786 [==============================] - 0s 43us/step - loss: 0.0019 - acc: 1.0219e-04 - val_loss: 0.0053 - val_acc: 2.3844e-04\n",
      "Epoch 42/90\n",
      "9786/9786 [==============================] - 0s 43us/step - loss: 0.0022 - acc: 1.0219e-04 - val_loss: 0.0065 - val_acc: 2.3844e-04\n",
      "Epoch 43/90\n",
      "9786/9786 [==============================] - 0s 42us/step - loss: 0.0019 - acc: 1.0219e-04 - val_loss: 0.0056 - val_acc: 2.3844e-04\n",
      "Epoch 44/90\n",
      "9786/9786 [==============================] - 0s 43us/step - loss: 0.0018 - acc: 1.0219e-04 - val_loss: 0.0061 - val_acc: 2.3844e-04\n",
      "Epoch 45/90\n",
      "9786/9786 [==============================] - 0s 43us/step - loss: 0.0019 - acc: 1.0219e-04 - val_loss: 0.0068 - val_acc: 2.3844e-04\n",
      "Epoch 46/90\n",
      "9786/9786 [==============================] - 0s 44us/step - loss: 0.0018 - acc: 1.0219e-04 - val_loss: 0.0076 - val_acc: 2.3844e-04\n",
      "Epoch 47/90\n",
      "9786/9786 [==============================] - 0s 43us/step - loss: 0.0018 - acc: 1.0219e-04 - val_loss: 0.0058 - val_acc: 2.3844e-04\n",
      "Epoch 48/90\n",
      "9786/9786 [==============================] - 0s 45us/step - loss: 0.0017 - acc: 1.0219e-04 - val_loss: 0.0071 - val_acc: 2.3844e-04\n",
      "Epoch 49/90\n",
      "9786/9786 [==============================] - 0s 44us/step - loss: 0.0017 - acc: 1.0219e-04 - val_loss: 0.0077 - val_acc: 2.3844e-04\n",
      "Epoch 50/90\n",
      "9786/9786 [==============================] - 0s 43us/step - loss: 0.0017 - acc: 1.0219e-04 - val_loss: 0.0072 - val_acc: 2.3844e-04\n",
      "Epoch 51/90\n",
      "9786/9786 [==============================] - 0s 43us/step - loss: 0.0017 - acc: 1.0219e-04 - val_loss: 0.0075 - val_acc: 2.3844e-04\n",
      "Epoch 52/90\n",
      "9786/9786 [==============================] - 0s 43us/step - loss: 0.0017 - acc: 1.0219e-04 - val_loss: 0.0067 - val_acc: 2.3844e-04\n",
      "Epoch 53/90\n",
      "9786/9786 [==============================] - 0s 43us/step - loss: 0.0017 - acc: 1.0219e-04 - val_loss: 0.0062 - val_acc: 2.3844e-04\n",
      "Epoch 54/90\n",
      "9786/9786 [==============================] - 0s 43us/step - loss: 0.0016 - acc: 1.0219e-04 - val_loss: 0.0080 - val_acc: 2.3844e-04\n",
      "Epoch 55/90\n",
      "9786/9786 [==============================] - 0s 44us/step - loss: 0.0017 - acc: 1.0219e-04 - val_loss: 0.0077 - val_acc: 2.3844e-04\n",
      "Epoch 56/90\n",
      "9786/9786 [==============================] - 0s 43us/step - loss: 0.0016 - acc: 1.0219e-04 - val_loss: 0.0072 - val_acc: 2.3844e-04\n",
      "Epoch 57/90\n",
      "9786/9786 [==============================] - 0s 42us/step - loss: 0.0017 - acc: 1.0219e-04 - val_loss: 0.0079 - val_acc: 2.3844e-04\n",
      "Epoch 58/90\n",
      "9786/9786 [==============================] - 0s 43us/step - loss: 0.0016 - acc: 1.0219e-04 - val_loss: 0.0064 - val_acc: 2.3844e-04\n",
      "Epoch 59/90\n",
      "9786/9786 [==============================] - 0s 43us/step - loss: 0.0016 - acc: 1.0219e-04 - val_loss: 0.0067 - val_acc: 2.3844e-04\n",
      "Epoch 60/90\n",
      "9786/9786 [==============================] - 0s 43us/step - loss: 0.0016 - acc: 1.0219e-04 - val_loss: 0.0065 - val_acc: 2.3844e-04\n",
      "Epoch 61/90\n",
      "9786/9786 [==============================] - 0s 43us/step - loss: 0.0016 - acc: 1.0219e-04 - val_loss: 0.0064 - val_acc: 2.3844e-04\n",
      "Epoch 62/90\n",
      "9786/9786 [==============================] - 0s 45us/step - loss: 0.0017 - acc: 1.0219e-04 - val_loss: 0.0058 - val_acc: 2.3844e-04\n",
      "Epoch 63/90\n",
      "9786/9786 [==============================] - 0s 45us/step - loss: 0.0016 - acc: 1.0219e-04 - val_loss: 0.0059 - val_acc: 2.3844e-04\n",
      "Epoch 64/90\n",
      "9786/9786 [==============================] - 0s 44us/step - loss: 0.0015 - acc: 1.0219e-04 - val_loss: 0.0072 - val_acc: 2.3844e-04\n",
      "Epoch 65/90\n",
      "9786/9786 [==============================] - 0s 46us/step - loss: 0.0016 - acc: 1.0219e-04 - val_loss: 0.0064 - val_acc: 2.3844e-04\n",
      "Epoch 66/90\n",
      "9786/9786 [==============================] - 0s 50us/step - loss: 0.0016 - acc: 1.0219e-04 - val_loss: 0.0076 - val_acc: 2.3844e-04\n",
      "Epoch 67/90\n",
      "9786/9786 [==============================] - 0s 45us/step - loss: 0.0016 - acc: 1.0219e-04 - val_loss: 0.0069 - val_acc: 2.3844e-04\n",
      "Epoch 68/90\n",
      "9786/9786 [==============================] - 0s 42us/step - loss: 0.0015 - acc: 1.0219e-04 - val_loss: 0.0062 - val_acc: 2.3844e-04\n",
      "Epoch 69/90\n",
      "9786/9786 [==============================] - 0s 44us/step - loss: 0.0015 - acc: 1.0219e-04 - val_loss: 0.0064 - val_acc: 2.3844e-04\n",
      "Epoch 70/90\n",
      "9786/9786 [==============================] - 0s 43us/step - loss: 0.0015 - acc: 1.0219e-04 - val_loss: 0.0062 - val_acc: 2.3844e-04\n",
      "Epoch 71/90\n",
      "9786/9786 [==============================] - 0s 44us/step - loss: 0.0015 - acc: 1.0219e-04 - val_loss: 0.0062 - val_acc: 2.3844e-04\n",
      "Epoch 72/90\n",
      "9786/9786 [==============================] - 0s 43us/step - loss: 0.0014 - acc: 1.0219e-04 - val_loss: 0.0074 - val_acc: 2.3844e-04\n",
      "Epoch 73/90\n",
      "9786/9786 [==============================] - 0s 43us/step - loss: 0.0015 - acc: 1.0219e-04 - val_loss: 0.0087 - val_acc: 2.3844e-04\n",
      "Epoch 74/90\n",
      "9786/9786 [==============================] - 0s 45us/step - loss: 0.0015 - acc: 1.0219e-04 - val_loss: 0.0064 - val_acc: 2.3844e-04\n",
      "Epoch 75/90\n",
      "9786/9786 [==============================] - 0s 45us/step - loss: 0.0014 - acc: 1.0219e-04 - val_loss: 0.0074 - val_acc: 2.3844e-04\n",
      "Epoch 76/90\n",
      "9786/9786 [==============================] - 0s 45us/step - loss: 0.0014 - acc: 1.0219e-04 - val_loss: 0.0075 - val_acc: 2.3844e-04\n",
      "Epoch 77/90\n",
      "9786/9786 [==============================] - 0s 45us/step - loss: 0.0015 - acc: 1.0219e-04 - val_loss: 0.0070 - val_acc: 2.3844e-04\n",
      "Epoch 78/90\n",
      "9786/9786 [==============================] - 0s 46us/step - loss: 0.0014 - acc: 1.0219e-04 - val_loss: 0.0071 - val_acc: 2.3844e-04\n",
      "Epoch 79/90\n",
      "9786/9786 [==============================] - 0s 49us/step - loss: 0.0014 - acc: 1.0219e-04 - val_loss: 0.0071 - val_acc: 2.3844e-04\n",
      "Epoch 80/90\n",
      "9786/9786 [==============================] - 0s 51us/step - loss: 0.0014 - acc: 1.0219e-04 - val_loss: 0.0084 - val_acc: 2.3844e-04\n",
      "Epoch 81/90\n",
      "9786/9786 [==============================] - 0s 48us/step - loss: 0.0014 - acc: 1.0219e-04 - val_loss: 0.0071 - val_acc: 2.3844e-04\n",
      "Epoch 82/90\n",
      "9786/9786 [==============================] - 0s 45us/step - loss: 0.0014 - acc: 1.0219e-04 - val_loss: 0.0067 - val_acc: 2.3844e-04\n",
      "Epoch 83/90\n",
      "9786/9786 [==============================] - 0s 46us/step - loss: 0.0014 - acc: 1.0219e-04 - val_loss: 0.0062 - val_acc: 2.3844e-04\n",
      "Epoch 84/90\n",
      "9786/9786 [==============================] - 1s 51us/step - loss: 0.0013 - acc: 1.0219e-04 - val_loss: 0.0080 - val_acc: 2.3844e-04\n",
      "Epoch 85/90\n",
      "9786/9786 [==============================] - 0s 50us/step - loss: 0.0013 - acc: 1.0219e-04 - val_loss: 0.0072 - val_acc: 2.3844e-04\n",
      "Epoch 86/90\n",
      "9786/9786 [==============================] - 1s 52us/step - loss: 0.0013 - acc: 1.0219e-04 - val_loss: 0.0080 - val_acc: 2.3844e-04\n",
      "Epoch 87/90\n",
      "9786/9786 [==============================] - 0s 50us/step - loss: 0.0013 - acc: 1.0219e-04 - val_loss: 0.0057 - val_acc: 2.3844e-04\n",
      "Epoch 88/90\n",
      "9786/9786 [==============================] - 0s 47us/step - loss: 0.0013 - acc: 1.0219e-04 - val_loss: 0.0086 - val_acc: 2.3844e-04\n",
      "Epoch 89/90\n",
      "9786/9786 [==============================] - 0s 47us/step - loss: 0.0013 - acc: 1.0219e-04 - val_loss: 0.0077 - val_acc: 2.3844e-04\n",
      "Epoch 90/90\n",
      "9786/9786 [==============================] - 0s 48us/step - loss: 0.0013 - acc: 1.0219e-04 - val_loss: 0.0076 - val_acc: 2.3844e-04\n",
      "-38.58969388822745\n",
      "Amount of features = 9\n",
      "Amount of training data = 14029\n",
      "Amount of testing data = 6013\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_7 (Conv1D)            (None, 21, 64)            1216      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 10, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 9, 64)             8256      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 4, 64)             0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 4, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 250)               64250     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 251       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 73,973\n",
      "Trainable params: 73,973\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 9804 samples, validate on 4202 samples\n",
      "Epoch 1/90\n",
      "9804/9804 [==============================] - 1s 98us/step - loss: 0.0392 - acc: 1.0200e-04 - val_loss: 0.0010 - val_acc: 2.3798e-04\n",
      "Epoch 2/90\n",
      "9804/9804 [==============================] - 0s 43us/step - loss: 0.0076 - acc: 1.0200e-04 - val_loss: 0.0011 - val_acc: 2.3798e-04\n",
      "Epoch 3/90\n",
      "9804/9804 [==============================] - 0s 44us/step - loss: 0.0046 - acc: 1.0200e-04 - val_loss: 0.0017 - val_acc: 2.3798e-04\n",
      "Epoch 4/90\n",
      "9804/9804 [==============================] - 0s 45us/step - loss: 0.0036 - acc: 1.0200e-04 - val_loss: 0.0020 - val_acc: 2.3798e-04\n",
      "Epoch 5/90\n",
      "9804/9804 [==============================] - 0s 45us/step - loss: 0.0031 - acc: 1.0200e-04 - val_loss: 6.0587e-04 - val_acc: 2.3798e-04\n",
      "Epoch 6/90\n",
      "9804/9804 [==============================] - 0s 44us/step - loss: 0.0028 - acc: 1.0200e-04 - val_loss: 0.0012 - val_acc: 2.3798e-04\n",
      "Epoch 7/90\n",
      "9804/9804 [==============================] - 0s 43us/step - loss: 0.0025 - acc: 1.0200e-04 - val_loss: 0.0018 - val_acc: 2.3798e-04\n",
      "Epoch 8/90\n",
      "9804/9804 [==============================] - 0s 43us/step - loss: 0.0023 - acc: 1.0200e-04 - val_loss: 0.0013 - val_acc: 2.3798e-04\n",
      "Epoch 9/90\n",
      "9804/9804 [==============================] - 0s 44us/step - loss: 0.0022 - acc: 1.0200e-04 - val_loss: 9.9487e-04 - val_acc: 2.3798e-04\n",
      "Epoch 10/90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9804/9804 [==============================] - 0s 44us/step - loss: 0.0020 - acc: 1.0200e-04 - val_loss: 0.0015 - val_acc: 2.3798e-04\n",
      "Epoch 11/90\n",
      "9804/9804 [==============================] - 0s 43us/step - loss: 0.0019 - acc: 1.0200e-04 - val_loss: 0.0018 - val_acc: 2.3798e-04\n",
      "Epoch 12/90\n",
      "9804/9804 [==============================] - 0s 43us/step - loss: 0.0019 - acc: 1.0200e-04 - val_loss: 0.0011 - val_acc: 2.3798e-04\n",
      "Epoch 13/90\n",
      "9804/9804 [==============================] - 0s 43us/step - loss: 0.0017 - acc: 1.0200e-04 - val_loss: 9.1970e-04 - val_acc: 2.3798e-04\n",
      "Epoch 14/90\n",
      "9804/9804 [==============================] - 0s 44us/step - loss: 0.0017 - acc: 1.0200e-04 - val_loss: 5.5694e-04 - val_acc: 2.3798e-04\n",
      "Epoch 15/90\n",
      "9804/9804 [==============================] - 0s 44us/step - loss: 0.0016 - acc: 1.0200e-04 - val_loss: 8.2795e-04 - val_acc: 2.3798e-04\n",
      "Epoch 16/90\n",
      "9804/9804 [==============================] - 0s 43us/step - loss: 0.0015 - acc: 1.0200e-04 - val_loss: 8.6215e-04 - val_acc: 2.3798e-04\n",
      "Epoch 17/90\n",
      "9804/9804 [==============================] - 0s 44us/step - loss: 0.0015 - acc: 1.0200e-04 - val_loss: 0.0012 - val_acc: 2.3798e-04\n",
      "Epoch 18/90\n",
      "9804/9804 [==============================] - 0s 43us/step - loss: 0.0014 - acc: 1.0200e-04 - val_loss: 0.0016 - val_acc: 2.3798e-04\n",
      "Epoch 19/90\n",
      "9804/9804 [==============================] - 0s 43us/step - loss: 0.0015 - acc: 1.0200e-04 - val_loss: 0.0016 - val_acc: 2.3798e-04\n",
      "Epoch 20/90\n",
      "9804/9804 [==============================] - 0s 44us/step - loss: 0.0014 - acc: 1.0200e-04 - val_loss: 0.0019 - val_acc: 2.3798e-04\n",
      "Epoch 21/90\n",
      "9804/9804 [==============================] - 0s 43us/step - loss: 0.0014 - acc: 1.0200e-04 - val_loss: 0.0028 - val_acc: 2.3798e-04\n",
      "Epoch 22/90\n",
      "9804/9804 [==============================] - 0s 42us/step - loss: 0.0014 - acc: 1.0200e-04 - val_loss: 0.0017 - val_acc: 2.3798e-04\n",
      "Epoch 23/90\n",
      "9804/9804 [==============================] - 0s 43us/step - loss: 0.0014 - acc: 1.0200e-04 - val_loss: 6.0520e-04 - val_acc: 2.3798e-04\n",
      "Epoch 24/90\n",
      "9804/9804 [==============================] - 0s 43us/step - loss: 0.0013 - acc: 1.0200e-04 - val_loss: 8.8495e-04 - val_acc: 2.3798e-04\n",
      "Epoch 25/90\n",
      "9804/9804 [==============================] - 0s 45us/step - loss: 0.0012 - acc: 1.0200e-04 - val_loss: 0.0010 - val_acc: 2.3798e-04\n",
      "Epoch 26/90\n",
      "9804/9804 [==============================] - 0s 45us/step - loss: 0.0012 - acc: 1.0200e-04 - val_loss: 0.0011 - val_acc: 2.3798e-04\n",
      "Epoch 27/90\n",
      "9804/9804 [==============================] - 0s 44us/step - loss: 0.0012 - acc: 1.0200e-04 - val_loss: 9.0348e-04 - val_acc: 2.3798e-04\n",
      "Epoch 28/90\n",
      "9804/9804 [==============================] - 0s 44us/step - loss: 0.0012 - acc: 1.0200e-04 - val_loss: 0.0011 - val_acc: 2.3798e-04\n",
      "Epoch 29/90\n",
      "9804/9804 [==============================] - 0s 49us/step - loss: 0.0012 - acc: 1.0200e-04 - val_loss: 6.5798e-04 - val_acc: 2.3798e-04\n",
      "Epoch 30/90\n",
      "9804/9804 [==============================] - 0s 48us/step - loss: 0.0012 - acc: 1.0200e-04 - val_loss: 6.9011e-04 - val_acc: 2.3798e-04\n",
      "Epoch 31/90\n",
      "9804/9804 [==============================] - 0s 44us/step - loss: 0.0012 - acc: 1.0200e-04 - val_loss: 5.6353e-04 - val_acc: 2.3798e-04\n",
      "Epoch 32/90\n",
      "9804/9804 [==============================] - 0s 48us/step - loss: 0.0012 - acc: 1.0200e-04 - val_loss: 6.7466e-04 - val_acc: 2.3798e-04\n",
      "Epoch 33/90\n",
      "9804/9804 [==============================] - 1s 68us/step - loss: 0.0011 - acc: 1.0200e-04 - val_loss: 5.1372e-04 - val_acc: 2.3798e-04\n",
      "Epoch 34/90\n",
      "9804/9804 [==============================] - 1s 54us/step - loss: 0.0011 - acc: 1.0200e-04 - val_loss: 4.9548e-04 - val_acc: 2.3798e-04\n",
      "Epoch 35/90\n",
      "9804/9804 [==============================] - 0s 49us/step - loss: 0.0011 - acc: 1.0200e-04 - val_loss: 5.1671e-04 - val_acc: 2.3798e-04\n",
      "Epoch 36/90\n",
      "9804/9804 [==============================] - 0s 46us/step - loss: 0.0011 - acc: 1.0200e-04 - val_loss: 6.3151e-04 - val_acc: 2.3798e-04\n",
      "Epoch 37/90\n",
      "9804/9804 [==============================] - 0s 44us/step - loss: 0.0012 - acc: 1.0200e-04 - val_loss: 0.0014 - val_acc: 2.3798e-04\n",
      "Epoch 38/90\n",
      "9804/9804 [==============================] - 0s 46us/step - loss: 0.0012 - acc: 1.0200e-04 - val_loss: 0.0014 - val_acc: 2.3798e-04\n",
      "Epoch 39/90\n",
      "9804/9804 [==============================] - 0s 44us/step - loss: 0.0011 - acc: 1.0200e-04 - val_loss: 0.0015 - val_acc: 2.3798e-04\n",
      "Epoch 40/90\n",
      "9804/9804 [==============================] - 0s 46us/step - loss: 0.0011 - acc: 1.0200e-04 - val_loss: 8.8733e-04 - val_acc: 2.3798e-04\n",
      "Epoch 41/90\n",
      "9804/9804 [==============================] - 0s 42us/step - loss: 0.0010 - acc: 1.0200e-04 - val_loss: 6.2824e-04 - val_acc: 2.3798e-04\n",
      "Epoch 42/90\n",
      "9804/9804 [==============================] - 0s 45us/step - loss: 0.0010 - acc: 1.0200e-04 - val_loss: 4.9529e-04 - val_acc: 2.3798e-04\n",
      "Epoch 43/90\n",
      "9804/9804 [==============================] - 0s 44us/step - loss: 0.0011 - acc: 1.0200e-04 - val_loss: 5.7220e-04 - val_acc: 2.3798e-04\n",
      "Epoch 44/90\n",
      "9804/9804 [==============================] - 0s 49us/step - loss: 0.0010 - acc: 1.0200e-04 - val_loss: 5.2924e-04 - val_acc: 2.3798e-04\n",
      "Epoch 45/90\n",
      "9804/9804 [==============================] - 0s 47us/step - loss: 9.9256e-04 - acc: 1.0200e-04 - val_loss: 4.5157e-04 - val_acc: 2.3798e-04\n",
      "Epoch 46/90\n",
      "9804/9804 [==============================] - 0s 48us/step - loss: 0.0010 - acc: 1.0200e-04 - val_loss: 5.5683e-04 - val_acc: 2.3798e-04\n",
      "Epoch 47/90\n",
      "9804/9804 [==============================] - 1s 51us/step - loss: 9.8717e-04 - acc: 1.0200e-04 - val_loss: 8.4643e-04 - val_acc: 2.3798e-04\n",
      "Epoch 48/90\n",
      "9804/9804 [==============================] - 1s 54us/step - loss: 9.9042e-04 - acc: 1.0200e-04 - val_loss: 4.6185e-04 - val_acc: 2.3798e-04\n",
      "Epoch 49/90\n",
      "9804/9804 [==============================] - 0s 49us/step - loss: 9.6865e-04 - acc: 1.0200e-04 - val_loss: 4.4481e-04 - val_acc: 2.3798e-04\n",
      "Epoch 50/90\n",
      "9804/9804 [==============================] - 1s 55us/step - loss: 9.7885e-04 - acc: 1.0200e-04 - val_loss: 4.7056e-04 - val_acc: 2.3798e-04\n",
      "Epoch 51/90\n",
      "9804/9804 [==============================] - 1s 54us/step - loss: 9.4244e-04 - acc: 1.0200e-04 - val_loss: 4.3876e-04 - val_acc: 2.3798e-04\n",
      "Epoch 52/90\n",
      "9804/9804 [==============================] - 1s 53us/step - loss: 9.7093e-04 - acc: 1.0200e-04 - val_loss: 5.5535e-04 - val_acc: 2.3798e-04\n",
      "Epoch 53/90\n",
      "9804/9804 [==============================] - 1s 56us/step - loss: 9.0833e-04 - acc: 1.0200e-04 - val_loss: 6.1529e-04 - val_acc: 2.3798e-04\n",
      "Epoch 54/90\n",
      "9804/9804 [==============================] - 0s 50us/step - loss: 9.6058e-04 - acc: 1.0200e-04 - val_loss: 6.2329e-04 - val_acc: 2.3798e-04\n",
      "Epoch 55/90\n",
      "9804/9804 [==============================] - 1s 53us/step - loss: 9.3240e-04 - acc: 1.0200e-04 - val_loss: 9.2544e-04 - val_acc: 2.3798e-04\n",
      "Epoch 56/90\n",
      "9804/9804 [==============================] - 0s 48us/step - loss: 9.5536e-04 - acc: 1.0200e-04 - val_loss: 5.0170e-04 - val_acc: 2.3798e-04\n",
      "Epoch 57/90\n",
      "9804/9804 [==============================] - 0s 50us/step - loss: 9.0542e-04 - acc: 1.0200e-04 - val_loss: 4.8441e-04 - val_acc: 2.3798e-04\n",
      "Epoch 58/90\n",
      "9804/9804 [==============================] - 0s 47us/step - loss: 8.9965e-04 - acc: 1.0200e-04 - val_loss: 5.8862e-04 - val_acc: 2.3798e-04\n",
      "Epoch 59/90\n",
      "9804/9804 [==============================] - 0s 45us/step - loss: 9.0513e-04 - acc: 1.0200e-04 - val_loss: 4.5329e-04 - val_acc: 2.3798e-04\n",
      "Epoch 60/90\n",
      "9804/9804 [==============================] - 0s 47us/step - loss: 8.7694e-04 - acc: 1.0200e-04 - val_loss: 5.9782e-04 - val_acc: 2.3798e-04\n",
      "Epoch 61/90\n",
      "9804/9804 [==============================] - 0s 45us/step - loss: 8.8383e-04 - acc: 1.0200e-04 - val_loss: 4.9155e-04 - val_acc: 2.3798e-04\n",
      "Epoch 62/90\n",
      "9804/9804 [==============================] - 0s 47us/step - loss: 8.5453e-04 - acc: 1.0200e-04 - val_loss: 5.6628e-04 - val_acc: 2.3798e-04\n",
      "Epoch 63/90\n",
      "9804/9804 [==============================] - 0s 48us/step - loss: 9.2703e-04 - acc: 1.0200e-04 - val_loss: 4.6474e-04 - val_acc: 2.3798e-04\n",
      "Epoch 64/90\n",
      "9804/9804 [==============================] - 0s 45us/step - loss: 9.2999e-04 - acc: 1.0200e-04 - val_loss: 0.0023 - val_acc: 2.3798e-04\n",
      "Epoch 65/90\n",
      "9804/9804 [==============================] - 0s 49us/step - loss: 9.6776e-04 - acc: 1.0200e-04 - val_loss: 5.6872e-04 - val_acc: 2.3798e-04\n",
      "Epoch 66/90\n",
      "9804/9804 [==============================] - 0s 46us/step - loss: 8.5282e-04 - acc: 1.0200e-04 - val_loss: 4.7770e-04 - val_acc: 2.3798e-04\n",
      "Epoch 67/90\n",
      "9804/9804 [==============================] - 0s 44us/step - loss: 8.5885e-04 - acc: 1.0200e-04 - val_loss: 4.9279e-04 - val_acc: 2.3798e-04\n",
      "Epoch 68/90\n",
      "9804/9804 [==============================] - 0s 45us/step - loss: 8.8748e-04 - acc: 1.0200e-04 - val_loss: 6.1098e-04 - val_acc: 2.3798e-04\n",
      "Epoch 69/90\n",
      "9804/9804 [==============================] - 0s 44us/step - loss: 8.5571e-04 - acc: 1.0200e-04 - val_loss: 9.4135e-04 - val_acc: 2.3798e-04\n",
      "Epoch 70/90\n",
      "9804/9804 [==============================] - 0s 44us/step - loss: 8.5937e-04 - acc: 1.0200e-04 - val_loss: 5.7485e-04 - val_acc: 2.3798e-04\n",
      "Epoch 71/90\n",
      "9804/9804 [==============================] - 0s 44us/step - loss: 8.4873e-04 - acc: 1.0200e-04 - val_loss: 5.6410e-04 - val_acc: 2.3798e-04\n",
      "Epoch 72/90\n",
      "9804/9804 [==============================] - 0s 45us/step - loss: 9.4725e-04 - acc: 1.0200e-04 - val_loss: 0.0011 - val_acc: 2.3798e-04\n",
      "Epoch 73/90\n",
      "9804/9804 [==============================] - 0s 43us/step - loss: 8.7855e-04 - acc: 1.0200e-04 - val_loss: 6.4593e-04 - val_acc: 2.3798e-04\n",
      "Epoch 74/90\n",
      "9804/9804 [==============================] - 0s 43us/step - loss: 8.3413e-04 - acc: 1.0200e-04 - val_loss: 4.0980e-04 - val_acc: 2.3798e-04\n",
      "Epoch 75/90\n",
      "9804/9804 [==============================] - 0s 44us/step - loss: 8.0863e-04 - acc: 1.0200e-04 - val_loss: 4.6277e-04 - val_acc: 2.3798e-04\n",
      "Epoch 76/90\n",
      "9804/9804 [==============================] - 0s 43us/step - loss: 8.1413e-04 - acc: 1.0200e-04 - val_loss: 4.0350e-04 - val_acc: 2.3798e-04\n",
      "Epoch 77/90\n",
      "9804/9804 [==============================] - 0s 43us/step - loss: 7.9859e-04 - acc: 1.0200e-04 - val_loss: 8.8087e-04 - val_acc: 2.3798e-04\n",
      "Epoch 78/90\n",
      "9804/9804 [==============================] - 0s 44us/step - loss: 7.9836e-04 - acc: 1.0200e-04 - val_loss: 6.7352e-04 - val_acc: 2.3798e-04\n",
      "Epoch 79/90\n",
      "9804/9804 [==============================] - 0s 43us/step - loss: 8.3725e-04 - acc: 1.0200e-04 - val_loss: 8.3706e-04 - val_acc: 2.3798e-04\n",
      "Epoch 80/90\n",
      "9804/9804 [==============================] - 0s 44us/step - loss: 8.0019e-04 - acc: 1.0200e-04 - val_loss: 3.8577e-04 - val_acc: 2.3798e-04\n",
      "Epoch 81/90\n",
      "9804/9804 [==============================] - 0s 44us/step - loss: 8.2629e-04 - acc: 1.0200e-04 - val_loss: 5.5949e-04 - val_acc: 2.3798e-04\n",
      "Epoch 82/90\n",
      "9804/9804 [==============================] - 0s 44us/step - loss: 7.6155e-04 - acc: 1.0200e-04 - val_loss: 5.1455e-04 - val_acc: 2.3798e-04\n",
      "Epoch 83/90\n",
      "9804/9804 [==============================] - 0s 44us/step - loss: 7.6149e-04 - acc: 1.0200e-04 - val_loss: 9.9008e-04 - val_acc: 2.3798e-04\n",
      "Epoch 84/90\n",
      "9804/9804 [==============================] - 0s 44us/step - loss: 8.3515e-04 - acc: 1.0200e-04 - val_loss: 5.8892e-04 - val_acc: 2.3798e-04\n",
      "Epoch 85/90\n",
      "9804/9804 [==============================] - 0s 43us/step - loss: 7.8313e-04 - acc: 1.0200e-04 - val_loss: 5.9671e-04 - val_acc: 2.3798e-04\n",
      "Epoch 86/90\n",
      "9804/9804 [==============================] - 0s 44us/step - loss: 7.9817e-04 - acc: 1.0200e-04 - val_loss: 9.4045e-04 - val_acc: 2.3798e-04\n",
      "Epoch 87/90\n",
      "9804/9804 [==============================] - 0s 44us/step - loss: 8.1629e-04 - acc: 1.0200e-04 - val_loss: 3.9709e-04 - val_acc: 2.3798e-04\n",
      "Epoch 88/90\n",
      "9804/9804 [==============================] - 0s 44us/step - loss: 8.5417e-04 - acc: 1.0200e-04 - val_loss: 4.4312e-04 - val_acc: 2.3798e-04\n",
      "Epoch 89/90\n",
      "9804/9804 [==============================] - 0s 44us/step - loss: 7.8373e-04 - acc: 1.0200e-04 - val_loss: 7.1656e-04 - val_acc: 2.3798e-04\n",
      "Epoch 90/90\n",
      "9804/9804 [==============================] - 0s 44us/step - loss: 7.7845e-04 - acc: 1.0200e-04 - val_loss: 3.9584e-04 - val_acc: 2.3798e-04\n",
      "-48.502905617827714\n",
      "Amount of features = 9\n",
      "Amount of training data = 13971\n",
      "Amount of testing data = 5988\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_9 (Conv1D)            (None, 21, 64)            1216      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 10, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 9, 64)             8256      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 4, 64)             0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 4, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 250)               64250     \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 251       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 73,973\n",
      "Trainable params: 73,973\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 9763 samples, validate on 4185 samples\n",
      "Epoch 1/90\n",
      "9763/9763 [==============================] - 1s 103us/step - loss: 0.0286 - acc: 2.0486e-04 - val_loss: 0.0087 - val_acc: 0.0000e+00\n",
      "Epoch 2/90\n",
      "9763/9763 [==============================] - 0s 44us/step - loss: 0.0077 - acc: 2.0486e-04 - val_loss: 4.4781e-04 - val_acc: 0.0000e+00\n",
      "Epoch 3/90\n",
      "9763/9763 [==============================] - 0s 44us/step - loss: 0.0051 - acc: 2.0486e-04 - val_loss: 7.0959e-04 - val_acc: 0.0000e+00\n",
      "Epoch 4/90\n",
      "9763/9763 [==============================] - 0s 42us/step - loss: 0.0039 - acc: 2.0486e-04 - val_loss: 8.3233e-04 - val_acc: 0.0000e+00\n",
      "Epoch 5/90\n",
      "9763/9763 [==============================] - 0s 44us/step - loss: 0.0033 - acc: 2.0486e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 6/90\n",
      "9763/9763 [==============================] - 0s 42us/step - loss: 0.0028 - acc: 2.0486e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 7/90\n",
      "9763/9763 [==============================] - 0s 42us/step - loss: 0.0027 - acc: 2.0486e-04 - val_loss: 0.0028 - val_acc: 0.0000e+00\n",
      "Epoch 8/90\n",
      "9763/9763 [==============================] - 0s 44us/step - loss: 0.0025 - acc: 2.0486e-04 - val_loss: 0.0029 - val_acc: 0.0000e+00\n",
      "Epoch 9/90\n",
      "9763/9763 [==============================] - 0s 44us/step - loss: 0.0026 - acc: 2.0486e-04 - val_loss: 3.8336e-04 - val_acc: 0.0000e+00\n",
      "Epoch 10/90\n",
      "9763/9763 [==============================] - 0s 44us/step - loss: 0.0022 - acc: 2.0486e-04 - val_loss: 3.1041e-04 - val_acc: 0.0000e+00\n",
      "Epoch 11/90\n",
      "9763/9763 [==============================] - 0s 43us/step - loss: 0.0022 - acc: 2.0486e-04 - val_loss: 3.0808e-04 - val_acc: 0.0000e+00\n",
      "Epoch 12/90\n",
      "9763/9763 [==============================] - 0s 43us/step - loss: 0.0020 - acc: 2.0486e-04 - val_loss: 2.8769e-04 - val_acc: 0.0000e+00\n",
      "Epoch 13/90\n",
      "9763/9763 [==============================] - 0s 46us/step - loss: 0.0020 - acc: 2.0486e-04 - val_loss: 0.0038 - val_acc: 0.0000e+00\n",
      "Epoch 14/90\n",
      "9763/9763 [==============================] - 0s 43us/step - loss: 0.0020 - acc: 2.0486e-04 - val_loss: 2.7491e-04 - val_acc: 0.0000e+00\n",
      "Epoch 15/90\n",
      "9763/9763 [==============================] - 0s 44us/step - loss: 0.0019 - acc: 2.0486e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 16/90\n",
      "9763/9763 [==============================] - 0s 43us/step - loss: 0.0018 - acc: 2.0486e-04 - val_loss: 0.0024 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/90\n",
      "9763/9763 [==============================] - 0s 43us/step - loss: 0.0018 - acc: 2.0486e-04 - val_loss: 2.9382e-04 - val_acc: 0.0000e+00\n",
      "Epoch 18/90\n",
      "9763/9763 [==============================] - 0s 42us/step - loss: 0.0018 - acc: 2.0486e-04 - val_loss: 4.0383e-04 - val_acc: 0.0000e+00\n",
      "Epoch 19/90\n",
      "9763/9763 [==============================] - 0s 43us/step - loss: 0.0016 - acc: 2.0486e-04 - val_loss: 2.8767e-04 - val_acc: 0.0000e+00\n",
      "Epoch 20/90\n",
      "9763/9763 [==============================] - 0s 42us/step - loss: 0.0016 - acc: 2.0486e-04 - val_loss: 8.0553e-04 - val_acc: 0.0000e+00\n",
      "Epoch 21/90\n",
      "9763/9763 [==============================] - 0s 44us/step - loss: 0.0018 - acc: 2.0486e-04 - val_loss: 5.7760e-04 - val_acc: 0.0000e+00\n",
      "Epoch 22/90\n",
      "9763/9763 [==============================] - 0s 43us/step - loss: 0.0015 - acc: 2.0486e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 23/90\n",
      "9763/9763 [==============================] - 0s 42us/step - loss: 0.0016 - acc: 2.0486e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 24/90\n",
      "9763/9763 [==============================] - 0s 43us/step - loss: 0.0017 - acc: 2.0486e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 25/90\n",
      "9763/9763 [==============================] - 0s 44us/step - loss: 0.0015 - acc: 2.0486e-04 - val_loss: 3.1548e-04 - val_acc: 0.0000e+00\n",
      "Epoch 26/90\n",
      "9763/9763 [==============================] - 0s 44us/step - loss: 0.0016 - acc: 2.0486e-04 - val_loss: 2.6439e-04 - val_acc: 0.0000e+00\n",
      "Epoch 27/90\n",
      "9763/9763 [==============================] - 0s 43us/step - loss: 0.0015 - acc: 2.0486e-04 - val_loss: 2.7378e-04 - val_acc: 0.0000e+00\n",
      "Epoch 28/90\n",
      "9763/9763 [==============================] - 0s 43us/step - loss: 0.0015 - acc: 2.0486e-04 - val_loss: 4.0351e-04 - val_acc: 0.0000e+00\n",
      "Epoch 29/90\n",
      "9763/9763 [==============================] - 0s 42us/step - loss: 0.0016 - acc: 2.0486e-04 - val_loss: 2.5301e-04 - val_acc: 0.0000e+00\n",
      "Epoch 30/90\n",
      "9763/9763 [==============================] - 0s 43us/step - loss: 0.0015 - acc: 2.0486e-04 - val_loss: 4.7051e-04 - val_acc: 0.0000e+00\n",
      "Epoch 31/90\n",
      "9763/9763 [==============================] - 0s 43us/step - loss: 0.0015 - acc: 2.0486e-04 - val_loss: 4.9762e-04 - val_acc: 0.0000e+00\n",
      "Epoch 32/90\n",
      "9763/9763 [==============================] - 0s 43us/step - loss: 0.0016 - acc: 2.0486e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 33/90\n",
      "9763/9763 [==============================] - 0s 43us/step - loss: 0.0016 - acc: 2.0486e-04 - val_loss: 2.9367e-04 - val_acc: 0.0000e+00\n",
      "Epoch 34/90\n",
      "9763/9763 [==============================] - 0s 43us/step - loss: 0.0014 - acc: 2.0486e-04 - val_loss: 3.7922e-04 - val_acc: 0.0000e+00\n",
      "Epoch 35/90\n",
      "9763/9763 [==============================] - 0s 41us/step - loss: 0.0014 - acc: 2.0486e-04 - val_loss: 4.3770e-04 - val_acc: 0.0000e+00\n",
      "Epoch 36/90\n",
      "9763/9763 [==============================] - 0s 43us/step - loss: 0.0013 - acc: 2.0486e-04 - val_loss: 5.7766e-04 - val_acc: 0.0000e+00\n",
      "Epoch 37/90\n",
      "9763/9763 [==============================] - 0s 43us/step - loss: 0.0013 - acc: 2.0486e-04 - val_loss: 3.4820e-04 - val_acc: 0.0000e+00\n",
      "Epoch 38/90\n",
      "9763/9763 [==============================] - 0s 43us/step - loss: 0.0013 - acc: 2.0486e-04 - val_loss: 6.5956e-04 - val_acc: 0.0000e+00\n",
      "Epoch 39/90\n",
      "9763/9763 [==============================] - 0s 43us/step - loss: 0.0014 - acc: 2.0486e-04 - val_loss: 0.0025 - val_acc: 0.0000e+00\n",
      "Epoch 40/90\n",
      "9763/9763 [==============================] - 0s 42us/step - loss: 0.0016 - acc: 2.0486e-04 - val_loss: 4.8632e-04 - val_acc: 0.0000e+00\n",
      "Epoch 41/90\n",
      "9763/9763 [==============================] - 0s 43us/step - loss: 0.0014 - acc: 2.0486e-04 - val_loss: 2.7606e-04 - val_acc: 0.0000e+00\n",
      "Epoch 42/90\n",
      "9763/9763 [==============================] - 0s 42us/step - loss: 0.0013 - acc: 2.0486e-04 - val_loss: 3.7912e-04 - val_acc: 0.0000e+00\n",
      "Epoch 43/90\n",
      "9763/9763 [==============================] - 0s 42us/step - loss: 0.0013 - acc: 2.0486e-04 - val_loss: 3.5598e-04 - val_acc: 0.0000e+00\n",
      "Epoch 44/90\n",
      "9763/9763 [==============================] - 0s 42us/step - loss: 0.0014 - acc: 2.0486e-04 - val_loss: 2.7513e-04 - val_acc: 0.0000e+00\n",
      "Epoch 45/90\n",
      "9763/9763 [==============================] - 0s 43us/step - loss: 0.0013 - acc: 2.0486e-04 - val_loss: 6.5687e-04 - val_acc: 0.0000e+00\n",
      "Epoch 46/90\n",
      "9763/9763 [==============================] - 0s 43us/step - loss: 0.0013 - acc: 2.0486e-04 - val_loss: 3.7647e-04 - val_acc: 0.0000e+00\n",
      "Epoch 47/90\n",
      "9763/9763 [==============================] - 0s 42us/step - loss: 0.0013 - acc: 2.0486e-04 - val_loss: 4.7829e-04 - val_acc: 0.0000e+00\n",
      "Epoch 48/90\n",
      "9763/9763 [==============================] - 0s 43us/step - loss: 0.0014 - acc: 2.0486e-04 - val_loss: 6.4196e-04 - val_acc: 0.0000e+00\n",
      "Epoch 49/90\n",
      "9763/9763 [==============================] - 0s 43us/step - loss: 0.0012 - acc: 2.0486e-04 - val_loss: 3.5126e-04 - val_acc: 0.0000e+00\n",
      "Epoch 50/90\n",
      "9763/9763 [==============================] - 0s 43us/step - loss: 0.0012 - acc: 2.0486e-04 - val_loss: 4.4089e-04 - val_acc: 0.0000e+00\n",
      "Epoch 51/90\n",
      "9763/9763 [==============================] - 0s 44us/step - loss: 0.0012 - acc: 2.0486e-04 - val_loss: 2.8677e-04 - val_acc: 0.0000e+00\n",
      "Epoch 52/90\n",
      "9763/9763 [==============================] - 0s 44us/step - loss: 0.0012 - acc: 2.0486e-04 - val_loss: 0.0026 - val_acc: 0.0000e+00\n",
      "Epoch 53/90\n",
      "9763/9763 [==============================] - 0s 42us/step - loss: 0.0014 - acc: 2.0486e-04 - val_loss: 2.9531e-04 - val_acc: 0.0000e+00\n",
      "Epoch 54/90\n",
      "9763/9763 [==============================] - 0s 43us/step - loss: 0.0012 - acc: 2.0486e-04 - val_loss: 2.6167e-04 - val_acc: 0.0000e+00\n",
      "Epoch 55/90\n",
      "9763/9763 [==============================] - 0s 42us/step - loss: 0.0012 - acc: 2.0486e-04 - val_loss: 0.0038 - val_acc: 0.0000e+00\n",
      "Epoch 56/90\n",
      "9763/9763 [==============================] - 0s 42us/step - loss: 0.0015 - acc: 2.0486e-04 - val_loss: 3.2736e-04 - val_acc: 0.0000e+00\n",
      "Epoch 57/90\n",
      "9763/9763 [==============================] - 0s 42us/step - loss: 0.0012 - acc: 2.0486e-04 - val_loss: 3.9505e-04 - val_acc: 0.0000e+00\n",
      "Epoch 58/90\n",
      "9763/9763 [==============================] - 0s 43us/step - loss: 0.0011 - acc: 2.0486e-04 - val_loss: 3.2599e-04 - val_acc: 0.0000e+00\n",
      "Epoch 59/90\n",
      "9763/9763 [==============================] - 0s 42us/step - loss: 0.0011 - acc: 2.0486e-04 - val_loss: 2.5874e-04 - val_acc: 0.0000e+00\n",
      "Epoch 60/90\n",
      "9763/9763 [==============================] - 0s 43us/step - loss: 0.0011 - acc: 2.0486e-04 - val_loss: 3.0493e-04 - val_acc: 0.0000e+00\n",
      "Epoch 61/90\n",
      "9763/9763 [==============================] - 0s 42us/step - loss: 0.0011 - acc: 2.0486e-04 - val_loss: 2.5315e-04 - val_acc: 0.0000e+00\n",
      "Epoch 62/90\n",
      "9763/9763 [==============================] - 0s 43us/step - loss: 0.0011 - acc: 2.0486e-04 - val_loss: 6.8674e-04 - val_acc: 0.0000e+00\n",
      "Epoch 63/90\n",
      "9763/9763 [==============================] - 0s 43us/step - loss: 0.0012 - acc: 2.0486e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 64/90\n",
      "9763/9763 [==============================] - 0s 41us/step - loss: 0.0012 - acc: 2.0486e-04 - val_loss: 2.4966e-04 - val_acc: 0.0000e+00\n",
      "Epoch 65/90\n",
      "9763/9763 [==============================] - 0s 42us/step - loss: 0.0011 - acc: 2.0486e-04 - val_loss: 2.5145e-04 - val_acc: 0.0000e+00\n",
      "Epoch 66/90\n",
      "9763/9763 [==============================] - 0s 43us/step - loss: 0.0011 - acc: 2.0486e-04 - val_loss: 6.8603e-04 - val_acc: 0.0000e+00\n",
      "Epoch 67/90\n",
      "9763/9763 [==============================] - 0s 43us/step - loss: 0.0011 - acc: 2.0486e-04 - val_loss: 3.5910e-04 - val_acc: 0.0000e+00\n",
      "Epoch 68/90\n",
      "9763/9763 [==============================] - 0s 43us/step - loss: 0.0011 - acc: 2.0486e-04 - val_loss: 2.7664e-04 - val_acc: 0.0000e+00\n",
      "Epoch 69/90\n",
      "9763/9763 [==============================] - 0s 43us/step - loss: 0.0011 - acc: 2.0486e-04 - val_loss: 5.9479e-04 - val_acc: 0.0000e+00\n",
      "Epoch 70/90\n",
      "9763/9763 [==============================] - 0s 44us/step - loss: 0.0011 - acc: 2.0486e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 71/90\n",
      "9763/9763 [==============================] - 0s 44us/step - loss: 0.0010 - acc: 2.0486e-04 - val_loss: 2.3022e-04 - val_acc: 0.0000e+00\n",
      "Epoch 72/90\n",
      "9763/9763 [==============================] - 0s 43us/step - loss: 0.0010 - acc: 2.0486e-04 - val_loss: 2.6686e-04 - val_acc: 0.0000e+00\n",
      "Epoch 73/90\n",
      "9763/9763 [==============================] - 0s 42us/step - loss: 0.0010 - acc: 2.0486e-04 - val_loss: 5.2003e-04 - val_acc: 0.0000e+00\n",
      "Epoch 74/90\n",
      "9763/9763 [==============================] - 0s 43us/step - loss: 0.0010 - acc: 2.0486e-04 - val_loss: 3.2148e-04 - val_acc: 0.0000e+00\n",
      "Epoch 75/90\n",
      "9763/9763 [==============================] - 0s 43us/step - loss: 0.0011 - acc: 2.0486e-04 - val_loss: 2.3347e-04 - val_acc: 0.0000e+00\n",
      "Epoch 76/90\n",
      "9763/9763 [==============================] - 0s 42us/step - loss: 0.0011 - acc: 2.0486e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 77/90\n",
      "9763/9763 [==============================] - 0s 42us/step - loss: 0.0011 - acc: 2.0486e-04 - val_loss: 2.3412e-04 - val_acc: 0.0000e+00\n",
      "Epoch 78/90\n",
      "9763/9763 [==============================] - 0s 43us/step - loss: 9.6441e-04 - acc: 2.0486e-04 - val_loss: 2.2097e-04 - val_acc: 0.0000e+00\n",
      "Epoch 79/90\n",
      "9763/9763 [==============================] - 0s 42us/step - loss: 9.9895e-04 - acc: 2.0486e-04 - val_loss: 9.6849e-04 - val_acc: 0.0000e+00\n",
      "Epoch 80/90\n",
      "9763/9763 [==============================] - 0s 43us/step - loss: 9.8205e-04 - acc: 2.0486e-04 - val_loss: 5.6647e-04 - val_acc: 0.0000e+00\n",
      "Epoch 81/90\n",
      "9763/9763 [==============================] - 0s 44us/step - loss: 0.0010 - acc: 2.0486e-04 - val_loss: 3.9062e-04 - val_acc: 0.0000e+00\n",
      "Epoch 82/90\n",
      "9763/9763 [==============================] - 0s 44us/step - loss: 0.0010 - acc: 2.0486e-04 - val_loss: 5.1858e-04 - val_acc: 0.0000e+00\n",
      "Epoch 83/90\n",
      "9763/9763 [==============================] - 0s 44us/step - loss: 0.0011 - acc: 2.0486e-04 - val_loss: 2.8122e-04 - val_acc: 0.0000e+00\n",
      "Epoch 84/90\n",
      "9763/9763 [==============================] - 0s 44us/step - loss: 9.9437e-04 - acc: 2.0486e-04 - val_loss: 8.6845e-04 - val_acc: 0.0000e+00\n",
      "Epoch 85/90\n",
      "9763/9763 [==============================] - 0s 43us/step - loss: 0.0010 - acc: 2.0486e-04 - val_loss: 2.2290e-04 - val_acc: 0.0000e+00\n",
      "Epoch 86/90\n",
      "9763/9763 [==============================] - 0s 46us/step - loss: 9.2715e-04 - acc: 2.0486e-04 - val_loss: 3.1673e-04 - val_acc: 0.0000e+00\n",
      "Epoch 87/90\n",
      "9763/9763 [==============================] - 0s 44us/step - loss: 9.4536e-04 - acc: 2.0486e-04 - val_loss: 6.7578e-04 - val_acc: 0.0000e+00\n",
      "Epoch 88/90\n",
      "9763/9763 [==============================] - 0s 44us/step - loss: 9.6922e-04 - acc: 2.0486e-04 - val_loss: 4.2935e-04 - val_acc: 0.0000e+00\n",
      "Epoch 89/90\n",
      "9763/9763 [==============================] - 0s 48us/step - loss: 9.2113e-04 - acc: 2.0486e-04 - val_loss: 3.5723e-04 - val_acc: 0.0000e+00\n",
      "Epoch 90/90\n",
      "9763/9763 [==============================] - 0s 46us/step - loss: 9.0833e-04 - acc: 2.0486e-04 - val_loss: 3.3574e-04 - val_acc: 0.0000e+00\n",
      "-40.451155729494715\n",
      "Amount of features = 9\n",
      "Amount of training data = 14008\n",
      "Amount of testing data = 6004\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_11 (Conv1D)           (None, 21, 64)            1216      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 10, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 9, 64)             8256      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 4, 64)             0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 4, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 250)               64250     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 251       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 73,973\n",
      "Trainable params: 73,973\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 9789 samples, validate on 4196 samples\n",
      "Epoch 1/90\n",
      "9789/9789 [==============================] - 1s 105us/step - loss: 0.0647 - acc: 1.0216e-04 - val_loss: 0.0011 - val_acc: 7.1497e-04\n",
      "Epoch 2/90\n",
      "9789/9789 [==============================] - 0s 42us/step - loss: 0.0104 - acc: 2.0431e-04 - val_loss: 7.3174e-04 - val_acc: 7.1497e-04\n",
      "Epoch 3/90\n",
      "9789/9789 [==============================] - 0s 42us/step - loss: 0.0071 - acc: 2.0431e-04 - val_loss: 8.0284e-04 - val_acc: 7.1497e-04\n",
      "Epoch 4/90\n",
      "9789/9789 [==============================] - 0s 43us/step - loss: 0.0059 - acc: 2.0431e-04 - val_loss: 7.5567e-04 - val_acc: 7.1497e-04\n",
      "Epoch 5/90\n",
      "9789/9789 [==============================] - 0s 43us/step - loss: 0.0052 - acc: 2.0431e-04 - val_loss: 8.9901e-04 - val_acc: 7.1497e-04\n",
      "Epoch 6/90\n",
      "9789/9789 [==============================] - 0s 43us/step - loss: 0.0045 - acc: 2.0431e-04 - val_loss: 6.9674e-04 - val_acc: 7.1497e-04\n",
      "Epoch 7/90\n",
      "9789/9789 [==============================] - 0s 42us/step - loss: 0.0041 - acc: 2.0431e-04 - val_loss: 6.5747e-04 - val_acc: 7.1497e-04\n",
      "Epoch 8/90\n",
      "9789/9789 [==============================] - 0s 42us/step - loss: 0.0037 - acc: 2.0431e-04 - val_loss: 7.1071e-04 - val_acc: 7.1497e-04\n",
      "Epoch 9/90\n",
      "9789/9789 [==============================] - 0s 42us/step - loss: 0.0035 - acc: 2.0431e-04 - val_loss: 7.3025e-04 - val_acc: 7.1497e-04\n",
      "Epoch 10/90\n",
      "9789/9789 [==============================] - 0s 45us/step - loss: 0.0033 - acc: 2.0431e-04 - val_loss: 0.0011 - val_acc: 7.1497e-04\n",
      "Epoch 11/90\n",
      "9789/9789 [==============================] - 0s 43us/step - loss: 0.0031 - acc: 2.0431e-04 - val_loss: 0.0011 - val_acc: 7.1497e-04\n",
      "Epoch 12/90\n",
      "9789/9789 [==============================] - 0s 45us/step - loss: 0.0029 - acc: 2.0431e-04 - val_loss: 9.2801e-04 - val_acc: 7.1497e-04\n",
      "Epoch 13/90\n",
      "9789/9789 [==============================] - 0s 46us/step - loss: 0.0029 - acc: 2.0431e-04 - val_loss: 7.2459e-04 - val_acc: 7.1497e-04\n",
      "Epoch 14/90\n",
      "9789/9789 [==============================] - 0s 44us/step - loss: 0.0027 - acc: 2.0431e-04 - val_loss: 0.0010 - val_acc: 7.1497e-04\n",
      "Epoch 15/90\n",
      "9789/9789 [==============================] - 0s 43us/step - loss: 0.0026 - acc: 2.0431e-04 - val_loss: 8.5241e-04 - val_acc: 7.1497e-04\n",
      "Epoch 16/90\n",
      "9789/9789 [==============================] - 0s 47us/step - loss: 0.0026 - acc: 2.0431e-04 - val_loss: 8.9525e-04 - val_acc: 7.1497e-04\n",
      "Epoch 17/90\n",
      "9789/9789 [==============================] - 0s 50us/step - loss: 0.0026 - acc: 2.0431e-04 - val_loss: 7.7432e-04 - val_acc: 7.1497e-04\n",
      "Epoch 18/90\n",
      "9789/9789 [==============================] - 1s 51us/step - loss: 0.0024 - acc: 2.0431e-04 - val_loss: 8.6019e-04 - val_acc: 7.1497e-04\n",
      "Epoch 19/90\n",
      "9789/9789 [==============================] - 1s 61us/step - loss: 0.0024 - acc: 2.0431e-04 - val_loss: 8.1484e-04 - val_acc: 7.1497e-04\n",
      "Epoch 20/90\n",
      "9789/9789 [==============================] - 0s 49us/step - loss: 0.0023 - acc: 2.0431e-04 - val_loss: 9.3865e-04 - val_acc: 7.1497e-04\n",
      "Epoch 21/90\n",
      "9789/9789 [==============================] - 0s 45us/step - loss: 0.0022 - acc: 2.0431e-04 - val_loss: 7.9490e-04 - val_acc: 7.1497e-04\n",
      "Epoch 22/90\n",
      "9789/9789 [==============================] - 0s 44us/step - loss: 0.0022 - acc: 2.0431e-04 - val_loss: 7.8921e-04 - val_acc: 7.1497e-04\n",
      "Epoch 23/90\n",
      "9789/9789 [==============================] - 0s 43us/step - loss: 0.0021 - acc: 2.0431e-04 - val_loss: 0.0011 - val_acc: 7.1497e-04\n",
      "Epoch 24/90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9789/9789 [==============================] - 0s 42us/step - loss: 0.0022 - acc: 2.0431e-04 - val_loss: 8.2471e-04 - val_acc: 7.1497e-04\n",
      "Epoch 25/90\n",
      "9789/9789 [==============================] - 0s 42us/step - loss: 0.0022 - acc: 2.0431e-04 - val_loss: 9.8433e-04 - val_acc: 7.1497e-04\n",
      "Epoch 26/90\n",
      "9789/9789 [==============================] - 0s 42us/step - loss: 0.0024 - acc: 2.0431e-04 - val_loss: 0.0011 - val_acc: 7.1497e-04\n",
      "Epoch 27/90\n",
      "9789/9789 [==============================] - 0s 43us/step - loss: 0.0021 - acc: 2.0431e-04 - val_loss: 9.8686e-04 - val_acc: 7.1497e-04\n",
      "Epoch 28/90\n",
      "9789/9789 [==============================] - 0s 44us/step - loss: 0.0020 - acc: 2.0431e-04 - val_loss: 8.4303e-04 - val_acc: 7.1497e-04\n",
      "Epoch 29/90\n",
      "9789/9789 [==============================] - 0s 43us/step - loss: 0.0019 - acc: 2.0431e-04 - val_loss: 8.1499e-04 - val_acc: 7.1497e-04\n",
      "Epoch 30/90\n",
      "9789/9789 [==============================] - 0s 42us/step - loss: 0.0019 - acc: 2.0431e-04 - val_loss: 7.7818e-04 - val_acc: 7.1497e-04\n",
      "Epoch 31/90\n",
      "9789/9789 [==============================] - 0s 44us/step - loss: 0.0019 - acc: 2.0431e-04 - val_loss: 7.4743e-04 - val_acc: 7.1497e-04\n",
      "Epoch 32/90\n",
      "9789/9789 [==============================] - 0s 43us/step - loss: 0.0019 - acc: 2.0431e-04 - val_loss: 8.8827e-04 - val_acc: 7.1497e-04\n",
      "Epoch 33/90\n",
      "9789/9789 [==============================] - 0s 44us/step - loss: 0.0018 - acc: 2.0431e-04 - val_loss: 0.0011 - val_acc: 7.1497e-04\n",
      "Epoch 34/90\n",
      "9789/9789 [==============================] - 0s 44us/step - loss: 0.0019 - acc: 2.0431e-04 - val_loss: 7.5487e-04 - val_acc: 7.1497e-04\n",
      "Epoch 35/90\n",
      "9789/9789 [==============================] - 0s 46us/step - loss: 0.0018 - acc: 2.0431e-04 - val_loss: 0.0013 - val_acc: 7.1497e-04\n",
      "Epoch 36/90\n",
      "9789/9789 [==============================] - 0s 49us/step - loss: 0.0018 - acc: 2.0431e-04 - val_loss: 9.0426e-04 - val_acc: 7.1497e-04\n",
      "Epoch 37/90\n",
      "9789/9789 [==============================] - 0s 49us/step - loss: 0.0018 - acc: 2.0431e-04 - val_loss: 0.0012 - val_acc: 7.1497e-04\n",
      "Epoch 38/90\n",
      "9789/9789 [==============================] - 0s 48us/step - loss: 0.0018 - acc: 2.0431e-04 - val_loss: 0.0011 - val_acc: 7.1497e-04\n",
      "Epoch 39/90\n",
      "9789/9789 [==============================] - 0s 44us/step - loss: 0.0017 - acc: 2.0431e-04 - val_loss: 0.0013 - val_acc: 7.1497e-04\n",
      "Epoch 40/90\n",
      "9789/9789 [==============================] - 0s 49us/step - loss: 0.0018 - acc: 2.0431e-04 - val_loss: 0.0014 - val_acc: 7.1497e-04\n",
      "Epoch 41/90\n",
      "9789/9789 [==============================] - 0s 48us/step - loss: 0.0017 - acc: 2.0431e-04 - val_loss: 0.0016 - val_acc: 7.1497e-04\n",
      "Epoch 42/90\n",
      "9789/9789 [==============================] - 0s 45us/step - loss: 0.0016 - acc: 2.0431e-04 - val_loss: 0.0012 - val_acc: 7.1497e-04\n",
      "Epoch 43/90\n",
      "9789/9789 [==============================] - 0s 44us/step - loss: 0.0016 - acc: 2.0431e-04 - val_loss: 0.0014 - val_acc: 7.1497e-04\n",
      "Epoch 44/90\n",
      "9789/9789 [==============================] - 0s 44us/step - loss: 0.0016 - acc: 2.0431e-04 - val_loss: 0.0013 - val_acc: 7.1497e-04\n",
      "Epoch 45/90\n",
      "9789/9789 [==============================] - 0s 43us/step - loss: 0.0016 - acc: 2.0431e-04 - val_loss: 0.0013 - val_acc: 7.1497e-04\n",
      "Epoch 46/90\n",
      "9789/9789 [==============================] - 0s 42us/step - loss: 0.0015 - acc: 2.0431e-04 - val_loss: 0.0011 - val_acc: 7.1497e-04\n",
      "Epoch 47/90\n",
      "9789/9789 [==============================] - 0s 45us/step - loss: 0.0016 - acc: 2.0431e-04 - val_loss: 9.9485e-04 - val_acc: 7.1497e-04\n",
      "Epoch 48/90\n",
      "9789/9789 [==============================] - 0s 43us/step - loss: 0.0016 - acc: 2.0431e-04 - val_loss: 0.0011 - val_acc: 7.1497e-04\n",
      "Epoch 49/90\n",
      "9789/9789 [==============================] - 0s 45us/step - loss: 0.0016 - acc: 2.0431e-04 - val_loss: 8.8997e-04 - val_acc: 7.1497e-04\n",
      "Epoch 50/90\n",
      "9789/9789 [==============================] - 0s 45us/step - loss: 0.0015 - acc: 2.0431e-04 - val_loss: 8.8588e-04 - val_acc: 7.1497e-04\n",
      "Epoch 51/90\n",
      "9789/9789 [==============================] - 0s 43us/step - loss: 0.0015 - acc: 2.0431e-04 - val_loss: 7.9830e-04 - val_acc: 7.1497e-04\n",
      "Epoch 52/90\n",
      "9789/9789 [==============================] - 0s 43us/step - loss: 0.0015 - acc: 2.0431e-04 - val_loss: 9.9066e-04 - val_acc: 7.1497e-04\n",
      "Epoch 53/90\n",
      "9789/9789 [==============================] - 0s 44us/step - loss: 0.0015 - acc: 2.0431e-04 - val_loss: 0.0011 - val_acc: 7.1497e-04\n",
      "Epoch 54/90\n",
      "9789/9789 [==============================] - 0s 44us/step - loss: 0.0016 - acc: 2.0431e-04 - val_loss: 0.0011 - val_acc: 7.1497e-04\n",
      "Epoch 55/90\n",
      "9789/9789 [==============================] - 0s 43us/step - loss: 0.0014 - acc: 2.0431e-04 - val_loss: 0.0013 - val_acc: 7.1497e-04\n",
      "Epoch 56/90\n",
      "9789/9789 [==============================] - 0s 44us/step - loss: 0.0015 - acc: 2.0431e-04 - val_loss: 0.0011 - val_acc: 7.1497e-04\n",
      "Epoch 57/90\n",
      "9789/9789 [==============================] - 0s 42us/step - loss: 0.0014 - acc: 2.0431e-04 - val_loss: 0.0011 - val_acc: 7.1497e-04\n",
      "Epoch 58/90\n",
      "9789/9789 [==============================] - 0s 44us/step - loss: 0.0014 - acc: 2.0431e-04 - val_loss: 0.0012 - val_acc: 7.1497e-04\n",
      "Epoch 59/90\n",
      "9789/9789 [==============================] - 0s 44us/step - loss: 0.0014 - acc: 2.0431e-04 - val_loss: 0.0012 - val_acc: 7.1497e-04\n",
      "Epoch 60/90\n",
      "9789/9789 [==============================] - 0s 43us/step - loss: 0.0014 - acc: 2.0431e-04 - val_loss: 0.0011 - val_acc: 7.1497e-04\n",
      "Epoch 61/90\n",
      "9789/9789 [==============================] - 0s 43us/step - loss: 0.0015 - acc: 2.0431e-04 - val_loss: 0.0014 - val_acc: 7.1497e-04\n",
      "Epoch 62/90\n",
      "9789/9789 [==============================] - 0s 44us/step - loss: 0.0014 - acc: 2.0431e-04 - val_loss: 0.0013 - val_acc: 7.1497e-04\n",
      "Epoch 63/90\n",
      "9789/9789 [==============================] - 0s 48us/step - loss: 0.0014 - acc: 2.0431e-04 - val_loss: 0.0017 - val_acc: 7.1497e-04\n",
      "Epoch 64/90\n",
      "9789/9789 [==============================] - 0s 46us/step - loss: 0.0013 - acc: 2.0431e-04 - val_loss: 0.0012 - val_acc: 7.1497e-04\n",
      "Epoch 65/90\n",
      "9789/9789 [==============================] - 0s 46us/step - loss: 0.0014 - acc: 2.0431e-04 - val_loss: 0.0014 - val_acc: 7.1497e-04\n",
      "Epoch 66/90\n",
      "9789/9789 [==============================] - 0s 43us/step - loss: 0.0014 - acc: 2.0431e-04 - val_loss: 0.0016 - val_acc: 7.1497e-04\n",
      "Epoch 67/90\n",
      "9789/9789 [==============================] - 0s 45us/step - loss: 0.0014 - acc: 2.0431e-04 - val_loss: 0.0011 - val_acc: 7.1497e-04\n",
      "Epoch 68/90\n",
      "9789/9789 [==============================] - 0s 44us/step - loss: 0.0013 - acc: 2.0431e-04 - val_loss: 0.0011 - val_acc: 7.1497e-04\n",
      "Epoch 69/90\n",
      "9789/9789 [==============================] - 0s 45us/step - loss: 0.0013 - acc: 2.0431e-04 - val_loss: 0.0011 - val_acc: 7.1497e-04\n",
      "Epoch 70/90\n",
      "9789/9789 [==============================] - 0s 51us/step - loss: 0.0013 - acc: 2.0431e-04 - val_loss: 0.0011 - val_acc: 7.1497e-04\n",
      "Epoch 71/90\n",
      "9789/9789 [==============================] - 0s 45us/step - loss: 0.0013 - acc: 2.0431e-04 - val_loss: 0.0011 - val_acc: 7.1497e-04\n",
      "Epoch 72/90\n",
      "9789/9789 [==============================] - 0s 44us/step - loss: 0.0013 - acc: 2.0431e-04 - val_loss: 0.0011 - val_acc: 7.1497e-04\n",
      "Epoch 73/90\n",
      "9789/9789 [==============================] - 0s 46us/step - loss: 0.0013 - acc: 2.0431e-04 - val_loss: 0.0011 - val_acc: 7.1497e-04\n",
      "Epoch 74/90\n",
      "9789/9789 [==============================] - 0s 45us/step - loss: 0.0012 - acc: 2.0431e-04 - val_loss: 0.0013 - val_acc: 7.1497e-04\n",
      "Epoch 75/90\n",
      "9789/9789 [==============================] - 0s 45us/step - loss: 0.0013 - acc: 2.0431e-04 - val_loss: 0.0012 - val_acc: 7.1497e-04\n",
      "Epoch 76/90\n",
      "9789/9789 [==============================] - 0s 44us/step - loss: 0.0012 - acc: 2.0431e-04 - val_loss: 0.0018 - val_acc: 7.1497e-04\n",
      "Epoch 77/90\n",
      "9789/9789 [==============================] - 0s 43us/step - loss: 0.0012 - acc: 2.0431e-04 - val_loss: 0.0016 - val_acc: 7.1497e-04\n",
      "Epoch 78/90\n",
      "9789/9789 [==============================] - 0s 44us/step - loss: 0.0012 - acc: 2.0431e-04 - val_loss: 0.0012 - val_acc: 7.1497e-04\n",
      "Epoch 79/90\n",
      "9789/9789 [==============================] - 0s 44us/step - loss: 0.0013 - acc: 2.0431e-04 - val_loss: 0.0013 - val_acc: 7.1497e-04\n",
      "Epoch 80/90\n",
      "9789/9789 [==============================] - 0s 43us/step - loss: 0.0012 - acc: 2.0431e-04 - val_loss: 0.0012 - val_acc: 7.1497e-04\n",
      "Epoch 81/90\n",
      "9789/9789 [==============================] - 0s 44us/step - loss: 0.0012 - acc: 2.0431e-04 - val_loss: 0.0013 - val_acc: 7.1497e-04\n",
      "Epoch 82/90\n",
      "9789/9789 [==============================] - 0s 44us/step - loss: 0.0011 - acc: 2.0431e-04 - val_loss: 0.0017 - val_acc: 7.1497e-04\n",
      "Epoch 83/90\n",
      "9789/9789 [==============================] - 0s 45us/step - loss: 0.0012 - acc: 2.0431e-04 - val_loss: 0.0015 - val_acc: 7.1497e-04\n",
      "Epoch 84/90\n",
      "9789/9789 [==============================] - 0s 44us/step - loss: 0.0011 - acc: 2.0431e-04 - val_loss: 0.0015 - val_acc: 7.1497e-04\n",
      "Epoch 85/90\n",
      "9789/9789 [==============================] - 0s 44us/step - loss: 0.0012 - acc: 2.0431e-04 - val_loss: 0.0014 - val_acc: 7.1497e-04\n",
      "Epoch 86/90\n",
      "9789/9789 [==============================] - 0s 44us/step - loss: 0.0012 - acc: 2.0431e-04 - val_loss: 0.0016 - val_acc: 7.1497e-04\n",
      "Epoch 87/90\n",
      "9789/9789 [==============================] - 0s 42us/step - loss: 0.0011 - acc: 2.0431e-04 - val_loss: 0.0017 - val_acc: 7.1497e-04\n",
      "Epoch 88/90\n",
      "9789/9789 [==============================] - 0s 45us/step - loss: 0.0012 - acc: 2.0431e-04 - val_loss: 0.0015 - val_acc: 7.1497e-04\n",
      "Epoch 89/90\n",
      "9789/9789 [==============================] - 0s 43us/step - loss: 0.0012 - acc: 2.0431e-04 - val_loss: 0.0012 - val_acc: 7.1497e-04\n",
      "Epoch 90/90\n",
      "9789/9789 [==============================] - 0s 43us/step - loss: 0.0011 - acc: 2.0431e-04 - val_loss: 0.0019 - val_acc: 7.1497e-04\n",
      "-44.504524126221725\n",
      "Amount of features = 9\n",
      "Amount of training data = 14001\n",
      "Amount of testing data = 6001\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_13 (Conv1D)           (None, 21, 64)            1216      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 10, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 9, 64)             8256      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 4, 64)             0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 4, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 250)               64250     \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 251       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 73,973\n",
      "Trainable params: 73,973\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 9784 samples, validate on 4194 samples\n",
      "Epoch 1/90\n",
      "9784/9784 [==============================] - 1s 111us/step - loss: 0.0400 - acc: 1.0221e-04 - val_loss: 0.0042 - val_acc: 2.3844e-04\n",
      "Epoch 2/90\n",
      "9784/9784 [==============================] - 0s 44us/step - loss: 0.0077 - acc: 1.0221e-04 - val_loss: 4.9899e-04 - val_acc: 2.3844e-04\n",
      "Epoch 3/90\n",
      "9784/9784 [==============================] - 0s 42us/step - loss: 0.0046 - acc: 1.0221e-04 - val_loss: 0.0020 - val_acc: 2.3844e-04\n",
      "Epoch 4/90\n",
      "9784/9784 [==============================] - 0s 44us/step - loss: 0.0036 - acc: 1.0221e-04 - val_loss: 0.0013 - val_acc: 2.3844e-04\n",
      "Epoch 5/90\n",
      "9784/9784 [==============================] - 0s 43us/step - loss: 0.0030 - acc: 1.0221e-04 - val_loss: 5.9408e-04 - val_acc: 2.3844e-04\n",
      "Epoch 6/90\n",
      "9784/9784 [==============================] - 0s 42us/step - loss: 0.0028 - acc: 1.0221e-04 - val_loss: 7.8743e-04 - val_acc: 2.3844e-04\n",
      "Epoch 7/90\n",
      "9784/9784 [==============================] - 0s 44us/step - loss: 0.0025 - acc: 1.0221e-04 - val_loss: 3.9616e-04 - val_acc: 2.3844e-04\n",
      "Epoch 8/90\n",
      "9784/9784 [==============================] - 0s 43us/step - loss: 0.0022 - acc: 1.0221e-04 - val_loss: 5.6517e-04 - val_acc: 2.3844e-04\n",
      "Epoch 9/90\n",
      "9784/9784 [==============================] - 0s 44us/step - loss: 0.0020 - acc: 1.0221e-04 - val_loss: 3.8707e-04 - val_acc: 2.3844e-04\n",
      "Epoch 10/90\n",
      "9784/9784 [==============================] - 0s 43us/step - loss: 0.0019 - acc: 1.0221e-04 - val_loss: 4.9799e-04 - val_acc: 2.3844e-04\n",
      "Epoch 11/90\n",
      "9784/9784 [==============================] - 0s 44us/step - loss: 0.0018 - acc: 1.0221e-04 - val_loss: 4.9276e-04 - val_acc: 2.3844e-04\n",
      "Epoch 12/90\n",
      "9784/9784 [==============================] - 0s 44us/step - loss: 0.0017 - acc: 1.0221e-04 - val_loss: 7.0799e-04 - val_acc: 2.3844e-04\n",
      "Epoch 13/90\n",
      "9784/9784 [==============================] - 0s 44us/step - loss: 0.0017 - acc: 1.0221e-04 - val_loss: 3.3671e-04 - val_acc: 2.3844e-04\n",
      "Epoch 14/90\n",
      "9784/9784 [==============================] - 0s 43us/step - loss: 0.0016 - acc: 1.0221e-04 - val_loss: 7.2088e-04 - val_acc: 2.3844e-04\n",
      "Epoch 15/90\n",
      "9784/9784 [==============================] - 0s 43us/step - loss: 0.0015 - acc: 1.0221e-04 - val_loss: 0.0012 - val_acc: 2.3844e-04\n",
      "Epoch 16/90\n",
      "9784/9784 [==============================] - 0s 43us/step - loss: 0.0015 - acc: 1.0221e-04 - val_loss: 4.4580e-04 - val_acc: 2.3844e-04\n",
      "Epoch 17/90\n",
      "9784/9784 [==============================] - 0s 43us/step - loss: 0.0015 - acc: 1.0221e-04 - val_loss: 4.0158e-04 - val_acc: 2.3844e-04\n",
      "Epoch 18/90\n",
      "9784/9784 [==============================] - 0s 45us/step - loss: 0.0014 - acc: 1.0221e-04 - val_loss: 3.7861e-04 - val_acc: 2.3844e-04\n",
      "Epoch 19/90\n",
      "9784/9784 [==============================] - 0s 45us/step - loss: 0.0014 - acc: 1.0221e-04 - val_loss: 3.5754e-04 - val_acc: 2.3844e-04\n",
      "Epoch 20/90\n",
      "9784/9784 [==============================] - 0s 44us/step - loss: 0.0013 - acc: 1.0221e-04 - val_loss: 3.6776e-04 - val_acc: 2.3844e-04\n",
      "Epoch 21/90\n",
      "9784/9784 [==============================] - 0s 45us/step - loss: 0.0013 - acc: 1.0221e-04 - val_loss: 3.5030e-04 - val_acc: 2.3844e-04\n",
      "Epoch 22/90\n",
      "9784/9784 [==============================] - 0s 44us/step - loss: 0.0013 - acc: 1.0221e-04 - val_loss: 0.0011 - val_acc: 2.3844e-04\n",
      "Epoch 23/90\n",
      "9784/9784 [==============================] - 0s 44us/step - loss: 0.0013 - acc: 1.0221e-04 - val_loss: 8.9290e-04 - val_acc: 2.3844e-04\n",
      "Epoch 24/90\n",
      "9784/9784 [==============================] - 0s 43us/step - loss: 0.0013 - acc: 1.0221e-04 - val_loss: 6.5352e-04 - val_acc: 2.3844e-04\n",
      "Epoch 25/90\n",
      "9784/9784 [==============================] - 0s 43us/step - loss: 0.0013 - acc: 1.0221e-04 - val_loss: 7.7774e-04 - val_acc: 2.3844e-04\n",
      "Epoch 26/90\n",
      "9784/9784 [==============================] - 0s 43us/step - loss: 0.0013 - acc: 1.0221e-04 - val_loss: 3.6334e-04 - val_acc: 2.3844e-04\n",
      "Epoch 27/90\n",
      "9784/9784 [==============================] - 0s 44us/step - loss: 0.0012 - acc: 1.0221e-04 - val_loss: 3.8513e-04 - val_acc: 2.3844e-04\n",
      "Epoch 28/90\n",
      "9784/9784 [==============================] - 0s 43us/step - loss: 0.0012 - acc: 1.0221e-04 - val_loss: 3.8088e-04 - val_acc: 2.3844e-04\n",
      "Epoch 29/90\n",
      "9784/9784 [==============================] - 0s 43us/step - loss: 0.0012 - acc: 1.0221e-04 - val_loss: 8.5588e-04 - val_acc: 2.3844e-04\n",
      "Epoch 30/90\n",
      "9784/9784 [==============================] - 0s 42us/step - loss: 0.0012 - acc: 1.0221e-04 - val_loss: 3.5997e-04 - val_acc: 2.3844e-04\n",
      "Epoch 31/90\n",
      "9784/9784 [==============================] - 0s 44us/step - loss: 0.0011 - acc: 1.0221e-04 - val_loss: 4.4924e-04 - val_acc: 2.3844e-04\n",
      "Epoch 32/90\n",
      "9784/9784 [==============================] - 0s 44us/step - loss: 0.0011 - acc: 1.0221e-04 - val_loss: 4.2060e-04 - val_acc: 2.3844e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/90\n",
      "9784/9784 [==============================] - 0s 43us/step - loss: 0.0011 - acc: 1.0221e-04 - val_loss: 3.2347e-04 - val_acc: 2.3844e-04\n",
      "Epoch 34/90\n",
      "9784/9784 [==============================] - 0s 43us/step - loss: 0.0011 - acc: 1.0221e-04 - val_loss: 3.4366e-04 - val_acc: 2.3844e-04\n",
      "Epoch 35/90\n",
      "9784/9784 [==============================] - 0s 42us/step - loss: 0.0011 - acc: 1.0221e-04 - val_loss: 3.5911e-04 - val_acc: 2.3844e-04\n",
      "Epoch 36/90\n",
      "9784/9784 [==============================] - 0s 43us/step - loss: 0.0011 - acc: 1.0221e-04 - val_loss: 9.6026e-04 - val_acc: 2.3844e-04\n",
      "Epoch 37/90\n",
      "9784/9784 [==============================] - 0s 43us/step - loss: 0.0011 - acc: 1.0221e-04 - val_loss: 4.5010e-04 - val_acc: 2.3844e-04\n",
      "Epoch 38/90\n",
      "9784/9784 [==============================] - 0s 43us/step - loss: 0.0010 - acc: 1.0221e-04 - val_loss: 7.0683e-04 - val_acc: 2.3844e-04\n",
      "Epoch 39/90\n",
      "9784/9784 [==============================] - 0s 43us/step - loss: 0.0010 - acc: 1.0221e-04 - val_loss: 4.9456e-04 - val_acc: 2.3844e-04\n",
      "Epoch 40/90\n",
      "9784/9784 [==============================] - 0s 42us/step - loss: 0.0010 - acc: 1.0221e-04 - val_loss: 7.2921e-04 - val_acc: 2.3844e-04\n",
      "Epoch 41/90\n",
      "9784/9784 [==============================] - 0s 44us/step - loss: 0.0010 - acc: 1.0221e-04 - val_loss: 4.7152e-04 - val_acc: 2.3844e-04\n",
      "Epoch 42/90\n",
      "9784/9784 [==============================] - 0s 45us/step - loss: 0.0010 - acc: 1.0221e-04 - val_loss: 4.0348e-04 - val_acc: 2.3844e-04\n",
      "Epoch 43/90\n",
      "9784/9784 [==============================] - 0s 47us/step - loss: 0.0010 - acc: 1.0221e-04 - val_loss: 4.8862e-04 - val_acc: 2.3844e-04\n",
      "Epoch 44/90\n",
      "9784/9784 [==============================] - 0s 46us/step - loss: 0.0010 - acc: 1.0221e-04 - val_loss: 7.9191e-04 - val_acc: 2.3844e-04\n",
      "Epoch 45/90\n",
      "9784/9784 [==============================] - 0s 43us/step - loss: 0.0011 - acc: 1.0221e-04 - val_loss: 7.8503e-04 - val_acc: 2.3844e-04\n",
      "Epoch 46/90\n",
      "9784/9784 [==============================] - 0s 45us/step - loss: 0.0010 - acc: 1.0221e-04 - val_loss: 3.8992e-04 - val_acc: 2.3844e-04\n",
      "Epoch 47/90\n",
      "9784/9784 [==============================] - 0s 45us/step - loss: 0.0011 - acc: 1.0221e-04 - val_loss: 5.5466e-04 - val_acc: 2.3844e-04\n",
      "Epoch 48/90\n",
      "9784/9784 [==============================] - 0s 48us/step - loss: 9.9139e-04 - acc: 1.0221e-04 - val_loss: 4.9587e-04 - val_acc: 2.3844e-04\n",
      "Epoch 49/90\n",
      "9784/9784 [==============================] - 0s 46us/step - loss: 9.7913e-04 - acc: 1.0221e-04 - val_loss: 0.0010 - val_acc: 2.3844e-04\n",
      "Epoch 50/90\n",
      "9784/9784 [==============================] - 0s 44us/step - loss: 9.5080e-04 - acc: 1.0221e-04 - val_loss: 4.3506e-04 - val_acc: 2.3844e-04\n",
      "Epoch 51/90\n",
      "9784/9784 [==============================] - 0s 43us/step - loss: 9.4766e-04 - acc: 1.0221e-04 - val_loss: 4.0772e-04 - val_acc: 2.3844e-04\n",
      "Epoch 52/90\n",
      "9784/9784 [==============================] - 0s 43us/step - loss: 9.0450e-04 - acc: 1.0221e-04 - val_loss: 3.6436e-04 - val_acc: 2.3844e-04\n",
      "Epoch 53/90\n",
      "9784/9784 [==============================] - 0s 44us/step - loss: 8.7632e-04 - acc: 1.0221e-04 - val_loss: 5.6751e-04 - val_acc: 2.3844e-04\n",
      "Epoch 54/90\n",
      "9784/9784 [==============================] - 0s 46us/step - loss: 8.8230e-04 - acc: 1.0221e-04 - val_loss: 8.8940e-04 - val_acc: 2.3844e-04\n",
      "Epoch 55/90\n",
      "9784/9784 [==============================] - 0s 45us/step - loss: 8.9737e-04 - acc: 1.0221e-04 - val_loss: 0.0011 - val_acc: 2.3844e-04\n",
      "Epoch 56/90\n",
      "9784/9784 [==============================] - 0s 43us/step - loss: 9.2935e-04 - acc: 1.0221e-04 - val_loss: 4.2732e-04 - val_acc: 2.3844e-04\n",
      "Epoch 57/90\n",
      "9784/9784 [==============================] - 0s 43us/step - loss: 8.5932e-04 - acc: 1.0221e-04 - val_loss: 5.8725e-04 - val_acc: 2.3844e-04\n",
      "Epoch 58/90\n",
      "9784/9784 [==============================] - 0s 43us/step - loss: 8.6167e-04 - acc: 1.0221e-04 - val_loss: 3.7932e-04 - val_acc: 2.3844e-04\n",
      "Epoch 59/90\n",
      "9784/9784 [==============================] - 0s 43us/step - loss: 8.6756e-04 - acc: 1.0221e-04 - val_loss: 4.2871e-04 - val_acc: 2.3844e-04\n",
      "Epoch 60/90\n",
      "9784/9784 [==============================] - 0s 44us/step - loss: 8.8056e-04 - acc: 1.0221e-04 - val_loss: 4.5158e-04 - val_acc: 2.3844e-04\n",
      "Epoch 61/90\n",
      "9784/9784 [==============================] - 0s 44us/step - loss: 8.5845e-04 - acc: 1.0221e-04 - val_loss: 4.2054e-04 - val_acc: 2.3844e-04\n",
      "Epoch 62/90\n",
      "9784/9784 [==============================] - 0s 43us/step - loss: 8.4722e-04 - acc: 1.0221e-04 - val_loss: 6.2460e-04 - val_acc: 2.3844e-04\n",
      "Epoch 63/90\n",
      "9784/9784 [==============================] - 0s 44us/step - loss: 8.6030e-04 - acc: 1.0221e-04 - val_loss: 4.2596e-04 - val_acc: 2.3844e-04\n",
      "Epoch 64/90\n",
      "9784/9784 [==============================] - 0s 44us/step - loss: 8.2362e-04 - acc: 1.0221e-04 - val_loss: 5.2329e-04 - val_acc: 2.3844e-04\n",
      "Epoch 65/90\n",
      "9784/9784 [==============================] - 0s 43us/step - loss: 8.5319e-04 - acc: 1.0221e-04 - val_loss: 0.0013 - val_acc: 2.3844e-04\n",
      "Epoch 66/90\n",
      "9784/9784 [==============================] - 0s 46us/step - loss: 8.7675e-04 - acc: 1.0221e-04 - val_loss: 5.7464e-04 - val_acc: 2.3844e-04\n",
      "Epoch 67/90\n",
      "9784/9784 [==============================] - 0s 43us/step - loss: 7.7489e-04 - acc: 1.0221e-04 - val_loss: 4.4168e-04 - val_acc: 2.3844e-04\n",
      "Epoch 68/90\n",
      "9784/9784 [==============================] - 0s 45us/step - loss: 7.7805e-04 - acc: 1.0221e-04 - val_loss: 9.0761e-04 - val_acc: 2.3844e-04\n",
      "Epoch 69/90\n",
      "9784/9784 [==============================] - 0s 50us/step - loss: 8.4106e-04 - acc: 1.0221e-04 - val_loss: 5.9146e-04 - val_acc: 2.3844e-04\n",
      "Epoch 70/90\n",
      "9784/9784 [==============================] - 0s 47us/step - loss: 8.4239e-04 - acc: 1.0221e-04 - val_loss: 5.2257e-04 - val_acc: 2.3844e-04\n",
      "Epoch 71/90\n",
      "9784/9784 [==============================] - 0s 44us/step - loss: 8.0199e-04 - acc: 1.0221e-04 - val_loss: 0.0012 - val_acc: 2.3844e-04\n",
      "Epoch 72/90\n",
      "9784/9784 [==============================] - 0s 43us/step - loss: 8.0890e-04 - acc: 1.0221e-04 - val_loss: 5.2164e-04 - val_acc: 2.3844e-04\n",
      "Epoch 73/90\n",
      "9784/9784 [==============================] - 0s 50us/step - loss: 7.4056e-04 - acc: 1.0221e-04 - val_loss: 3.3200e-04 - val_acc: 2.3844e-04\n",
      "Epoch 74/90\n",
      "9784/9784 [==============================] - 0s 48us/step - loss: 7.3791e-04 - acc: 1.0221e-04 - val_loss: 3.9468e-04 - val_acc: 2.3844e-04\n",
      "Epoch 75/90\n",
      "9784/9784 [==============================] - 0s 43us/step - loss: 7.4819e-04 - acc: 1.0221e-04 - val_loss: 4.9198e-04 - val_acc: 2.3844e-04\n",
      "Epoch 76/90\n",
      "9784/9784 [==============================] - 0s 43us/step - loss: 7.4730e-04 - acc: 1.0221e-04 - val_loss: 5.1468e-04 - val_acc: 2.3844e-04\n",
      "Epoch 77/90\n",
      "9784/9784 [==============================] - 0s 44us/step - loss: 7.3849e-04 - acc: 1.0221e-04 - val_loss: 3.6017e-04 - val_acc: 2.3844e-04\n",
      "Epoch 78/90\n",
      "9784/9784 [==============================] - 0s 42us/step - loss: 7.4791e-04 - acc: 1.0221e-04 - val_loss: 5.5913e-04 - val_acc: 2.3844e-04\n",
      "Epoch 79/90\n",
      "9784/9784 [==============================] - 0s 43us/step - loss: 7.3179e-04 - acc: 1.0221e-04 - val_loss: 7.0933e-04 - val_acc: 2.3844e-04\n",
      "Epoch 80/90\n",
      "9784/9784 [==============================] - 0s 43us/step - loss: 7.7365e-04 - acc: 1.0221e-04 - val_loss: 7.7086e-04 - val_acc: 2.3844e-04\n",
      "Epoch 81/90\n",
      "9784/9784 [==============================] - 0s 43us/step - loss: 7.6139e-04 - acc: 1.0221e-04 - val_loss: 7.5322e-04 - val_acc: 2.3844e-04\n",
      "Epoch 82/90\n",
      "9784/9784 [==============================] - 0s 44us/step - loss: 7.4538e-04 - acc: 1.0221e-04 - val_loss: 4.7028e-04 - val_acc: 2.3844e-04\n",
      "Epoch 83/90\n",
      "9784/9784 [==============================] - 0s 43us/step - loss: 6.8438e-04 - acc: 1.0221e-04 - val_loss: 3.2724e-04 - val_acc: 2.3844e-04\n",
      "Epoch 84/90\n",
      "9784/9784 [==============================] - 0s 43us/step - loss: 6.9875e-04 - acc: 1.0221e-04 - val_loss: 3.8605e-04 - val_acc: 2.3844e-04\n",
      "Epoch 85/90\n",
      "9784/9784 [==============================] - 0s 43us/step - loss: 6.9874e-04 - acc: 1.0221e-04 - val_loss: 4.1817e-04 - val_acc: 2.3844e-04\n",
      "Epoch 86/90\n",
      "9784/9784 [==============================] - 0s 44us/step - loss: 6.9110e-04 - acc: 1.0221e-04 - val_loss: 3.6951e-04 - val_acc: 2.3844e-04\n",
      "Epoch 87/90\n",
      "9784/9784 [==============================] - 0s 43us/step - loss: 6.6512e-04 - acc: 1.0221e-04 - val_loss: 4.0585e-04 - val_acc: 2.3844e-04\n",
      "Epoch 88/90\n",
      "9784/9784 [==============================] - 0s 42us/step - loss: 7.1844e-04 - acc: 1.0221e-04 - val_loss: 4.3511e-04 - val_acc: 2.3844e-04\n",
      "Epoch 89/90\n",
      "9784/9784 [==============================] - 0s 45us/step - loss: 6.7784e-04 - acc: 1.0221e-04 - val_loss: 3.4436e-04 - val_acc: 2.3844e-04\n",
      "Epoch 90/90\n",
      "9784/9784 [==============================] - 0s 44us/step - loss: 6.7360e-04 - acc: 1.0221e-04 - val_loss: 4.0780e-04 - val_acc: 2.3844e-04\n",
      "-49.737433039913\n",
      "Amount of features = 9\n",
      "Amount of training data = 14018\n",
      "Amount of testing data = 6007\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_15 (Conv1D)           (None, 21, 64)            1216      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 10, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 9, 64)             8256      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 4, 64)             0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 4, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 250)               64250     \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1)                 251       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 73,973\n",
      "Trainable params: 73,973\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 9796 samples, validate on 4199 samples\n",
      "Epoch 1/90\n",
      "9796/9796 [==============================] - 1s 118us/step - loss: 0.0497 - acc: 1.0208e-04 - val_loss: 0.0043 - val_acc: 4.7630e-04\n",
      "Epoch 2/90\n",
      "9796/9796 [==============================] - 0s 44us/step - loss: 0.0115 - acc: 1.0208e-04 - val_loss: 0.0013 - val_acc: 4.7630e-04\n",
      "Epoch 3/90\n",
      "9796/9796 [==============================] - 0s 43us/step - loss: 0.0076 - acc: 1.0208e-04 - val_loss: 0.0021 - val_acc: 4.7630e-04\n",
      "Epoch 4/90\n",
      "9796/9796 [==============================] - 0s 44us/step - loss: 0.0060 - acc: 1.0208e-04 - val_loss: 0.0030 - val_acc: 4.7630e-04\n",
      "Epoch 5/90\n",
      "9796/9796 [==============================] - 0s 44us/step - loss: 0.0052 - acc: 1.0208e-04 - val_loss: 0.0036 - val_acc: 4.7630e-04\n",
      "Epoch 6/90\n",
      "9796/9796 [==============================] - 0s 43us/step - loss: 0.0042 - acc: 1.0208e-04 - val_loss: 0.0036 - val_acc: 4.7630e-04\n",
      "Epoch 7/90\n",
      "9796/9796 [==============================] - 0s 43us/step - loss: 0.0039 - acc: 1.0208e-04 - val_loss: 0.0042 - val_acc: 4.7630e-04\n",
      "Epoch 8/90\n",
      "9796/9796 [==============================] - 0s 42us/step - loss: 0.0036 - acc: 1.0208e-04 - val_loss: 0.0059 - val_acc: 4.7630e-04\n",
      "Epoch 9/90\n",
      "9796/9796 [==============================] - 0s 44us/step - loss: 0.0034 - acc: 1.0208e-04 - val_loss: 0.0065 - val_acc: 4.7630e-04\n",
      "Epoch 10/90\n",
      "9796/9796 [==============================] - 0s 44us/step - loss: 0.0032 - acc: 1.0208e-04 - val_loss: 0.0071 - val_acc: 4.7630e-04\n",
      "Epoch 11/90\n",
      "9796/9796 [==============================] - 0s 44us/step - loss: 0.0031 - acc: 1.0208e-04 - val_loss: 0.0066 - val_acc: 4.7630e-04\n",
      "Epoch 12/90\n",
      "9796/9796 [==============================] - 0s 44us/step - loss: 0.0029 - acc: 1.0208e-04 - val_loss: 0.0065 - val_acc: 4.7630e-04\n",
      "Epoch 13/90\n",
      "9796/9796 [==============================] - 0s 44us/step - loss: 0.0029 - acc: 1.0208e-04 - val_loss: 0.0085 - val_acc: 4.7630e-04\n",
      "Epoch 14/90\n",
      "9796/9796 [==============================] - 0s 46us/step - loss: 0.0028 - acc: 1.0208e-04 - val_loss: 0.0092 - val_acc: 4.7630e-04\n",
      "Epoch 15/90\n",
      "9796/9796 [==============================] - 0s 45us/step - loss: 0.0026 - acc: 1.0208e-04 - val_loss: 0.0086 - val_acc: 4.7630e-04\n",
      "Epoch 16/90\n",
      "9796/9796 [==============================] - 0s 46us/step - loss: 0.0027 - acc: 1.0208e-04 - val_loss: 0.0121 - val_acc: 4.7630e-04\n",
      "Epoch 17/90\n",
      "9796/9796 [==============================] - 0s 44us/step - loss: 0.0025 - acc: 1.0208e-04 - val_loss: 0.0113 - val_acc: 4.7630e-04\n",
      "Epoch 18/90\n",
      "9796/9796 [==============================] - 0s 45us/step - loss: 0.0024 - acc: 1.0208e-04 - val_loss: 0.0112 - val_acc: 4.7630e-04\n",
      "Epoch 19/90\n",
      "9796/9796 [==============================] - 0s 45us/step - loss: 0.0024 - acc: 1.0208e-04 - val_loss: 0.0128 - val_acc: 4.7630e-04\n",
      "Epoch 20/90\n",
      "9796/9796 [==============================] - 0s 46us/step - loss: 0.0024 - acc: 1.0208e-04 - val_loss: 0.0126 - val_acc: 4.7630e-04\n",
      "Epoch 21/90\n",
      "9796/9796 [==============================] - 0s 44us/step - loss: 0.0023 - acc: 1.0208e-04 - val_loss: 0.0143 - val_acc: 4.7630e-04\n",
      "Epoch 22/90\n",
      "9796/9796 [==============================] - 0s 47us/step - loss: 0.0022 - acc: 1.0208e-04 - val_loss: 0.0152 - val_acc: 4.7630e-04\n",
      "Epoch 23/90\n",
      "9796/9796 [==============================] - 0s 46us/step - loss: 0.0021 - acc: 1.0208e-04 - val_loss: 0.0148 - val_acc: 4.7630e-04\n",
      "Epoch 24/90\n",
      "9796/9796 [==============================] - 1s 51us/step - loss: 0.0021 - acc: 1.0208e-04 - val_loss: 0.0159 - val_acc: 4.7630e-04\n",
      "Epoch 25/90\n",
      "9796/9796 [==============================] - 1s 60us/step - loss: 0.0022 - acc: 1.0208e-04 - val_loss: 0.0157 - val_acc: 4.7630e-04\n",
      "Epoch 26/90\n",
      "9796/9796 [==============================] - 0s 48us/step - loss: 0.0022 - acc: 1.0208e-04 - val_loss: 0.0166 - val_acc: 4.7630e-04\n",
      "Epoch 27/90\n",
      "9796/9796 [==============================] - 0s 47us/step - loss: 0.0022 - acc: 1.0208e-04 - val_loss: 0.0167 - val_acc: 4.7630e-04\n",
      "Epoch 28/90\n",
      "9796/9796 [==============================] - 0s 48us/step - loss: 0.0020 - acc: 1.0208e-04 - val_loss: 0.0179 - val_acc: 4.7630e-04\n",
      "Epoch 29/90\n",
      "9796/9796 [==============================] - 0s 49us/step - loss: 0.0022 - acc: 1.0208e-04 - val_loss: 0.0168 - val_acc: 4.7630e-04\n",
      "Epoch 30/90\n",
      "9796/9796 [==============================] - 0s 49us/step - loss: 0.0020 - acc: 1.0208e-04 - val_loss: 0.0198 - val_acc: 4.7630e-04\n",
      "Epoch 31/90\n",
      "9796/9796 [==============================] - 1s 52us/step - loss: 0.0020 - acc: 1.0208e-04 - val_loss: 0.0220 - val_acc: 4.7630e-04\n",
      "Epoch 32/90\n",
      "9796/9796 [==============================] - 0s 48us/step - loss: 0.0020 - acc: 1.0208e-04 - val_loss: 0.0203 - val_acc: 4.7630e-04\n",
      "Epoch 33/90\n",
      "9796/9796 [==============================] - 0s 46us/step - loss: 0.0020 - acc: 1.0208e-04 - val_loss: 0.0211 - val_acc: 4.7630e-04\n",
      "Epoch 34/90\n",
      "9796/9796 [==============================] - 0s 47us/step - loss: 0.0020 - acc: 1.0208e-04 - val_loss: 0.0234 - val_acc: 4.7630e-04\n",
      "Epoch 35/90\n",
      "9796/9796 [==============================] - 0s 45us/step - loss: 0.0019 - acc: 1.0208e-04 - val_loss: 0.0260 - val_acc: 4.7630e-04\n",
      "Epoch 36/90\n",
      "9796/9796 [==============================] - 0s 48us/step - loss: 0.0020 - acc: 1.0208e-04 - val_loss: 0.0237 - val_acc: 4.7630e-04\n",
      "Epoch 37/90\n",
      "9796/9796 [==============================] - 0s 44us/step - loss: 0.0020 - acc: 1.0208e-04 - val_loss: 0.0250 - val_acc: 4.7630e-04\n",
      "Epoch 38/90\n",
      "9796/9796 [==============================] - 0s 44us/step - loss: 0.0018 - acc: 1.0208e-04 - val_loss: 0.0238 - val_acc: 4.7630e-04\n",
      "Epoch 39/90\n",
      "9796/9796 [==============================] - 0s 45us/step - loss: 0.0019 - acc: 1.0208e-04 - val_loss: 0.0255 - val_acc: 4.7630e-04\n",
      "Epoch 40/90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9796/9796 [==============================] - 0s 45us/step - loss: 0.0020 - acc: 1.0208e-04 - val_loss: 0.0251 - val_acc: 4.7630e-04\n",
      "Epoch 41/90\n",
      "9796/9796 [==============================] - 0s 44us/step - loss: 0.0018 - acc: 1.0208e-04 - val_loss: 0.0240 - val_acc: 4.7630e-04\n",
      "Epoch 42/90\n",
      "9796/9796 [==============================] - 0s 43us/step - loss: 0.0018 - acc: 1.0208e-04 - val_loss: 0.0235 - val_acc: 4.7630e-04\n",
      "Epoch 43/90\n",
      "9796/9796 [==============================] - 0s 44us/step - loss: 0.0018 - acc: 1.0208e-04 - val_loss: 0.0254 - val_acc: 4.7630e-04\n",
      "Epoch 44/90\n",
      "9796/9796 [==============================] - 0s 45us/step - loss: 0.0017 - acc: 1.0208e-04 - val_loss: 0.0258 - val_acc: 4.7630e-04\n",
      "Epoch 45/90\n",
      "9796/9796 [==============================] - 0s 45us/step - loss: 0.0019 - acc: 1.0208e-04 - val_loss: 0.0254 - val_acc: 4.7630e-04\n",
      "Epoch 46/90\n",
      "9796/9796 [==============================] - 0s 46us/step - loss: 0.0019 - acc: 1.0208e-04 - val_loss: 0.0241 - val_acc: 4.7630e-04\n",
      "Epoch 47/90\n",
      "9796/9796 [==============================] - 0s 47us/step - loss: 0.0018 - acc: 1.0208e-04 - val_loss: 0.0257 - val_acc: 4.7630e-04\n",
      "Epoch 48/90\n",
      "9796/9796 [==============================] - 0s 45us/step - loss: 0.0018 - acc: 1.0208e-04 - val_loss: 0.0262 - val_acc: 4.7630e-04\n",
      "Epoch 49/90\n",
      "9796/9796 [==============================] - 0s 44us/step - loss: 0.0017 - acc: 1.0208e-04 - val_loss: 0.0252 - val_acc: 4.7630e-04\n",
      "Epoch 50/90\n",
      "9796/9796 [==============================] - 0s 44us/step - loss: 0.0018 - acc: 1.0208e-04 - val_loss: 0.0254 - val_acc: 4.7630e-04\n",
      "Epoch 51/90\n",
      "9796/9796 [==============================] - 0s 44us/step - loss: 0.0017 - acc: 1.0208e-04 - val_loss: 0.0243 - val_acc: 4.7630e-04\n",
      "Epoch 52/90\n",
      "9796/9796 [==============================] - 0s 44us/step - loss: 0.0017 - acc: 1.0208e-04 - val_loss: 0.0256 - val_acc: 4.7630e-04\n",
      "Epoch 53/90\n",
      "9796/9796 [==============================] - 0s 44us/step - loss: 0.0018 - acc: 1.0208e-04 - val_loss: 0.0251 - val_acc: 4.7630e-04\n",
      "Epoch 54/90\n",
      "9796/9796 [==============================] - 0s 43us/step - loss: 0.0016 - acc: 1.0208e-04 - val_loss: 0.0262 - val_acc: 4.7630e-04\n",
      "Epoch 55/90\n",
      "9796/9796 [==============================] - 0s 44us/step - loss: 0.0016 - acc: 1.0208e-04 - val_loss: 0.0285 - val_acc: 4.7630e-04\n",
      "Epoch 56/90\n",
      "9796/9796 [==============================] - 0s 45us/step - loss: 0.0016 - acc: 1.0208e-04 - val_loss: 0.0272 - val_acc: 4.7630e-04\n",
      "Epoch 57/90\n",
      "9796/9796 [==============================] - 0s 45us/step - loss: 0.0016 - acc: 1.0208e-04 - val_loss: 0.0245 - val_acc: 4.7630e-04\n",
      "Epoch 58/90\n",
      "9796/9796 [==============================] - 0s 45us/step - loss: 0.0016 - acc: 1.0208e-04 - val_loss: 0.0250 - val_acc: 4.7630e-04\n",
      "Epoch 59/90\n",
      "9796/9796 [==============================] - 0s 45us/step - loss: 0.0016 - acc: 1.0208e-04 - val_loss: 0.0242 - val_acc: 4.7630e-04\n",
      "Epoch 60/90\n",
      "9796/9796 [==============================] - 0s 46us/step - loss: 0.0016 - acc: 1.0208e-04 - val_loss: 0.0242 - val_acc: 4.7630e-04\n",
      "Epoch 61/90\n",
      "9796/9796 [==============================] - 0s 46us/step - loss: 0.0015 - acc: 1.0208e-04 - val_loss: 0.0264 - val_acc: 4.7630e-04\n",
      "Epoch 62/90\n",
      "9796/9796 [==============================] - 0s 47us/step - loss: 0.0016 - acc: 1.0208e-04 - val_loss: 0.0244 - val_acc: 4.7630e-04\n",
      "Epoch 63/90\n",
      "9796/9796 [==============================] - 1s 51us/step - loss: 0.0015 - acc: 1.0208e-04 - val_loss: 0.0252 - val_acc: 4.7630e-04\n",
      "Epoch 64/90\n",
      "9796/9796 [==============================] - 0s 50us/step - loss: 0.0014 - acc: 1.0208e-04 - val_loss: 0.0252 - val_acc: 4.7630e-04\n",
      "Epoch 65/90\n",
      "9796/9796 [==============================] - 0s 47us/step - loss: 0.0015 - acc: 1.0208e-04 - val_loss: 0.0220 - val_acc: 4.7630e-04\n",
      "Epoch 66/90\n",
      "9796/9796 [==============================] - 0s 46us/step - loss: 0.0017 - acc: 1.0208e-04 - val_loss: 0.0256 - val_acc: 4.7630e-04\n",
      "Epoch 67/90\n",
      "9796/9796 [==============================] - 0s 47us/step - loss: 0.0015 - acc: 1.0208e-04 - val_loss: 0.0257 - val_acc: 4.7630e-04\n",
      "Epoch 68/90\n",
      "9796/9796 [==============================] - 0s 47us/step - loss: 0.0015 - acc: 1.0208e-04 - val_loss: 0.0270 - val_acc: 4.7630e-04\n",
      "Epoch 69/90\n",
      "9796/9796 [==============================] - 0s 48us/step - loss: 0.0014 - acc: 1.0208e-04 - val_loss: 0.0201 - val_acc: 4.7630e-04\n",
      "Epoch 70/90\n",
      "9796/9796 [==============================] - 0s 45us/step - loss: 0.0014 - acc: 1.0208e-04 - val_loss: 0.0239 - val_acc: 4.7630e-04\n",
      "Epoch 71/90\n",
      "9796/9796 [==============================] - 0s 46us/step - loss: 0.0014 - acc: 1.0208e-04 - val_loss: 0.0235 - val_acc: 4.7630e-04\n",
      "Epoch 72/90\n",
      "9796/9796 [==============================] - 0s 46us/step - loss: 0.0014 - acc: 1.0208e-04 - val_loss: 0.0237 - val_acc: 4.7630e-04\n",
      "Epoch 73/90\n",
      "9796/9796 [==============================] - 0s 44us/step - loss: 0.0014 - acc: 1.0208e-04 - val_loss: 0.0252 - val_acc: 4.7630e-04\n",
      "Epoch 74/90\n",
      "9796/9796 [==============================] - 0s 51us/step - loss: 0.0014 - acc: 1.0208e-04 - val_loss: 0.0242 - val_acc: 4.7630e-04\n",
      "Epoch 75/90\n",
      "9796/9796 [==============================] - 0s 48us/step - loss: 0.0015 - acc: 1.0208e-04 - val_loss: 0.0237 - val_acc: 4.7630e-04\n",
      "Epoch 76/90\n",
      "9796/9796 [==============================] - 0s 45us/step - loss: 0.0013 - acc: 1.0208e-04 - val_loss: 0.0233 - val_acc: 4.7630e-04\n",
      "Epoch 77/90\n",
      "9796/9796 [==============================] - 0s 42us/step - loss: 0.0015 - acc: 1.0208e-04 - val_loss: 0.0243 - val_acc: 4.7630e-04\n",
      "Epoch 78/90\n",
      "9796/9796 [==============================] - 0s 43us/step - loss: 0.0015 - acc: 1.0208e-04 - val_loss: 0.0265 - val_acc: 4.7630e-04\n",
      "Epoch 79/90\n",
      "9796/9796 [==============================] - 0s 44us/step - loss: 0.0014 - acc: 1.0208e-04 - val_loss: 0.0233 - val_acc: 4.7630e-04\n",
      "Epoch 80/90\n",
      "9796/9796 [==============================] - 0s 44us/step - loss: 0.0014 - acc: 1.0208e-04 - val_loss: 0.0226 - val_acc: 4.7630e-04\n",
      "Epoch 81/90\n",
      "9796/9796 [==============================] - 0s 43us/step - loss: 0.0013 - acc: 1.0208e-04 - val_loss: 0.0217 - val_acc: 4.7630e-04\n",
      "Epoch 82/90\n",
      "9796/9796 [==============================] - 0s 44us/step - loss: 0.0014 - acc: 1.0208e-04 - val_loss: 0.0236 - val_acc: 4.7630e-04\n",
      "Epoch 83/90\n",
      "9796/9796 [==============================] - 0s 43us/step - loss: 0.0013 - acc: 1.0208e-04 - val_loss: 0.0222 - val_acc: 4.7630e-04\n",
      "Epoch 84/90\n",
      "9796/9796 [==============================] - 0s 43us/step - loss: 0.0013 - acc: 1.0208e-04 - val_loss: 0.0227 - val_acc: 4.7630e-04\n",
      "Epoch 85/90\n",
      "9796/9796 [==============================] - 0s 45us/step - loss: 0.0013 - acc: 1.0208e-04 - val_loss: 0.0228 - val_acc: 4.7630e-04\n",
      "Epoch 86/90\n",
      "9796/9796 [==============================] - 0s 44us/step - loss: 0.0013 - acc: 1.0208e-04 - val_loss: 0.0233 - val_acc: 4.7630e-04\n",
      "Epoch 87/90\n",
      "9796/9796 [==============================] - 0s 45us/step - loss: 0.0013 - acc: 1.0208e-04 - val_loss: 0.0222 - val_acc: 4.7630e-04\n",
      "Epoch 88/90\n",
      "9796/9796 [==============================] - 0s 43us/step - loss: 0.0013 - acc: 1.0208e-04 - val_loss: 0.0203 - val_acc: 4.7630e-04\n",
      "Epoch 89/90\n",
      "9796/9796 [==============================] - 0s 44us/step - loss: 0.0013 - acc: 1.0208e-04 - val_loss: 0.0231 - val_acc: 4.7630e-04\n",
      "Epoch 90/90\n",
      "9796/9796 [==============================] - 0s 43us/step - loss: 0.0013 - acc: 1.0208e-04 - val_loss: 0.0231 - val_acc: 4.7630e-04\n",
      "-29.78178319445293\n",
      "Amount of features = 9\n",
      "Amount of training data = 13991\n",
      "Amount of testing data = 5996\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_17 (Conv1D)           (None, 21, 64)            1216      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 10, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 9, 64)             8256      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 4, 64)             0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 4, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 250)               64250     \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1)                 251       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 73,973\n",
      "Trainable params: 73,973\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9777 samples, validate on 4191 samples\n",
      "Epoch 1/90\n",
      "9777/9777 [==============================] - 1s 121us/step - loss: 0.0475 - acc: 4.0912e-04 - val_loss: 0.0071 - val_acc: 0.0000e+00\n",
      "Epoch 2/90\n",
      "9777/9777 [==============================] - 0s 44us/step - loss: 0.0093 - acc: 4.0912e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 3/90\n",
      "9777/9777 [==============================] - 0s 42us/step - loss: 0.0062 - acc: 4.0912e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 4/90\n",
      "9777/9777 [==============================] - 0s 42us/step - loss: 0.0049 - acc: 4.0912e-04 - val_loss: 8.7507e-04 - val_acc: 0.0000e+00\n",
      "Epoch 5/90\n",
      "9777/9777 [==============================] - 0s 43us/step - loss: 0.0039 - acc: 4.0912e-04 - val_loss: 0.0038 - val_acc: 0.0000e+00\n",
      "Epoch 6/90\n",
      "9777/9777 [==============================] - 0s 43us/step - loss: 0.0036 - acc: 4.0912e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 7/90\n",
      "9777/9777 [==============================] - 0s 42us/step - loss: 0.0031 - acc: 4.0912e-04 - val_loss: 0.0030 - val_acc: 0.0000e+00\n",
      "Epoch 8/90\n",
      "9777/9777 [==============================] - 0s 44us/step - loss: 0.0028 - acc: 4.0912e-04 - val_loss: 0.0036 - val_acc: 0.0000e+00\n",
      "Epoch 9/90\n",
      "9777/9777 [==============================] - 0s 43us/step - loss: 0.0027 - acc: 4.0912e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 10/90\n",
      "9777/9777 [==============================] - 0s 43us/step - loss: 0.0026 - acc: 4.0912e-04 - val_loss: 0.0029 - val_acc: 0.0000e+00\n",
      "Epoch 11/90\n",
      "9777/9777 [==============================] - 0s 43us/step - loss: 0.0023 - acc: 4.0912e-04 - val_loss: 0.0048 - val_acc: 0.0000e+00\n",
      "Epoch 12/90\n",
      "9777/9777 [==============================] - 0s 42us/step - loss: 0.0022 - acc: 4.0912e-04 - val_loss: 0.0030 - val_acc: 0.0000e+00\n",
      "Epoch 13/90\n",
      "9777/9777 [==============================] - 0s 43us/step - loss: 0.0022 - acc: 4.0912e-04 - val_loss: 0.0059 - val_acc: 0.0000e+00\n",
      "Epoch 14/90\n",
      "9777/9777 [==============================] - 0s 43us/step - loss: 0.0021 - acc: 4.0912e-04 - val_loss: 0.0040 - val_acc: 0.0000e+00\n",
      "Epoch 15/90\n",
      "9777/9777 [==============================] - 0s 43us/step - loss: 0.0021 - acc: 4.0912e-04 - val_loss: 0.0045 - val_acc: 0.0000e+00\n",
      "Epoch 16/90\n",
      "9777/9777 [==============================] - 0s 42us/step - loss: 0.0021 - acc: 4.0912e-04 - val_loss: 0.0039 - val_acc: 0.0000e+00\n",
      "Epoch 17/90\n",
      "9777/9777 [==============================] - 0s 43us/step - loss: 0.0021 - acc: 4.0912e-04 - val_loss: 0.0064 - val_acc: 0.0000e+00\n",
      "Epoch 18/90\n",
      "9777/9777 [==============================] - 0s 42us/step - loss: 0.0020 - acc: 4.0912e-04 - val_loss: 0.0025 - val_acc: 0.0000e+00\n",
      "Epoch 19/90\n",
      "9777/9777 [==============================] - 0s 43us/step - loss: 0.0020 - acc: 4.0912e-04 - val_loss: 0.0041 - val_acc: 0.0000e+00\n",
      "Epoch 20/90\n",
      "9777/9777 [==============================] - 0s 43us/step - loss: 0.0020 - acc: 4.0912e-04 - val_loss: 0.0026 - val_acc: 0.0000e+00\n",
      "Epoch 21/90\n",
      "9777/9777 [==============================] - 0s 42us/step - loss: 0.0019 - acc: 4.0912e-04 - val_loss: 0.0043 - val_acc: 0.0000e+00\n",
      "Epoch 22/90\n",
      "9777/9777 [==============================] - 0s 42us/step - loss: 0.0019 - acc: 4.0912e-04 - val_loss: 0.0039 - val_acc: 0.0000e+00\n",
      "Epoch 23/90\n",
      "9777/9777 [==============================] - 0s 42us/step - loss: 0.0018 - acc: 4.0912e-04 - val_loss: 0.0027 - val_acc: 0.0000e+00\n",
      "Epoch 24/90\n",
      "9777/9777 [==============================] - 0s 40us/step - loss: 0.0018 - acc: 4.0912e-04 - val_loss: 0.0061 - val_acc: 0.0000e+00\n",
      "Epoch 25/90\n",
      "9777/9777 [==============================] - 0s 41us/step - loss: 0.0019 - acc: 4.0912e-04 - val_loss: 0.0050 - val_acc: 0.0000e+00\n",
      "Epoch 26/90\n",
      "9777/9777 [==============================] - 0s 41us/step - loss: 0.0019 - acc: 4.0912e-04 - val_loss: 0.0027 - val_acc: 0.0000e+00\n",
      "Epoch 27/90\n",
      "9777/9777 [==============================] - 0s 40us/step - loss: 0.0019 - acc: 4.0912e-04 - val_loss: 0.0044 - val_acc: 0.0000e+00\n",
      "Epoch 28/90\n",
      "9777/9777 [==============================] - 0s 43us/step - loss: 0.0018 - acc: 4.0912e-04 - val_loss: 0.0044 - val_acc: 0.0000e+00\n",
      "Epoch 29/90\n",
      "9777/9777 [==============================] - 0s 43us/step - loss: 0.0018 - acc: 4.0912e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 30/90\n",
      "9777/9777 [==============================] - 0s 43us/step - loss: 0.0018 - acc: 4.0912e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 31/90\n",
      "9777/9777 [==============================] - 0s 43us/step - loss: 0.0018 - acc: 4.0912e-04 - val_loss: 0.0053 - val_acc: 0.0000e+00\n",
      "Epoch 32/90\n",
      "9777/9777 [==============================] - 0s 43us/step - loss: 0.0018 - acc: 4.0912e-04 - val_loss: 7.6732e-04 - val_acc: 0.0000e+00\n",
      "Epoch 33/90\n",
      "9777/9777 [==============================] - 0s 42us/step - loss: 0.0019 - acc: 4.0912e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 34/90\n",
      "9777/9777 [==============================] - 0s 45us/step - loss: 0.0018 - acc: 4.0912e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 35/90\n",
      "9777/9777 [==============================] - 0s 43us/step - loss: 0.0017 - acc: 4.0912e-04 - val_loss: 0.0036 - val_acc: 0.0000e+00\n",
      "Epoch 36/90\n",
      "9777/9777 [==============================] - 0s 43us/step - loss: 0.0017 - acc: 4.0912e-04 - val_loss: 0.0028 - val_acc: 0.0000e+00\n",
      "Epoch 37/90\n",
      "9777/9777 [==============================] - 0s 43us/step - loss: 0.0017 - acc: 4.0912e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 38/90\n",
      "9777/9777 [==============================] - 0s 42us/step - loss: 0.0017 - acc: 4.0912e-04 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
      "Epoch 39/90\n",
      "9777/9777 [==============================] - 0s 43us/step - loss: 0.0016 - acc: 4.0912e-04 - val_loss: 0.0043 - val_acc: 0.0000e+00\n",
      "Epoch 40/90\n",
      "9777/9777 [==============================] - 0s 42us/step - loss: 0.0018 - acc: 4.0912e-04 - val_loss: 0.0044 - val_acc: 0.0000e+00\n",
      "Epoch 41/90\n",
      "9777/9777 [==============================] - 0s 43us/step - loss: 0.0018 - acc: 4.0912e-04 - val_loss: 7.6216e-04 - val_acc: 0.0000e+00\n",
      "Epoch 42/90\n",
      "9777/9777 [==============================] - 0s 44us/step - loss: 0.0017 - acc: 4.0912e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 43/90\n",
      "9777/9777 [==============================] - 0s 43us/step - loss: 0.0016 - acc: 4.0912e-04 - val_loss: 0.0030 - val_acc: 0.0000e+00\n",
      "Epoch 44/90\n",
      "9777/9777 [==============================] - 0s 42us/step - loss: 0.0017 - acc: 4.0912e-04 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
      "Epoch 45/90\n",
      "9777/9777 [==============================] - 0s 43us/step - loss: 0.0015 - acc: 4.0912e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 46/90\n",
      "9777/9777 [==============================] - 0s 44us/step - loss: 0.0015 - acc: 4.0912e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 47/90\n",
      "9777/9777 [==============================] - 0s 43us/step - loss: 0.0016 - acc: 4.0912e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 48/90\n",
      "9777/9777 [==============================] - 0s 42us/step - loss: 0.0016 - acc: 4.0912e-04 - val_loss: 0.0029 - val_acc: 0.0000e+00\n",
      "Epoch 49/90\n",
      "9777/9777 [==============================] - 0s 42us/step - loss: 0.0016 - acc: 4.0912e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 50/90\n",
      "9777/9777 [==============================] - 0s 44us/step - loss: 0.0016 - acc: 4.0912e-04 - val_loss: 0.0028 - val_acc: 0.0000e+00\n",
      "Epoch 51/90\n",
      "9777/9777 [==============================] - 0s 43us/step - loss: 0.0017 - acc: 4.0912e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 52/90\n",
      "9777/9777 [==============================] - 0s 48us/step - loss: 0.0016 - acc: 4.0912e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 53/90\n",
      "9777/9777 [==============================] - 0s 47us/step - loss: 0.0015 - acc: 4.0912e-04 - val_loss: 0.0025 - val_acc: 0.0000e+00\n",
      "Epoch 54/90\n",
      "9777/9777 [==============================] - 0s 46us/step - loss: 0.0016 - acc: 4.0912e-04 - val_loss: 8.3483e-04 - val_acc: 0.0000e+00\n",
      "Epoch 55/90\n",
      "9777/9777 [==============================] - 0s 45us/step - loss: 0.0016 - acc: 4.0912e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 56/90\n",
      "9777/9777 [==============================] - 0s 48us/step - loss: 0.0016 - acc: 4.0912e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 57/90\n",
      "9777/9777 [==============================] - 1s 58us/step - loss: 0.0015 - acc: 4.0912e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 58/90\n",
      "9777/9777 [==============================] - 1s 53us/step - loss: 0.0015 - acc: 4.0912e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 59/90\n",
      "9777/9777 [==============================] - 1s 54us/step - loss: 0.0015 - acc: 4.0912e-04 - val_loss: 4.9712e-04 - val_acc: 0.0000e+00\n",
      "Epoch 60/90\n",
      "9777/9777 [==============================] - 1s 54us/step - loss: 0.0015 - acc: 4.0912e-04 - val_loss: 5.7083e-04 - val_acc: 0.0000e+00\n",
      "Epoch 61/90\n",
      "9777/9777 [==============================] - 0s 45us/step - loss: 0.0014 - acc: 4.0912e-04 - val_loss: 9.8587e-04 - val_acc: 0.0000e+00\n",
      "Epoch 62/90\n",
      "9777/9777 [==============================] - 0s 43us/step - loss: 0.0015 - acc: 4.0912e-04 - val_loss: 5.5029e-04 - val_acc: 0.0000e+00\n",
      "Epoch 63/90\n",
      "9777/9777 [==============================] - 0s 42us/step - loss: 0.0016 - acc: 4.0912e-04 - val_loss: 7.7532e-04 - val_acc: 0.0000e+00\n",
      "Epoch 64/90\n",
      "9777/9777 [==============================] - 0s 45us/step - loss: 0.0015 - acc: 4.0912e-04 - val_loss: 8.5640e-04 - val_acc: 0.0000e+00\n",
      "Epoch 65/90\n",
      "9777/9777 [==============================] - 0s 43us/step - loss: 0.0014 - acc: 4.0912e-04 - val_loss: 5.1553e-04 - val_acc: 0.0000e+00\n",
      "Epoch 66/90\n",
      "9777/9777 [==============================] - 0s 44us/step - loss: 0.0014 - acc: 4.0912e-04 - val_loss: 6.0911e-04 - val_acc: 0.0000e+00\n",
      "Epoch 67/90\n",
      "9777/9777 [==============================] - 0s 43us/step - loss: 0.0014 - acc: 4.0912e-04 - val_loss: 7.1090e-04 - val_acc: 0.0000e+00\n",
      "Epoch 68/90\n",
      "9777/9777 [==============================] - 0s 44us/step - loss: 0.0014 - acc: 4.0912e-04 - val_loss: 4.9169e-04 - val_acc: 0.0000e+00\n",
      "Epoch 69/90\n",
      "9777/9777 [==============================] - 0s 44us/step - loss: 0.0016 - acc: 4.0912e-04 - val_loss: 8.1030e-04 - val_acc: 0.0000e+00\n",
      "Epoch 70/90\n",
      "9777/9777 [==============================] - 0s 49us/step - loss: 0.0014 - acc: 4.0912e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 71/90\n",
      "9777/9777 [==============================] - 0s 48us/step - loss: 0.0014 - acc: 4.0912e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 72/90\n",
      "9777/9777 [==============================] - 0s 46us/step - loss: 0.0014 - acc: 4.0912e-04 - val_loss: 9.1703e-04 - val_acc: 0.0000e+00\n",
      "Epoch 73/90\n",
      "9777/9777 [==============================] - 0s 45us/step - loss: 0.0014 - acc: 4.0912e-04 - val_loss: 4.0098e-04 - val_acc: 0.0000e+00\n",
      "Epoch 74/90\n",
      "9777/9777 [==============================] - 0s 44us/step - loss: 0.0014 - acc: 4.0912e-04 - val_loss: 6.2216e-04 - val_acc: 0.0000e+00\n",
      "Epoch 75/90\n",
      "9777/9777 [==============================] - 0s 42us/step - loss: 0.0013 - acc: 4.0912e-04 - val_loss: 6.4916e-04 - val_acc: 0.0000e+00\n",
      "Epoch 76/90\n",
      "9777/9777 [==============================] - 0s 44us/step - loss: 0.0013 - acc: 4.0912e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 77/90\n",
      "9777/9777 [==============================] - 0s 47us/step - loss: 0.0014 - acc: 4.0912e-04 - val_loss: 9.9580e-04 - val_acc: 0.0000e+00\n",
      "Epoch 78/90\n",
      "9777/9777 [==============================] - 0s 44us/step - loss: 0.0013 - acc: 4.0912e-04 - val_loss: 5.0702e-04 - val_acc: 0.0000e+00\n",
      "Epoch 79/90\n",
      "9777/9777 [==============================] - 0s 44us/step - loss: 0.0013 - acc: 4.0912e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 80/90\n",
      "9777/9777 [==============================] - 0s 45us/step - loss: 0.0013 - acc: 4.0912e-04 - val_loss: 6.7298e-04 - val_acc: 0.0000e+00\n",
      "Epoch 81/90\n",
      "9777/9777 [==============================] - 0s 44us/step - loss: 0.0013 - acc: 4.0912e-04 - val_loss: 6.1992e-04 - val_acc: 0.0000e+00\n",
      "Epoch 82/90\n",
      "9777/9777 [==============================] - 0s 44us/step - loss: 0.0012 - acc: 4.0912e-04 - val_loss: 3.7327e-04 - val_acc: 0.0000e+00\n",
      "Epoch 83/90\n",
      "9777/9777 [==============================] - 0s 44us/step - loss: 0.0013 - acc: 4.0912e-04 - val_loss: 6.1866e-04 - val_acc: 0.0000e+00\n",
      "Epoch 84/90\n",
      "9777/9777 [==============================] - 0s 43us/step - loss: 0.0013 - acc: 4.0912e-04 - val_loss: 5.2535e-04 - val_acc: 0.0000e+00\n",
      "Epoch 85/90\n",
      "9777/9777 [==============================] - 0s 44us/step - loss: 0.0012 - acc: 4.0912e-04 - val_loss: 6.1090e-04 - val_acc: 0.0000e+00\n",
      "Epoch 86/90\n",
      "9777/9777 [==============================] - 0s 44us/step - loss: 0.0013 - acc: 4.0912e-04 - val_loss: 4.9173e-04 - val_acc: 0.0000e+00\n",
      "Epoch 87/90\n",
      "9777/9777 [==============================] - 0s 43us/step - loss: 0.0013 - acc: 4.0912e-04 - val_loss: 3.5393e-04 - val_acc: 0.0000e+00\n",
      "Epoch 88/90\n",
      "9777/9777 [==============================] - 0s 44us/step - loss: 0.0012 - acc: 4.0912e-04 - val_loss: 3.3747e-04 - val_acc: 0.0000e+00\n",
      "Epoch 89/90\n",
      "9777/9777 [==============================] - 0s 43us/step - loss: 0.0013 - acc: 4.0912e-04 - val_loss: 7.0192e-04 - val_acc: 0.0000e+00\n",
      "Epoch 90/90\n",
      "9777/9777 [==============================] - 0s 45us/step - loss: 0.0012 - acc: 4.0912e-04 - val_loss: 3.6419e-04 - val_acc: 0.0000e+00\n",
      "-48.09073022718754\n",
      "Amount of features = 9\n",
      "Amount of training data = 14005\n",
      "Amount of testing data = 6002\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_19 (Conv1D)           (None, 21, 64)            1216      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 10, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 9, 64)             8256      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 4, 64)             0         \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 4, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 250)               64250     \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 251       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 73,973\n",
      "Trainable params: 73,973\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 9787 samples, validate on 4195 samples\n",
      "Epoch 1/90\n",
      "9787/9787 [==============================] - 1s 130us/step - loss: 0.0191 - acc: 1.0218e-04 - val_loss: 0.0064 - val_acc: 2.3838e-04\n",
      "Epoch 2/90\n",
      "9787/9787 [==============================] - 0s 45us/step - loss: 0.0044 - acc: 1.0218e-04 - val_loss: 0.0035 - val_acc: 2.3838e-04\n",
      "Epoch 3/90\n",
      "9787/9787 [==============================] - 0s 46us/step - loss: 0.0028 - acc: 1.0218e-04 - val_loss: 0.0025 - val_acc: 2.3838e-04\n",
      "Epoch 4/90\n",
      "9787/9787 [==============================] - 0s 45us/step - loss: 0.0024 - acc: 1.0218e-04 - val_loss: 0.0026 - val_acc: 2.3838e-04\n",
      "Epoch 5/90\n",
      "9787/9787 [==============================] - 0s 46us/step - loss: 0.0020 - acc: 1.0218e-04 - val_loss: 0.0036 - val_acc: 2.3838e-04\n",
      "Epoch 6/90\n",
      "9787/9787 [==============================] - 0s 45us/step - loss: 0.0019 - acc: 1.0218e-04 - val_loss: 0.0030 - val_acc: 2.3838e-04\n",
      "Epoch 7/90\n",
      "9787/9787 [==============================] - 0s 48us/step - loss: 0.0016 - acc: 1.0218e-04 - val_loss: 0.0045 - val_acc: 2.3838e-04\n",
      "Epoch 8/90\n",
      "9787/9787 [==============================] - 0s 46us/step - loss: 0.0015 - acc: 1.0218e-04 - val_loss: 0.0043 - val_acc: 2.3838e-04\n",
      "Epoch 9/90\n",
      "9787/9787 [==============================] - 0s 46us/step - loss: 0.0014 - acc: 1.0218e-04 - val_loss: 0.0056 - val_acc: 2.3838e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/90\n",
      "9787/9787 [==============================] - 0s 44us/step - loss: 0.0013 - acc: 1.0218e-04 - val_loss: 0.0068 - val_acc: 2.3838e-04\n",
      "Epoch 11/90\n",
      "9787/9787 [==============================] - 0s 46us/step - loss: 0.0012 - acc: 1.0218e-04 - val_loss: 0.0038 - val_acc: 2.3838e-04\n",
      "Epoch 12/90\n",
      "9787/9787 [==============================] - 0s 45us/step - loss: 0.0012 - acc: 1.0218e-04 - val_loss: 0.0064 - val_acc: 2.3838e-04\n",
      "Epoch 13/90\n",
      "9787/9787 [==============================] - 0s 45us/step - loss: 0.0012 - acc: 1.0218e-04 - val_loss: 0.0069 - val_acc: 2.3838e-04\n",
      "Epoch 14/90\n",
      "9787/9787 [==============================] - 0s 46us/step - loss: 0.0011 - acc: 1.0218e-04 - val_loss: 0.0031 - val_acc: 2.3838e-04\n",
      "Epoch 15/90\n",
      "9787/9787 [==============================] - 0s 45us/step - loss: 0.0011 - acc: 1.0218e-04 - val_loss: 0.0047 - val_acc: 2.3838e-04\n",
      "Epoch 16/90\n",
      "9787/9787 [==============================] - 0s 45us/step - loss: 0.0010 - acc: 1.0218e-04 - val_loss: 0.0035 - val_acc: 2.3838e-04\n",
      "Epoch 17/90\n",
      "9787/9787 [==============================] - 0s 44us/step - loss: 0.0011 - acc: 1.0218e-04 - val_loss: 0.0037 - val_acc: 2.3838e-04\n",
      "Epoch 18/90\n",
      "9787/9787 [==============================] - 0s 44us/step - loss: 9.9730e-04 - acc: 1.0218e-04 - val_loss: 0.0035 - val_acc: 2.3838e-04\n",
      "Epoch 19/90\n",
      "9787/9787 [==============================] - 0s 45us/step - loss: 9.9430e-04 - acc: 1.0218e-04 - val_loss: 0.0070 - val_acc: 2.3838e-04\n",
      "Epoch 20/90\n",
      "9787/9787 [==============================] - 0s 45us/step - loss: 9.5578e-04 - acc: 1.0218e-04 - val_loss: 0.0040 - val_acc: 2.3838e-04\n",
      "Epoch 21/90\n",
      "9787/9787 [==============================] - 0s 46us/step - loss: 0.0010 - acc: 1.0218e-04 - val_loss: 0.0088 - val_acc: 2.3838e-04\n",
      "Epoch 22/90\n",
      "9787/9787 [==============================] - 0s 45us/step - loss: 9.7204e-04 - acc: 1.0218e-04 - val_loss: 0.0053 - val_acc: 2.3838e-04\n",
      "Epoch 23/90\n",
      "9787/9787 [==============================] - 0s 47us/step - loss: 8.9351e-04 - acc: 1.0218e-04 - val_loss: 0.0077 - val_acc: 2.3838e-04\n",
      "Epoch 24/90\n",
      "9787/9787 [==============================] - 0s 46us/step - loss: 8.8124e-04 - acc: 1.0218e-04 - val_loss: 0.0047 - val_acc: 2.3838e-04\n",
      "Epoch 25/90\n",
      "9787/9787 [==============================] - 0s 46us/step - loss: 8.6329e-04 - acc: 1.0218e-04 - val_loss: 0.0079 - val_acc: 2.3838e-04\n",
      "Epoch 26/90\n",
      "9787/9787 [==============================] - 0s 46us/step - loss: 8.4481e-04 - acc: 1.0218e-04 - val_loss: 0.0036 - val_acc: 2.3838e-04\n",
      "Epoch 27/90\n",
      "9787/9787 [==============================] - 0s 45us/step - loss: 8.7176e-04 - acc: 1.0218e-04 - val_loss: 0.0044 - val_acc: 2.3838e-04\n",
      "Epoch 28/90\n",
      "9787/9787 [==============================] - 0s 46us/step - loss: 8.6574e-04 - acc: 1.0218e-04 - val_loss: 0.0071 - val_acc: 2.3838e-04\n",
      "Epoch 29/90\n",
      "9787/9787 [==============================] - 0s 47us/step - loss: 8.7978e-04 - acc: 1.0218e-04 - val_loss: 0.0037 - val_acc: 2.3838e-04\n",
      "Epoch 30/90\n",
      "9787/9787 [==============================] - 0s 45us/step - loss: 7.9729e-04 - acc: 1.0218e-04 - val_loss: 0.0063 - val_acc: 2.3838e-04\n",
      "Epoch 31/90\n",
      "9787/9787 [==============================] - 0s 45us/step - loss: 8.3796e-04 - acc: 1.0218e-04 - val_loss: 0.0059 - val_acc: 2.3838e-04\n",
      "Epoch 32/90\n",
      "9787/9787 [==============================] - 0s 45us/step - loss: 8.1217e-04 - acc: 1.0218e-04 - val_loss: 0.0035 - val_acc: 2.3838e-04\n",
      "Epoch 33/90\n",
      "9787/9787 [==============================] - 0s 46us/step - loss: 8.0711e-04 - acc: 1.0218e-04 - val_loss: 0.0040 - val_acc: 2.3838e-04\n",
      "Epoch 34/90\n",
      "9787/9787 [==============================] - 0s 45us/step - loss: 7.6514e-04 - acc: 1.0218e-04 - val_loss: 0.0046 - val_acc: 2.3838e-04\n",
      "Epoch 35/90\n",
      "9787/9787 [==============================] - 0s 46us/step - loss: 7.8282e-04 - acc: 1.0218e-04 - val_loss: 0.0070 - val_acc: 2.3838e-04\n",
      "Epoch 36/90\n",
      "9787/9787 [==============================] - 0s 47us/step - loss: 8.0515e-04 - acc: 1.0218e-04 - val_loss: 0.0028 - val_acc: 2.3838e-04\n",
      "Epoch 37/90\n",
      "9787/9787 [==============================] - 0s 45us/step - loss: 7.6163e-04 - acc: 1.0218e-04 - val_loss: 0.0018 - val_acc: 2.3838e-04\n",
      "Epoch 38/90\n",
      "9787/9787 [==============================] - 0s 45us/step - loss: 8.3637e-04 - acc: 1.0218e-04 - val_loss: 0.0046 - val_acc: 2.3838e-04\n",
      "Epoch 39/90\n",
      "9787/9787 [==============================] - 0s 46us/step - loss: 7.4634e-04 - acc: 1.0218e-04 - val_loss: 0.0020 - val_acc: 2.3838e-04\n",
      "Epoch 40/90\n",
      "9787/9787 [==============================] - 0s 45us/step - loss: 7.4623e-04 - acc: 1.0218e-04 - val_loss: 0.0028 - val_acc: 2.3838e-04\n",
      "Epoch 41/90\n",
      "9787/9787 [==============================] - 0s 45us/step - loss: 7.5019e-04 - acc: 1.0218e-04 - val_loss: 0.0044 - val_acc: 2.3838e-04\n",
      "Epoch 42/90\n",
      "9787/9787 [==============================] - 0s 46us/step - loss: 7.0997e-04 - acc: 1.0218e-04 - val_loss: 0.0031 - val_acc: 2.3838e-04\n",
      "Epoch 43/90\n",
      "9787/9787 [==============================] - 0s 46us/step - loss: 7.0034e-04 - acc: 1.0218e-04 - val_loss: 0.0036 - val_acc: 2.3838e-04\n",
      "Epoch 44/90\n",
      "9787/9787 [==============================] - 0s 45us/step - loss: 7.5511e-04 - acc: 1.0218e-04 - val_loss: 0.0027 - val_acc: 2.3838e-04\n",
      "Epoch 45/90\n",
      "9787/9787 [==============================] - 0s 46us/step - loss: 7.4550e-04 - acc: 1.0218e-04 - val_loss: 0.0038 - val_acc: 2.3838e-04\n",
      "Epoch 46/90\n",
      "9787/9787 [==============================] - 0s 47us/step - loss: 7.3909e-04 - acc: 1.0218e-04 - val_loss: 0.0047 - val_acc: 2.3838e-04\n",
      "Epoch 47/90\n",
      "9787/9787 [==============================] - 0s 47us/step - loss: 7.4314e-04 - acc: 1.0218e-04 - val_loss: 0.0028 - val_acc: 2.3838e-04\n",
      "Epoch 48/90\n",
      "9787/9787 [==============================] - 0s 48us/step - loss: 7.1862e-04 - acc: 1.0218e-04 - val_loss: 0.0024 - val_acc: 2.3838e-04\n",
      "Epoch 49/90\n",
      "9787/9787 [==============================] - 0s 47us/step - loss: 7.1964e-04 - acc: 1.0218e-04 - val_loss: 0.0039 - val_acc: 2.3838e-04\n",
      "Epoch 50/90\n",
      "9787/9787 [==============================] - 0s 48us/step - loss: 6.7013e-04 - acc: 1.0218e-04 - val_loss: 0.0052 - val_acc: 2.3838e-04\n",
      "Epoch 51/90\n",
      "9787/9787 [==============================] - 0s 46us/step - loss: 7.0354e-04 - acc: 1.0218e-04 - val_loss: 0.0027 - val_acc: 2.3838e-04\n",
      "Epoch 52/90\n",
      "9787/9787 [==============================] - 0s 47us/step - loss: 6.7687e-04 - acc: 1.0218e-04 - val_loss: 0.0040 - val_acc: 2.3838e-04\n",
      "Epoch 53/90\n",
      "9787/9787 [==============================] - 0s 48us/step - loss: 6.6818e-04 - acc: 1.0218e-04 - val_loss: 0.0037 - val_acc: 2.3838e-04\n",
      "Epoch 54/90\n",
      "9787/9787 [==============================] - 0s 49us/step - loss: 7.2515e-04 - acc: 1.0218e-04 - val_loss: 0.0029 - val_acc: 2.3838e-04\n",
      "Epoch 55/90\n",
      "9787/9787 [==============================] - 1s 65us/step - loss: 6.9214e-04 - acc: 1.0218e-04 - val_loss: 0.0028 - val_acc: 2.3838e-04\n",
      "Epoch 56/90\n",
      "9787/9787 [==============================] - 1s 53us/step - loss: 6.6753e-04 - acc: 1.0218e-04 - val_loss: 0.0031 - val_acc: 2.3838e-04\n",
      "Epoch 57/90\n",
      "9787/9787 [==============================] - 1s 54us/step - loss: 6.7467e-04 - acc: 1.0218e-04 - val_loss: 0.0050 - val_acc: 2.3838e-04\n",
      "Epoch 58/90\n",
      "9787/9787 [==============================] - 1s 54us/step - loss: 6.7276e-04 - acc: 1.0218e-04 - val_loss: 0.0021 - val_acc: 2.3838e-04\n",
      "Epoch 59/90\n",
      "9787/9787 [==============================] - 0s 47us/step - loss: 6.2447e-04 - acc: 1.0218e-04 - val_loss: 0.0031 - val_acc: 2.3838e-04\n",
      "Epoch 60/90\n",
      "9787/9787 [==============================] - 0s 48us/step - loss: 6.0727e-04 - acc: 1.0218e-04 - val_loss: 0.0032 - val_acc: 2.3838e-04\n",
      "Epoch 61/90\n",
      "9787/9787 [==============================] - 0s 48us/step - loss: 6.6346e-04 - acc: 1.0218e-04 - val_loss: 0.0025 - val_acc: 2.3838e-04\n",
      "Epoch 62/90\n",
      "9787/9787 [==============================] - 0s 48us/step - loss: 6.2853e-04 - acc: 1.0218e-04 - val_loss: 0.0020 - val_acc: 2.3838e-04\n",
      "Epoch 63/90\n",
      "9787/9787 [==============================] - 0s 47us/step - loss: 6.3182e-04 - acc: 1.0218e-04 - val_loss: 0.0023 - val_acc: 2.3838e-04\n",
      "Epoch 64/90\n",
      "9787/9787 [==============================] - 0s 46us/step - loss: 6.1143e-04 - acc: 1.0218e-04 - val_loss: 0.0057 - val_acc: 2.3838e-04\n",
      "Epoch 65/90\n",
      "9787/9787 [==============================] - 0s 48us/step - loss: 6.5772e-04 - acc: 1.0218e-04 - val_loss: 0.0031 - val_acc: 2.3838e-04\n",
      "Epoch 66/90\n",
      "9787/9787 [==============================] - 0s 51us/step - loss: 6.2519e-04 - acc: 1.0218e-04 - val_loss: 0.0025 - val_acc: 2.3838e-04\n",
      "Epoch 67/90\n",
      "9787/9787 [==============================] - 0s 48us/step - loss: 5.9594e-04 - acc: 1.0218e-04 - val_loss: 0.0021 - val_acc: 2.3838e-04\n",
      "Epoch 68/90\n",
      "9787/9787 [==============================] - 0s 46us/step - loss: 5.8191e-04 - acc: 1.0218e-04 - val_loss: 0.0025 - val_acc: 2.3838e-04\n",
      "Epoch 69/90\n",
      "9787/9787 [==============================] - 0s 45us/step - loss: 5.8305e-04 - acc: 1.0218e-04 - val_loss: 0.0029 - val_acc: 2.3838e-04\n",
      "Epoch 70/90\n",
      "9787/9787 [==============================] - 0s 47us/step - loss: 6.0643e-04 - acc: 1.0218e-04 - val_loss: 0.0033 - val_acc: 2.3838e-04\n",
      "Epoch 71/90\n",
      "9787/9787 [==============================] - 0s 46us/step - loss: 5.6028e-04 - acc: 1.0218e-04 - val_loss: 0.0021 - val_acc: 2.3838e-04\n",
      "Epoch 72/90\n",
      "9787/9787 [==============================] - 0s 47us/step - loss: 5.8369e-04 - acc: 1.0218e-04 - val_loss: 0.0029 - val_acc: 2.3838e-04\n",
      "Epoch 73/90\n",
      "9787/9787 [==============================] - 0s 47us/step - loss: 5.7684e-04 - acc: 1.0218e-04 - val_loss: 0.0015 - val_acc: 2.3838e-04\n",
      "Epoch 74/90\n",
      "9787/9787 [==============================] - 1s 54us/step - loss: 6.0912e-04 - acc: 1.0218e-04 - val_loss: 0.0017 - val_acc: 2.3838e-04\n",
      "Epoch 75/90\n",
      "9787/9787 [==============================] - 1s 56us/step - loss: 5.6681e-04 - acc: 1.0218e-04 - val_loss: 0.0013 - val_acc: 2.3838e-04\n",
      "Epoch 76/90\n",
      "9787/9787 [==============================] - 1s 67us/step - loss: 5.7020e-04 - acc: 1.0218e-04 - val_loss: 0.0014 - val_acc: 2.3838e-04\n",
      "Epoch 77/90\n",
      "9787/9787 [==============================] - 0s 50us/step - loss: 5.4099e-04 - acc: 1.0218e-04 - val_loss: 0.0019 - val_acc: 2.3838e-04\n",
      "Epoch 78/90\n",
      "9787/9787 [==============================] - 0s 47us/step - loss: 5.5338e-04 - acc: 1.0218e-04 - val_loss: 0.0030 - val_acc: 2.3838e-04\n",
      "Epoch 79/90\n",
      "9787/9787 [==============================] - 0s 46us/step - loss: 5.3578e-04 - acc: 1.0218e-04 - val_loss: 0.0026 - val_acc: 2.3838e-04\n",
      "Epoch 80/90\n",
      "9787/9787 [==============================] - 0s 45us/step - loss: 5.3736e-04 - acc: 1.0218e-04 - val_loss: 0.0024 - val_acc: 2.3838e-04\n",
      "Epoch 81/90\n",
      "9787/9787 [==============================] - 0s 46us/step - loss: 5.1314e-04 - acc: 1.0218e-04 - val_loss: 0.0023 - val_acc: 2.3838e-04\n",
      "Epoch 82/90\n",
      "9787/9787 [==============================] - 0s 47us/step - loss: 4.9580e-04 - acc: 1.0218e-04 - val_loss: 0.0022 - val_acc: 2.3838e-04\n",
      "Epoch 83/90\n",
      "9787/9787 [==============================] - 0s 45us/step - loss: 4.8970e-04 - acc: 1.0218e-04 - val_loss: 0.0013 - val_acc: 2.3838e-04\n",
      "Epoch 84/90\n",
      "9787/9787 [==============================] - 0s 49us/step - loss: 5.1551e-04 - acc: 1.0218e-04 - val_loss: 0.0017 - val_acc: 2.3838e-04\n",
      "Epoch 85/90\n",
      "9787/9787 [==============================] - 0s 47us/step - loss: 5.1454e-04 - acc: 1.0218e-04 - val_loss: 0.0012 - val_acc: 2.3838e-04\n",
      "Epoch 86/90\n",
      "9787/9787 [==============================] - 0s 48us/step - loss: 5.0859e-04 - acc: 1.0218e-04 - val_loss: 8.6457e-04 - val_acc: 2.3838e-04\n",
      "Epoch 87/90\n",
      "9787/9787 [==============================] - 0s 46us/step - loss: 5.6159e-04 - acc: 1.0218e-04 - val_loss: 0.0012 - val_acc: 2.3838e-04\n",
      "Epoch 88/90\n",
      "9787/9787 [==============================] - 0s 46us/step - loss: 5.1183e-04 - acc: 1.0218e-04 - val_loss: 0.0013 - val_acc: 2.3838e-04\n",
      "Epoch 89/90\n",
      "9787/9787 [==============================] - 0s 48us/step - loss: 5.5043e-04 - acc: 1.0218e-04 - val_loss: 0.0023 - val_acc: 2.3838e-04\n",
      "Epoch 90/90\n",
      "9787/9787 [==============================] - 0s 47us/step - loss: 5.0667e-04 - acc: 1.0218e-04 - val_loss: 0.0020 - val_acc: 2.3838e-04\n",
      "-52.48642826939719\n",
      "Amount of features = 9\n",
      "Amount of training data = 13779\n",
      "Amount of testing data = 5905\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_21 (Conv1D)           (None, 21, 64)            1216      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 10, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 9, 64)             8256      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 4, 64)             0         \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 4, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 250)               64250     \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 1)                 251       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 73,973\n",
      "Trainable params: 73,973\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 9629 samples, validate on 4127 samples\n",
      "Epoch 1/90\n",
      "9629/9629 [==============================] - 1s 136us/step - loss: 0.0394 - acc: 1.0385e-04 - val_loss: 0.0037 - val_acc: 2.4231e-04\n",
      "Epoch 2/90\n",
      "9629/9629 [==============================] - 0s 43us/step - loss: 0.0068 - acc: 1.0385e-04 - val_loss: 0.0030 - val_acc: 2.4231e-04\n",
      "Epoch 3/90\n",
      "9629/9629 [==============================] - 0s 43us/step - loss: 0.0042 - acc: 1.0385e-04 - val_loss: 0.0017 - val_acc: 2.4231e-04\n",
      "Epoch 4/90\n",
      "9629/9629 [==============================] - 0s 43us/step - loss: 0.0037 - acc: 1.0385e-04 - val_loss: 0.0043 - val_acc: 2.4231e-04\n",
      "Epoch 5/90\n",
      "9629/9629 [==============================] - 0s 42us/step - loss: 0.0029 - acc: 1.0385e-04 - val_loss: 0.0025 - val_acc: 2.4231e-04\n",
      "Epoch 6/90\n",
      "9629/9629 [==============================] - 0s 44us/step - loss: 0.0026 - acc: 1.0385e-04 - val_loss: 0.0018 - val_acc: 2.4231e-04\n",
      "Epoch 7/90\n",
      "9629/9629 [==============================] - 0s 45us/step - loss: 0.0024 - acc: 1.0385e-04 - val_loss: 0.0011 - val_acc: 2.4231e-04\n",
      "Epoch 8/90\n",
      "9629/9629 [==============================] - 0s 48us/step - loss: 0.0022 - acc: 1.0385e-04 - val_loss: 0.0018 - val_acc: 2.4231e-04\n",
      "Epoch 9/90\n",
      "9629/9629 [==============================] - 0s 47us/step - loss: 0.0020 - acc: 1.0385e-04 - val_loss: 0.0010 - val_acc: 2.4231e-04\n",
      "Epoch 10/90\n",
      "9629/9629 [==============================] - 0s 48us/step - loss: 0.0020 - acc: 1.0385e-04 - val_loss: 0.0021 - val_acc: 2.4231e-04\n",
      "Epoch 11/90\n",
      "9629/9629 [==============================] - 1s 53us/step - loss: 0.0018 - acc: 1.0385e-04 - val_loss: 0.0029 - val_acc: 2.4231e-04\n",
      "Epoch 12/90\n",
      "9629/9629 [==============================] - 0s 50us/step - loss: 0.0018 - acc: 1.0385e-04 - val_loss: 0.0015 - val_acc: 2.4231e-04\n",
      "Epoch 13/90\n",
      "9629/9629 [==============================] - 0s 45us/step - loss: 0.0017 - acc: 1.0385e-04 - val_loss: 0.0013 - val_acc: 2.4231e-04\n",
      "Epoch 14/90\n",
      "9629/9629 [==============================] - 0s 45us/step - loss: 0.0015 - acc: 1.0385e-04 - val_loss: 0.0010 - val_acc: 2.4231e-04\n",
      "Epoch 15/90\n",
      "9629/9629 [==============================] - 0s 48us/step - loss: 0.0016 - acc: 1.0385e-04 - val_loss: 0.0016 - val_acc: 2.4231e-04\n",
      "Epoch 16/90\n",
      "9629/9629 [==============================] - 0s 44us/step - loss: 0.0015 - acc: 1.0385e-04 - val_loss: 7.9551e-04 - val_acc: 2.4231e-04\n",
      "Epoch 17/90\n",
      "9629/9629 [==============================] - 0s 43us/step - loss: 0.0015 - acc: 1.0385e-04 - val_loss: 0.0014 - val_acc: 2.4231e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/90\n",
      "9629/9629 [==============================] - 0s 47us/step - loss: 0.0014 - acc: 1.0385e-04 - val_loss: 9.0168e-04 - val_acc: 2.4231e-04\n",
      "Epoch 19/90\n",
      "9629/9629 [==============================] - 0s 46us/step - loss: 0.0014 - acc: 1.0385e-04 - val_loss: 0.0011 - val_acc: 2.4231e-04\n",
      "Epoch 20/90\n",
      "9629/9629 [==============================] - 0s 46us/step - loss: 0.0013 - acc: 1.0385e-04 - val_loss: 0.0011 - val_acc: 2.4231e-04\n",
      "Epoch 21/90\n",
      "9629/9629 [==============================] - 0s 49us/step - loss: 0.0013 - acc: 1.0385e-04 - val_loss: 8.0191e-04 - val_acc: 2.4231e-04\n",
      "Epoch 22/90\n",
      "9629/9629 [==============================] - 0s 51us/step - loss: 0.0013 - acc: 1.0385e-04 - val_loss: 0.0012 - val_acc: 2.4231e-04\n",
      "Epoch 23/90\n",
      "9629/9629 [==============================] - 1s 53us/step - loss: 0.0013 - acc: 1.0385e-04 - val_loss: 9.9480e-04 - val_acc: 2.4231e-04\n",
      "Epoch 24/90\n",
      "9629/9629 [==============================] - 0s 46us/step - loss: 0.0012 - acc: 1.0385e-04 - val_loss: 8.1323e-04 - val_acc: 2.4231e-04\n",
      "Epoch 25/90\n",
      "9629/9629 [==============================] - 0s 45us/step - loss: 0.0012 - acc: 1.0385e-04 - val_loss: 0.0016 - val_acc: 2.4231e-04\n",
      "Epoch 26/90\n",
      "9629/9629 [==============================] - 0s 45us/step - loss: 0.0012 - acc: 1.0385e-04 - val_loss: 7.4530e-04 - val_acc: 2.4231e-04\n",
      "Epoch 27/90\n",
      "9629/9629 [==============================] - 0s 47us/step - loss: 0.0012 - acc: 1.0385e-04 - val_loss: 7.5636e-04 - val_acc: 2.4231e-04\n",
      "Epoch 28/90\n",
      "9629/9629 [==============================] - 0s 45us/step - loss: 0.0012 - acc: 1.0385e-04 - val_loss: 0.0014 - val_acc: 2.4231e-04\n",
      "Epoch 29/90\n",
      "9629/9629 [==============================] - 0s 45us/step - loss: 0.0012 - acc: 1.0385e-04 - val_loss: 7.4744e-04 - val_acc: 2.4231e-04\n",
      "Epoch 30/90\n",
      "9629/9629 [==============================] - 0s 45us/step - loss: 0.0012 - acc: 1.0385e-04 - val_loss: 9.5742e-04 - val_acc: 2.4231e-04\n",
      "Epoch 31/90\n",
      "9629/9629 [==============================] - 0s 46us/step - loss: 0.0011 - acc: 1.0385e-04 - val_loss: 6.5040e-04 - val_acc: 2.4231e-04\n",
      "Epoch 32/90\n",
      "9629/9629 [==============================] - 0s 47us/step - loss: 0.0011 - acc: 1.0385e-04 - val_loss: 6.7142e-04 - val_acc: 2.4231e-04\n",
      "Epoch 33/90\n",
      "9629/9629 [==============================] - 0s 48us/step - loss: 0.0011 - acc: 1.0385e-04 - val_loss: 7.2972e-04 - val_acc: 2.4231e-04\n",
      "Epoch 34/90\n",
      "9629/9629 [==============================] - 0s 52us/step - loss: 0.0010 - acc: 1.0385e-04 - val_loss: 9.5304e-04 - val_acc: 2.4231e-04\n",
      "Epoch 35/90\n",
      "9629/9629 [==============================] - 0s 48us/step - loss: 0.0011 - acc: 1.0385e-04 - val_loss: 0.0010 - val_acc: 2.4231e-04\n",
      "Epoch 36/90\n",
      "9629/9629 [==============================] - 0s 46us/step - loss: 0.0011 - acc: 1.0385e-04 - val_loss: 8.1958e-04 - val_acc: 2.4231e-04\n",
      "Epoch 37/90\n",
      "9629/9629 [==============================] - 0s 44us/step - loss: 0.0010 - acc: 1.0385e-04 - val_loss: 6.3534e-04 - val_acc: 2.4231e-04\n",
      "Epoch 38/90\n",
      "9629/9629 [==============================] - 0s 43us/step - loss: 0.0011 - acc: 1.0385e-04 - val_loss: 6.1756e-04 - val_acc: 2.4231e-04\n",
      "Epoch 39/90\n",
      "9629/9629 [==============================] - 0s 43us/step - loss: 0.0010 - acc: 1.0385e-04 - val_loss: 7.9532e-04 - val_acc: 2.4231e-04\n",
      "Epoch 40/90\n",
      "9629/9629 [==============================] - 0s 43us/step - loss: 0.0010 - acc: 1.0385e-04 - val_loss: 5.8692e-04 - val_acc: 2.4231e-04\n",
      "Epoch 41/90\n",
      "9629/9629 [==============================] - 0s 43us/step - loss: 0.0011 - acc: 1.0385e-04 - val_loss: 6.5773e-04 - val_acc: 2.4231e-04\n",
      "Epoch 42/90\n",
      "9629/9629 [==============================] - 0s 43us/step - loss: 0.0010 - acc: 1.0385e-04 - val_loss: 9.5487e-04 - val_acc: 2.4231e-04\n",
      "Epoch 43/90\n",
      "9629/9629 [==============================] - 0s 45us/step - loss: 0.0010 - acc: 1.0385e-04 - val_loss: 0.0014 - val_acc: 2.4231e-04\n",
      "Epoch 44/90\n",
      "9629/9629 [==============================] - 0s 44us/step - loss: 0.0010 - acc: 1.0385e-04 - val_loss: 5.5154e-04 - val_acc: 2.4231e-04\n",
      "Epoch 45/90\n",
      "9629/9629 [==============================] - 0s 46us/step - loss: 9.8040e-04 - acc: 1.0385e-04 - val_loss: 7.6584e-04 - val_acc: 2.4231e-04\n",
      "Epoch 46/90\n",
      "9629/9629 [==============================] - 0s 46us/step - loss: 0.0010 - acc: 1.0385e-04 - val_loss: 6.9068e-04 - val_acc: 2.4231e-04\n",
      "Epoch 47/90\n",
      "9629/9629 [==============================] - 0s 52us/step - loss: 0.0011 - acc: 1.0385e-04 - val_loss: 0.0012 - val_acc: 2.4231e-04\n",
      "Epoch 48/90\n",
      "9629/9629 [==============================] - 0s 48us/step - loss: 0.0011 - acc: 1.0385e-04 - val_loss: 0.0012 - val_acc: 2.4231e-04\n",
      "Epoch 49/90\n",
      "9629/9629 [==============================] - 0s 46us/step - loss: 9.7981e-04 - acc: 1.0385e-04 - val_loss: 6.2855e-04 - val_acc: 2.4231e-04\n",
      "Epoch 50/90\n",
      "9629/9629 [==============================] - 0s 46us/step - loss: 9.2574e-04 - acc: 1.0385e-04 - val_loss: 7.9525e-04 - val_acc: 2.4231e-04\n",
      "Epoch 51/90\n",
      "9629/9629 [==============================] - 0s 46us/step - loss: 9.4633e-04 - acc: 1.0385e-04 - val_loss: 0.0010 - val_acc: 2.4231e-04\n",
      "Epoch 52/90\n",
      "9629/9629 [==============================] - 0s 45us/step - loss: 9.4395e-04 - acc: 1.0385e-04 - val_loss: 7.1523e-04 - val_acc: 2.4231e-04\n",
      "Epoch 53/90\n",
      "9629/9629 [==============================] - 0s 47us/step - loss: 9.1704e-04 - acc: 1.0385e-04 - val_loss: 5.1619e-04 - val_acc: 2.4231e-04\n",
      "Epoch 54/90\n",
      "9629/9629 [==============================] - 0s 46us/step - loss: 9.1103e-04 - acc: 1.0385e-04 - val_loss: 7.4644e-04 - val_acc: 2.4231e-04\n",
      "Epoch 55/90\n",
      "9629/9629 [==============================] - 0s 45us/step - loss: 8.9991e-04 - acc: 1.0385e-04 - val_loss: 6.8299e-04 - val_acc: 2.4231e-04\n",
      "Epoch 56/90\n",
      "9629/9629 [==============================] - 0s 48us/step - loss: 9.2458e-04 - acc: 1.0385e-04 - val_loss: 0.0011 - val_acc: 2.4231e-04\n",
      "Epoch 57/90\n",
      "9629/9629 [==============================] - 0s 47us/step - loss: 9.7248e-04 - acc: 1.0385e-04 - val_loss: 0.0015 - val_acc: 2.4231e-04\n",
      "Epoch 58/90\n",
      "9629/9629 [==============================] - 0s 48us/step - loss: 0.0010 - acc: 1.0385e-04 - val_loss: 5.3331e-04 - val_acc: 2.4231e-04\n",
      "Epoch 59/90\n",
      "9629/9629 [==============================] - 0s 46us/step - loss: 9.2667e-04 - acc: 1.0385e-04 - val_loss: 6.4765e-04 - val_acc: 2.4231e-04\n",
      "Epoch 60/90\n",
      "9629/9629 [==============================] - 0s 46us/step - loss: 9.3834e-04 - acc: 1.0385e-04 - val_loss: 0.0011 - val_acc: 2.4231e-04\n",
      "Epoch 61/90\n",
      "9629/9629 [==============================] - 0s 46us/step - loss: 9.7221e-04 - acc: 1.0385e-04 - val_loss: 8.2103e-04 - val_acc: 2.4231e-04\n",
      "Epoch 62/90\n",
      "9629/9629 [==============================] - 0s 45us/step - loss: 9.0986e-04 - acc: 1.0385e-04 - val_loss: 6.9224e-04 - val_acc: 2.4231e-04\n",
      "Epoch 63/90\n",
      "9629/9629 [==============================] - 0s 46us/step - loss: 8.9037e-04 - acc: 1.0385e-04 - val_loss: 6.0848e-04 - val_acc: 2.4231e-04\n",
      "Epoch 64/90\n",
      "9629/9629 [==============================] - 0s 43us/step - loss: 8.4198e-04 - acc: 1.0385e-04 - val_loss: 5.7297e-04 - val_acc: 2.4231e-04\n",
      "Epoch 65/90\n",
      "9629/9629 [==============================] - 0s 51us/step - loss: 8.4503e-04 - acc: 1.0385e-04 - val_loss: 0.0011 - val_acc: 2.4231e-04\n",
      "Epoch 66/90\n",
      "9629/9629 [==============================] - 0s 47us/step - loss: 8.3669e-04 - acc: 1.0385e-04 - val_loss: 0.0012 - val_acc: 2.4231e-04\n",
      "Epoch 67/90\n",
      "9629/9629 [==============================] - 0s 47us/step - loss: 8.2302e-04 - acc: 1.0385e-04 - val_loss: 6.5713e-04 - val_acc: 2.4231e-04\n",
      "Epoch 68/90\n",
      "9629/9629 [==============================] - 0s 45us/step - loss: 8.4700e-04 - acc: 1.0385e-04 - val_loss: 0.0014 - val_acc: 2.4231e-04\n",
      "Epoch 69/90\n",
      "9629/9629 [==============================] - 0s 46us/step - loss: 8.3639e-04 - acc: 1.0385e-04 - val_loss: 4.3712e-04 - val_acc: 2.4231e-04\n",
      "Epoch 70/90\n",
      "9629/9629 [==============================] - 0s 49us/step - loss: 8.0574e-04 - acc: 1.0385e-04 - val_loss: 9.9174e-04 - val_acc: 2.4231e-04\n",
      "Epoch 71/90\n",
      "9629/9629 [==============================] - 1s 56us/step - loss: 8.4959e-04 - acc: 1.0385e-04 - val_loss: 7.1370e-04 - val_acc: 2.4231e-04\n",
      "Epoch 72/90\n",
      "9629/9629 [==============================] - 0s 47us/step - loss: 8.0171e-04 - acc: 1.0385e-04 - val_loss: 5.5352e-04 - val_acc: 2.4231e-04\n",
      "Epoch 73/90\n",
      "9629/9629 [==============================] - 0s 45us/step - loss: 7.7637e-04 - acc: 1.0385e-04 - val_loss: 5.9917e-04 - val_acc: 2.4231e-04\n",
      "Epoch 74/90\n",
      "9629/9629 [==============================] - 0s 48us/step - loss: 7.7632e-04 - acc: 1.0385e-04 - val_loss: 9.0350e-04 - val_acc: 2.4231e-04\n",
      "Epoch 75/90\n",
      "9629/9629 [==============================] - 0s 46us/step - loss: 8.0633e-04 - acc: 1.0385e-04 - val_loss: 9.0605e-04 - val_acc: 2.4231e-04\n",
      "Epoch 76/90\n",
      "9629/9629 [==============================] - 0s 49us/step - loss: 8.0301e-04 - acc: 1.0385e-04 - val_loss: 4.8362e-04 - val_acc: 2.4231e-04\n",
      "Epoch 77/90\n",
      "9629/9629 [==============================] - 0s 48us/step - loss: 7.6403e-04 - acc: 1.0385e-04 - val_loss: 5.5914e-04 - val_acc: 2.4231e-04\n",
      "Epoch 78/90\n",
      "9629/9629 [==============================] - 1s 53us/step - loss: 7.6485e-04 - acc: 1.0385e-04 - val_loss: 5.7341e-04 - val_acc: 2.4231e-04\n",
      "Epoch 79/90\n",
      "9629/9629 [==============================] - 0s 48us/step - loss: 7.4615e-04 - acc: 1.0385e-04 - val_loss: 4.1386e-04 - val_acc: 2.4231e-04\n",
      "Epoch 80/90\n",
      "9629/9629 [==============================] - 0s 48us/step - loss: 7.4324e-04 - acc: 1.0385e-04 - val_loss: 6.2082e-04 - val_acc: 2.4231e-04\n",
      "Epoch 81/90\n",
      "9629/9629 [==============================] - 0s 51us/step - loss: 7.4146e-04 - acc: 1.0385e-04 - val_loss: 4.6512e-04 - val_acc: 2.4231e-04\n",
      "Epoch 82/90\n",
      "9629/9629 [==============================] - 0s 50us/step - loss: 8.1199e-04 - acc: 1.0385e-04 - val_loss: 6.5005e-04 - val_acc: 2.4231e-04\n",
      "Epoch 83/90\n",
      "9629/9629 [==============================] - 0s 52us/step - loss: 7.9027e-04 - acc: 1.0385e-04 - val_loss: 9.1634e-04 - val_acc: 2.4231e-04\n",
      "Epoch 84/90\n",
      "9629/9629 [==============================] - 1s 53us/step - loss: 8.0557e-04 - acc: 1.0385e-04 - val_loss: 7.9427e-04 - val_acc: 2.4231e-04\n",
      "Epoch 85/90\n",
      "9629/9629 [==============================] - 0s 43us/step - loss: 7.2005e-04 - acc: 1.0385e-04 - val_loss: 5.8193e-04 - val_acc: 2.4231e-04\n",
      "Epoch 86/90\n",
      "9629/9629 [==============================] - 0s 44us/step - loss: 7.4728e-04 - acc: 1.0385e-04 - val_loss: 6.2165e-04 - val_acc: 2.4231e-04\n",
      "Epoch 87/90\n",
      "9629/9629 [==============================] - 0s 42us/step - loss: 7.3905e-04 - acc: 1.0385e-04 - val_loss: 4.5877e-04 - val_acc: 2.4231e-04\n",
      "Epoch 88/90\n",
      "9629/9629 [==============================] - 0s 42us/step - loss: 7.0943e-04 - acc: 1.0385e-04 - val_loss: 5.2162e-04 - val_acc: 2.4231e-04\n",
      "Epoch 89/90\n",
      "9629/9629 [==============================] - 0s 41us/step - loss: 6.9510e-04 - acc: 1.0385e-04 - val_loss: 5.9899e-04 - val_acc: 2.4231e-04\n",
      "Epoch 90/90\n",
      "9629/9629 [==============================] - 0s 43us/step - loss: 7.1273e-04 - acc: 1.0385e-04 - val_loss: 5.4669e-04 - val_acc: 2.4231e-04\n",
      "-51.12349306712659\n"
     ]
    }
   ],
   "source": [
    "currency_list=[ 'GBP Curncy',\n",
    " 'JPY Curncy',\n",
    " 'EUR Curncy',\n",
    " 'CAD Curncy',\n",
    " 'NZD Curncy',\n",
    " 'SEK Curncy',\n",
    " 'AUD Curncy',\n",
    " 'CHF Curncy',\n",
    " 'NOK Curncy',\n",
    " 'ZAR Curncy']\n",
    "predictcur=portfolio(currency_list,file = 'FX-5-merg.xlsx',seq_len = 22,shape = [seq_len, 9, 1],neurons = [256, 256, 32, 1],dropout = 0.3,decay = 0.5,\n",
    "              epochs = 90,ma=[50, 100, 200],split=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MarkowitzWithTransactionsCost(n,mu,GT,x0,w,gamma,f,g):\n",
    "    # Upper bound on the traded amount\n",
    "    w0 = w+sum(x0)\n",
    "    u = n*[w0]\n",
    "\n",
    "    with Model(\"Markowitz portfolio with transaction costs\") as M:\n",
    "        #M.setLogHandler(sys.stdout)\n",
    "\n",
    "        # Defines the variables. No shortselling is allowed.\n",
    "        x = M.variable(\"x\", n, Domain.greaterThan(0.0))\n",
    "\n",
    "        # Additional \"helper\" variables \n",
    "        z = M.variable(\"z\", n, Domain.unbounded())   \n",
    "        # Binary variables\n",
    "        y = M.variable(\"y\", n, Domain.binary())\n",
    "\n",
    "        #  Maximize expected return\n",
    "        M.objective('obj', ObjectiveSense.Maximize, Expr.dot(mu,x))\n",
    "\n",
    "        # Invest amount + transactions costs = initial wealth\n",
    "        M.constraint('budget', Expr.add([ Expr.sum(x), Expr.dot(f,y),Expr.dot(g,z)] ), Domain.equalsTo(w0))\n",
    "\n",
    "        # Imposes a bound on the risk\n",
    "        M.constraint('risk', Expr.vstack( gamma,Expr.mul(GT,x)), Domain.inQCone())\n",
    "\n",
    "        # z >= |x-x0| \n",
    "        M.constraint('buy', Expr.sub(z,Expr.sub(x,x0)),Domain.greaterThan(0.0))\n",
    "        M.constraint('sell', Expr.sub(z,Expr.sub(x0,x)),Domain.greaterThan(0.0))\n",
    "        # Alternatively, formulate the two constraints as\n",
    "        #M.constraint('trade', Expr.hstack(z,Expr.sub(x,x0)), Domain.inQcone())\n",
    "\n",
    "        # Constraints for turning y off and on. z-diag(u)*y<=0 i.e. z_j <= u_j*y_j\n",
    "        M.constraint('y_on_off', Expr.sub(z,Expr.mulElm(u,y)), Domain.lessThan(0.0))\n",
    "\n",
    "        # Integer optimization problems can be very hard to solve so limiting the \n",
    "        # maximum amount of time is a valuable safe guard\n",
    "        M.setSolverParam('mioMaxTime', 1000.0) \n",
    "        M.solve()\n",
    "\n",
    "        return (np.dot(mu,x.level()), x.level())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebalance(n,previous_prices,x0,w,mu,gamma=1):\n",
    "    GT=np.cov(previous_prices)\n",
    "    f = n*[0.0001]\n",
    "    g = n*[0.0005]\n",
    "    _,weights=MarkowitzWithTransactionsCost(n,mu,GT,x0,w,gamma,f,g)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_diff(data):\n",
    "    return np.diff(np.log(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtest(prices, predictions, initial_weights):\n",
    "    t_prices = len(prices[1,:])\n",
    "    t_predictions = len(predictions)\n",
    "    length_past = t_prices - t_predictions\n",
    "    returns = np.apply_along_axis(log_diff, 1, prices)\n",
    "    prediction_return = []\n",
    "    port_return=1\n",
    "    for k in range(t_predictions):\n",
    "        prediction_return.append(np.log(predictions[k]/prices[:,length_past+k-1]))\n",
    "    weights = initial_weights\n",
    "    portfolio_return = []\n",
    "    prev_weight = weights\n",
    "    for i in range(0,t_predictions-1):\n",
    "        print(i)\n",
    "        predicted_return = prediction_return[i]\n",
    "        previous_return = returns[:,length_past+i-1]\n",
    "        previous_returns = returns[:,0:(length_past+i-1)]\n",
    "        if i==0:\n",
    "            new_weight = rebalance(10,previous_returns,mu=prices[:,length_past+k].tolist(),x0=prev_weight,w=1,gamma=0.05)\n",
    "        else:\n",
    "            new_weight = rebalance(10,previous_returns,mu=prices[:,length_past+k].tolist(),x0=prev_weight,w=np.sum(period_return),gamma=0.05)\n",
    "        period_return = new_weight*np.log(prices[:,length_past+i]/prices[:,length_past+i-1])\n",
    "        port_return=port_return*(1+np.sum(period_return))\n",
    "        portfolio_return.append(port_return)\n",
    "        prev_weight = new_weight\n",
    "        print(new_weight)\n",
    "    return portfolio_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'rebalance' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-3f07c31b8c93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbacktest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_close\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictcur\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-34f99ba58ed5>\u001b[0m in \u001b[0;36mbacktest\u001b[0;34m(prices, predictions, initial_weights)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mprevious_returns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength_past\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mnew_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrebalance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprevious_returns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlength_past\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprev_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mnew_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrebalance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprevious_returns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlength_past\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprev_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperiod_return\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rebalance' is not defined"
     ]
    }
   ],
   "source": [
    "x=backtest(df_close.T, predictcur, np.repeat(1/10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9178360510468676"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a32723780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x)\n",
    "x[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_result(stock_name, normalized_value_p, normalized_value_y_test):\n",
    "    newp = denormalize(stock_name, normalized_value_p,predict=True)\n",
    "    newy_test = denormalize(stock_name, normalized_value_y_test,predict=False)\n",
    "    plt2.plot(newp, color='red', label='Prediction')\n",
    "    plt2.plot(newy_test,color='blue', label='Actual')\n",
    "    plt2.legend(loc='best')\n",
    "    plt2.title('The test result for {}'.format(stock_name))\n",
    "    plt2.xlabel('5 Min ahead Forecast')\n",
    "    plt2.ylabel('Price')\n",
    "    plt2.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
