{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt2\n",
    "import pandas as pd\n",
    "from pandas import datetime\n",
    "import math, time\n",
    "import itertools\n",
    "from sklearn import preprocessing\n",
    "import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import load_model\n",
    "import keras\n",
    "import h5py\n",
    "import os\n",
    "from statistics import mean\n",
    "from keras import backend as K\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D\n",
    "from keras.layers.core import Flatten\n",
    "from mosek.fusion import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Initialize Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 22\n",
    "shape = [seq_len, 9, 1]\n",
    "neurons = [256, 256, 32, 1]\n",
    "dropout = 0.3\n",
    "decay = 0.5\n",
    "epochs = 90\n",
    "os.chdir(\"/Users/youssefberrada/Dropbox (MIT)/15.961 Independant Study/Data\")\n",
    "#os.chdir(\"/Users/michelcassard/Dropbox (MIT)/15.960 Independant Study/Data\")\n",
    "file = 'FX-5-merg.xlsx'\n",
    "# Load spreadsheet\n",
    "xl = pd.ExcelFile(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_data(stock_name, ma=[]):\n",
    "    \"\"\"\n",
    "    Return a dataframe of that stock and normalize all the values. \n",
    "    (Optional: create moving average)\n",
    "    \n",
    "    \"\"\"\n",
    "    df = xl.parse(stock_name)\n",
    "    df.set_index('Date', inplace=True)\n",
    "    \n",
    "    # Renaming all the columns so that we can use the old version code\n",
    "    df.rename(columns={'OPEN': 'Open', 'HIGH': 'High', 'LOW': 'Low', 'NUMBER_TICKS': 'Volume', 'LAST_PRICE': 'Adj Close'}, inplace=True)\n",
    "     # Percentage change\n",
    "    df['Pct'] = df['Adj Close'].pct_change()\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    # Moving Average    \n",
    "    if ma != []:\n",
    "        for moving in ma:\n",
    "            df['{}ma'.format(moving)] = df['Adj Close'].rolling(window=moving).mean()\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "  \n",
    "    # Move Adj Close to the rightmost for the ease of training\n",
    "    adj_close = df['Adj Close']\n",
    "    df.drop(labels=['Adj Close'], axis=1, inplace=True)\n",
    "    df = pd.concat([df, adj_close], axis=1)\n",
    "      \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_JPY=get_stock_data(\"JPY Curncy\",  ma=[50, 100, 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Pct</th>\n",
       "      <th>50ma</th>\n",
       "      <th>100ma</th>\n",
       "      <th>200ma</th>\n",
       "      <th>Adj Close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-09-26 09:45:00</th>\n",
       "      <td>112.19</td>\n",
       "      <td>112.16</td>\n",
       "      <td>112.17</td>\n",
       "      <td>4393</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>111.9248</td>\n",
       "      <td>111.7835</td>\n",
       "      <td>111.71785</td>\n",
       "      <td>112.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-26 09:50:00</th>\n",
       "      <td>112.19</td>\n",
       "      <td>112.14</td>\n",
       "      <td>112.19</td>\n",
       "      <td>4507</td>\n",
       "      <td>-0.000357</td>\n",
       "      <td>111.9336</td>\n",
       "      <td>111.7891</td>\n",
       "      <td>111.72000</td>\n",
       "      <td>112.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-26 09:55:00</th>\n",
       "      <td>112.19</td>\n",
       "      <td>112.14</td>\n",
       "      <td>112.15</td>\n",
       "      <td>3889</td>\n",
       "      <td>0.000357</td>\n",
       "      <td>111.9438</td>\n",
       "      <td>111.7956</td>\n",
       "      <td>111.72235</td>\n",
       "      <td>112.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-26 10:00:00</th>\n",
       "      <td>112.19</td>\n",
       "      <td>112.10</td>\n",
       "      <td>112.19</td>\n",
       "      <td>5530</td>\n",
       "      <td>-0.000446</td>\n",
       "      <td>111.9528</td>\n",
       "      <td>111.8018</td>\n",
       "      <td>111.72440</td>\n",
       "      <td>112.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-26 10:05:00</th>\n",
       "      <td>112.18</td>\n",
       "      <td>112.13</td>\n",
       "      <td>112.14</td>\n",
       "      <td>5066</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>111.9622</td>\n",
       "      <td>111.8078</td>\n",
       "      <td>111.72650</td>\n",
       "      <td>112.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-26 10:10:00</th>\n",
       "      <td>112.18</td>\n",
       "      <td>112.14</td>\n",
       "      <td>112.15</td>\n",
       "      <td>4417</td>\n",
       "      <td>0.000267</td>\n",
       "      <td>111.9720</td>\n",
       "      <td>111.8141</td>\n",
       "      <td>111.72865</td>\n",
       "      <td>112.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-26 10:15:00</th>\n",
       "      <td>112.19</td>\n",
       "      <td>112.13</td>\n",
       "      <td>112.18</td>\n",
       "      <td>4455</td>\n",
       "      <td>-0.000267</td>\n",
       "      <td>111.9808</td>\n",
       "      <td>111.8200</td>\n",
       "      <td>111.73070</td>\n",
       "      <td>112.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-26 10:20:00</th>\n",
       "      <td>112.18</td>\n",
       "      <td>112.13</td>\n",
       "      <td>112.15</td>\n",
       "      <td>4600</td>\n",
       "      <td>-0.000089</td>\n",
       "      <td>111.9892</td>\n",
       "      <td>111.8251</td>\n",
       "      <td>111.73275</td>\n",
       "      <td>112.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-26 10:25:00</th>\n",
       "      <td>112.15</td>\n",
       "      <td>112.10</td>\n",
       "      <td>112.14</td>\n",
       "      <td>4531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>111.9976</td>\n",
       "      <td>111.8305</td>\n",
       "      <td>111.73485</td>\n",
       "      <td>112.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-26 10:30:00</th>\n",
       "      <td>112.15</td>\n",
       "      <td>112.09</td>\n",
       "      <td>112.15</td>\n",
       "      <td>4758</td>\n",
       "      <td>-0.000268</td>\n",
       "      <td>112.0046</td>\n",
       "      <td>111.8362</td>\n",
       "      <td>111.73675</td>\n",
       "      <td>112.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-26 10:35:00</th>\n",
       "      <td>112.16</td>\n",
       "      <td>112.08</td>\n",
       "      <td>112.11</td>\n",
       "      <td>4821</td>\n",
       "      <td>0.000357</td>\n",
       "      <td>112.0126</td>\n",
       "      <td>111.8425</td>\n",
       "      <td>111.73890</td>\n",
       "      <td>112.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-26 10:40:00</th>\n",
       "      <td>112.18</td>\n",
       "      <td>112.14</td>\n",
       "      <td>112.15</td>\n",
       "      <td>4136</td>\n",
       "      <td>0.000267</td>\n",
       "      <td>112.0194</td>\n",
       "      <td>111.8490</td>\n",
       "      <td>111.74135</td>\n",
       "      <td>112.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-26 10:45:00</th>\n",
       "      <td>112.18</td>\n",
       "      <td>112.12</td>\n",
       "      <td>112.18</td>\n",
       "      <td>4212</td>\n",
       "      <td>-0.000535</td>\n",
       "      <td>112.0250</td>\n",
       "      <td>111.8549</td>\n",
       "      <td>111.74350</td>\n",
       "      <td>112.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-26 10:50:00</th>\n",
       "      <td>112.28</td>\n",
       "      <td>112.11</td>\n",
       "      <td>112.12</td>\n",
       "      <td>5586</td>\n",
       "      <td>0.001338</td>\n",
       "      <td>112.0340</td>\n",
       "      <td>111.8624</td>\n",
       "      <td>111.74620</td>\n",
       "      <td>112.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-26 10:55:00</th>\n",
       "      <td>112.30</td>\n",
       "      <td>112.23</td>\n",
       "      <td>112.27</td>\n",
       "      <td>5724</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>112.0430</td>\n",
       "      <td>111.8696</td>\n",
       "      <td>111.74885</td>\n",
       "      <td>112.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-26 11:00:00</th>\n",
       "      <td>112.29</td>\n",
       "      <td>112.22</td>\n",
       "      <td>112.28</td>\n",
       "      <td>5388</td>\n",
       "      <td>-0.000267</td>\n",
       "      <td>112.0520</td>\n",
       "      <td>111.8760</td>\n",
       "      <td>111.75125</td>\n",
       "      <td>112.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-26 11:05:00</th>\n",
       "      <td>112.26</td>\n",
       "      <td>112.22</td>\n",
       "      <td>112.25</td>\n",
       "      <td>4389</td>\n",
       "      <td>-0.000178</td>\n",
       "      <td>112.0612</td>\n",
       "      <td>111.8820</td>\n",
       "      <td>111.75355</td>\n",
       "      <td>112.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-26 11:10:00</th>\n",
       "      <td>112.25</td>\n",
       "      <td>112.18</td>\n",
       "      <td>112.23</td>\n",
       "      <td>4653</td>\n",
       "      <td>-0.000446</td>\n",
       "      <td>112.0692</td>\n",
       "      <td>111.8877</td>\n",
       "      <td>111.75565</td>\n",
       "      <td>112.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-26 11:15:00</th>\n",
       "      <td>112.22</td>\n",
       "      <td>112.17</td>\n",
       "      <td>112.18</td>\n",
       "      <td>4359</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>112.0774</td>\n",
       "      <td>111.8939</td>\n",
       "      <td>111.75785</td>\n",
       "      <td>112.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-26 11:20:00</th>\n",
       "      <td>112.19</td>\n",
       "      <td>112.16</td>\n",
       "      <td>112.19</td>\n",
       "      <td>3719</td>\n",
       "      <td>-0.000178</td>\n",
       "      <td>112.0854</td>\n",
       "      <td>111.8993</td>\n",
       "      <td>111.75995</td>\n",
       "      <td>112.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-26 11:25:00</th>\n",
       "      <td>112.18</td>\n",
       "      <td>112.09</td>\n",
       "      <td>112.17</td>\n",
       "      <td>4696</td>\n",
       "      <td>-0.000357</td>\n",
       "      <td>112.0924</td>\n",
       "      <td>111.9039</td>\n",
       "      <td>111.76190</td>\n",
       "      <td>112.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-26 11:30:00</th>\n",
       "      <td>112.18</td>\n",
       "      <td>112.11</td>\n",
       "      <td>112.13</td>\n",
       "      <td>4220</td>\n",
       "      <td>0.000446</td>\n",
       "      <td>112.0984</td>\n",
       "      <td>111.9089</td>\n",
       "      <td>111.76415</td>\n",
       "      <td>112.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-26 11:35:00</th>\n",
       "      <td>112.20</td>\n",
       "      <td>112.14</td>\n",
       "      <td>112.18</td>\n",
       "      <td>4211</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>112.1044</td>\n",
       "      <td>111.9144</td>\n",
       "      <td>111.76650</td>\n",
       "      <td>112.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-26 11:40:00</th>\n",
       "      <td>112.20</td>\n",
       "      <td>112.15</td>\n",
       "      <td>112.19</td>\n",
       "      <td>3820</td>\n",
       "      <td>-0.000267</td>\n",
       "      <td>112.1090</td>\n",
       "      <td>111.9202</td>\n",
       "      <td>111.76860</td>\n",
       "      <td>112.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-26 11:45:00</th>\n",
       "      <td>112.20</td>\n",
       "      <td>112.15</td>\n",
       "      <td>112.16</td>\n",
       "      <td>3481</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>112.1138</td>\n",
       "      <td>111.9258</td>\n",
       "      <td>111.77080</td>\n",
       "      <td>112.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-26 11:50:00</th>\n",
       "      <td>112.21</td>\n",
       "      <td>112.18</td>\n",
       "      <td>112.18</td>\n",
       "      <td>3891</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>112.1186</td>\n",
       "      <td>111.9316</td>\n",
       "      <td>111.77305</td>\n",
       "      <td>112.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-26 11:55:00</th>\n",
       "      <td>112.23</td>\n",
       "      <td>112.14</td>\n",
       "      <td>112.20</td>\n",
       "      <td>4370</td>\n",
       "      <td>-0.000178</td>\n",
       "      <td>112.1232</td>\n",
       "      <td>111.9369</td>\n",
       "      <td>111.77530</td>\n",
       "      <td>112.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-26 12:00:00</th>\n",
       "      <td>112.19</td>\n",
       "      <td>112.16</td>\n",
       "      <td>112.18</td>\n",
       "      <td>3617</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>112.1282</td>\n",
       "      <td>111.9421</td>\n",
       "      <td>111.77765</td>\n",
       "      <td>112.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-26 12:05:00</th>\n",
       "      <td>112.20</td>\n",
       "      <td>112.17</td>\n",
       "      <td>112.19</td>\n",
       "      <td>3742</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>112.1330</td>\n",
       "      <td>111.9465</td>\n",
       "      <td>111.77990</td>\n",
       "      <td>112.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-26 12:10:00</th>\n",
       "      <td>112.21</td>\n",
       "      <td>112.19</td>\n",
       "      <td>112.19</td>\n",
       "      <td>3384</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>112.1378</td>\n",
       "      <td>111.9512</td>\n",
       "      <td>111.78200</td>\n",
       "      <td>112.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-29 14:30:00</th>\n",
       "      <td>112.64</td>\n",
       "      <td>112.63</td>\n",
       "      <td>112.64</td>\n",
       "      <td>2594</td>\n",
       "      <td>-0.000089</td>\n",
       "      <td>112.6194</td>\n",
       "      <td>112.5767</td>\n",
       "      <td>112.62345</td>\n",
       "      <td>112.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-29 14:35:00</th>\n",
       "      <td>112.66</td>\n",
       "      <td>112.63</td>\n",
       "      <td>112.63</td>\n",
       "      <td>2724</td>\n",
       "      <td>0.000266</td>\n",
       "      <td>112.6220</td>\n",
       "      <td>112.5781</td>\n",
       "      <td>112.62305</td>\n",
       "      <td>112.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-29 14:40:00</th>\n",
       "      <td>112.66</td>\n",
       "      <td>112.65</td>\n",
       "      <td>112.66</td>\n",
       "      <td>2499</td>\n",
       "      <td>-0.000089</td>\n",
       "      <td>112.6244</td>\n",
       "      <td>112.5795</td>\n",
       "      <td>112.62250</td>\n",
       "      <td>112.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-29 14:45:00</th>\n",
       "      <td>112.66</td>\n",
       "      <td>112.62</td>\n",
       "      <td>112.65</td>\n",
       "      <td>2728</td>\n",
       "      <td>-0.000266</td>\n",
       "      <td>112.6264</td>\n",
       "      <td>112.5806</td>\n",
       "      <td>112.62180</td>\n",
       "      <td>112.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-29 14:50:00</th>\n",
       "      <td>112.62</td>\n",
       "      <td>112.61</td>\n",
       "      <td>112.62</td>\n",
       "      <td>2724</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>112.6282</td>\n",
       "      <td>112.5817</td>\n",
       "      <td>112.62110</td>\n",
       "      <td>112.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-29 14:55:00</th>\n",
       "      <td>112.65</td>\n",
       "      <td>112.62</td>\n",
       "      <td>112.62</td>\n",
       "      <td>3097</td>\n",
       "      <td>0.000266</td>\n",
       "      <td>112.6310</td>\n",
       "      <td>112.5830</td>\n",
       "      <td>112.62040</td>\n",
       "      <td>112.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-29 15:00:00</th>\n",
       "      <td>112.65</td>\n",
       "      <td>112.63</td>\n",
       "      <td>112.65</td>\n",
       "      <td>3151</td>\n",
       "      <td>-0.000089</td>\n",
       "      <td>112.6336</td>\n",
       "      <td>112.5842</td>\n",
       "      <td>112.61975</td>\n",
       "      <td>112.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-29 15:05:00</th>\n",
       "      <td>112.65</td>\n",
       "      <td>112.64</td>\n",
       "      <td>112.64</td>\n",
       "      <td>2826</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>112.6338</td>\n",
       "      <td>112.5852</td>\n",
       "      <td>112.61910</td>\n",
       "      <td>112.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-29 15:10:00</th>\n",
       "      <td>112.65</td>\n",
       "      <td>112.64</td>\n",
       "      <td>112.64</td>\n",
       "      <td>2538</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>112.6336</td>\n",
       "      <td>112.5865</td>\n",
       "      <td>112.61850</td>\n",
       "      <td>112.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-29 15:15:00</th>\n",
       "      <td>112.65</td>\n",
       "      <td>112.63</td>\n",
       "      <td>112.64</td>\n",
       "      <td>2779</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>112.6340</td>\n",
       "      <td>112.5877</td>\n",
       "      <td>112.61780</td>\n",
       "      <td>112.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-29 15:20:00</th>\n",
       "      <td>112.64</td>\n",
       "      <td>112.63</td>\n",
       "      <td>112.64</td>\n",
       "      <td>2804</td>\n",
       "      <td>-0.000089</td>\n",
       "      <td>112.6350</td>\n",
       "      <td>112.5888</td>\n",
       "      <td>112.61715</td>\n",
       "      <td>112.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-29 15:25:00</th>\n",
       "      <td>112.64</td>\n",
       "      <td>112.63</td>\n",
       "      <td>112.63</td>\n",
       "      <td>2776</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>112.6354</td>\n",
       "      <td>112.5902</td>\n",
       "      <td>112.61650</td>\n",
       "      <td>112.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-29 15:30:00</th>\n",
       "      <td>112.66</td>\n",
       "      <td>112.64</td>\n",
       "      <td>112.64</td>\n",
       "      <td>3281</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>112.6356</td>\n",
       "      <td>112.5920</td>\n",
       "      <td>112.61590</td>\n",
       "      <td>112.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-29 15:35:00</th>\n",
       "      <td>112.67</td>\n",
       "      <td>112.65</td>\n",
       "      <td>112.66</td>\n",
       "      <td>3894</td>\n",
       "      <td>-0.000089</td>\n",
       "      <td>112.6360</td>\n",
       "      <td>112.5936</td>\n",
       "      <td>112.61530</td>\n",
       "      <td>112.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-29 15:40:00</th>\n",
       "      <td>112.66</td>\n",
       "      <td>112.65</td>\n",
       "      <td>112.65</td>\n",
       "      <td>3615</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>112.6362</td>\n",
       "      <td>112.5953</td>\n",
       "      <td>112.61475</td>\n",
       "      <td>112.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-29 15:45:00</th>\n",
       "      <td>112.66</td>\n",
       "      <td>112.64</td>\n",
       "      <td>112.66</td>\n",
       "      <td>3430</td>\n",
       "      <td>-0.000178</td>\n",
       "      <td>112.6362</td>\n",
       "      <td>112.5968</td>\n",
       "      <td>112.61425</td>\n",
       "      <td>112.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-29 15:50:00</th>\n",
       "      <td>112.65</td>\n",
       "      <td>112.63</td>\n",
       "      <td>112.64</td>\n",
       "      <td>3394</td>\n",
       "      <td>-0.000089</td>\n",
       "      <td>112.6354</td>\n",
       "      <td>112.5977</td>\n",
       "      <td>112.61360</td>\n",
       "      <td>112.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-29 15:55:00</th>\n",
       "      <td>112.69</td>\n",
       "      <td>112.63</td>\n",
       "      <td>112.63</td>\n",
       "      <td>4163</td>\n",
       "      <td>0.000533</td>\n",
       "      <td>112.6354</td>\n",
       "      <td>112.5993</td>\n",
       "      <td>112.61330</td>\n",
       "      <td>112.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-29 16:00:00</th>\n",
       "      <td>112.69</td>\n",
       "      <td>112.65</td>\n",
       "      <td>112.69</td>\n",
       "      <td>3530</td>\n",
       "      <td>-0.000177</td>\n",
       "      <td>112.6348</td>\n",
       "      <td>112.6005</td>\n",
       "      <td>112.61290</td>\n",
       "      <td>112.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-29 16:05:00</th>\n",
       "      <td>112.67</td>\n",
       "      <td>112.65</td>\n",
       "      <td>112.67</td>\n",
       "      <td>2977</td>\n",
       "      <td>-0.000089</td>\n",
       "      <td>112.6342</td>\n",
       "      <td>112.6015</td>\n",
       "      <td>112.61240</td>\n",
       "      <td>112.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-29 16:10:00</th>\n",
       "      <td>112.68</td>\n",
       "      <td>112.66</td>\n",
       "      <td>112.66</td>\n",
       "      <td>3193</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>112.6342</td>\n",
       "      <td>112.6026</td>\n",
       "      <td>112.61195</td>\n",
       "      <td>112.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-29 16:15:00</th>\n",
       "      <td>112.68</td>\n",
       "      <td>112.66</td>\n",
       "      <td>112.67</td>\n",
       "      <td>3143</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>112.6348</td>\n",
       "      <td>112.6038</td>\n",
       "      <td>112.61160</td>\n",
       "      <td>112.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-29 16:20:00</th>\n",
       "      <td>112.68</td>\n",
       "      <td>112.65</td>\n",
       "      <td>112.68</td>\n",
       "      <td>3276</td>\n",
       "      <td>-0.000089</td>\n",
       "      <td>112.6352</td>\n",
       "      <td>112.6046</td>\n",
       "      <td>112.61120</td>\n",
       "      <td>112.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-29 16:25:00</th>\n",
       "      <td>112.68</td>\n",
       "      <td>112.67</td>\n",
       "      <td>112.67</td>\n",
       "      <td>3389</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>112.6360</td>\n",
       "      <td>112.6058</td>\n",
       "      <td>112.61080</td>\n",
       "      <td>112.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-29 16:30:00</th>\n",
       "      <td>112.69</td>\n",
       "      <td>112.67</td>\n",
       "      <td>112.68</td>\n",
       "      <td>3273</td>\n",
       "      <td>-0.000089</td>\n",
       "      <td>112.6366</td>\n",
       "      <td>112.6069</td>\n",
       "      <td>112.61035</td>\n",
       "      <td>112.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-29 16:35:00</th>\n",
       "      <td>112.68</td>\n",
       "      <td>112.67</td>\n",
       "      <td>112.67</td>\n",
       "      <td>4063</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>112.6382</td>\n",
       "      <td>112.6082</td>\n",
       "      <td>112.61005</td>\n",
       "      <td>112.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-29 16:40:00</th>\n",
       "      <td>112.69</td>\n",
       "      <td>112.68</td>\n",
       "      <td>112.68</td>\n",
       "      <td>3437</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>112.6396</td>\n",
       "      <td>112.6094</td>\n",
       "      <td>112.60975</td>\n",
       "      <td>112.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-29 16:45:00</th>\n",
       "      <td>112.69</td>\n",
       "      <td>112.67</td>\n",
       "      <td>112.69</td>\n",
       "      <td>2354</td>\n",
       "      <td>-0.000177</td>\n",
       "      <td>112.6402</td>\n",
       "      <td>112.6107</td>\n",
       "      <td>112.60935</td>\n",
       "      <td>112.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-29 16:50:00</th>\n",
       "      <td>112.72</td>\n",
       "      <td>112.67</td>\n",
       "      <td>112.67</td>\n",
       "      <td>2473</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>112.6412</td>\n",
       "      <td>112.6120</td>\n",
       "      <td>112.60895</td>\n",
       "      <td>112.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-29 16:55:00</th>\n",
       "      <td>112.70</td>\n",
       "      <td>112.68</td>\n",
       "      <td>112.68</td>\n",
       "      <td>2431</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>112.6434</td>\n",
       "      <td>112.6135</td>\n",
       "      <td>112.60860</td>\n",
       "      <td>112.69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19606 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Open    High     Low  Volume       Pct      50ma  \\\n",
       "Date                                                                      \n",
       "2017-09-26 09:45:00  112.19  112.16  112.17    4393  0.000178  111.9248   \n",
       "2017-09-26 09:50:00  112.19  112.14  112.19    4507 -0.000357  111.9336   \n",
       "2017-09-26 09:55:00  112.19  112.14  112.15    3889  0.000357  111.9438   \n",
       "2017-09-26 10:00:00  112.19  112.10  112.19    5530 -0.000446  111.9528   \n",
       "2017-09-26 10:05:00  112.18  112.13  112.14    5066  0.000089  111.9622   \n",
       "2017-09-26 10:10:00  112.18  112.14  112.15    4417  0.000267  111.9720   \n",
       "2017-09-26 10:15:00  112.19  112.13  112.18    4455 -0.000267  111.9808   \n",
       "2017-09-26 10:20:00  112.18  112.13  112.15    4600 -0.000089  111.9892   \n",
       "2017-09-26 10:25:00  112.15  112.10  112.14    4531  0.000000  111.9976   \n",
       "2017-09-26 10:30:00  112.15  112.09  112.15    4758 -0.000268  112.0046   \n",
       "2017-09-26 10:35:00  112.16  112.08  112.11    4821  0.000357  112.0126   \n",
       "2017-09-26 10:40:00  112.18  112.14  112.15    4136  0.000267  112.0194   \n",
       "2017-09-26 10:45:00  112.18  112.12  112.18    4212 -0.000535  112.0250   \n",
       "2017-09-26 10:50:00  112.28  112.11  112.12    5586  0.001338  112.0340   \n",
       "2017-09-26 10:55:00  112.30  112.23  112.27    5724  0.000089  112.0430   \n",
       "2017-09-26 11:00:00  112.29  112.22  112.28    5388 -0.000267  112.0520   \n",
       "2017-09-26 11:05:00  112.26  112.22  112.25    4389 -0.000178  112.0612   \n",
       "2017-09-26 11:10:00  112.25  112.18  112.23    4653 -0.000446  112.0692   \n",
       "2017-09-26 11:15:00  112.22  112.17  112.18    4359  0.000089  112.0774   \n",
       "2017-09-26 11:20:00  112.19  112.16  112.19    3719 -0.000178  112.0854   \n",
       "2017-09-26 11:25:00  112.18  112.09  112.17    4696 -0.000357  112.0924   \n",
       "2017-09-26 11:30:00  112.18  112.11  112.13    4220  0.000446  112.0984   \n",
       "2017-09-26 11:35:00  112.20  112.14  112.18    4211  0.000089  112.1044   \n",
       "2017-09-26 11:40:00  112.20  112.15  112.19    3820 -0.000267  112.1090   \n",
       "2017-09-26 11:45:00  112.20  112.15  112.16    3481  0.000178  112.1138   \n",
       "2017-09-26 11:50:00  112.21  112.18  112.18    3891  0.000178  112.1186   \n",
       "2017-09-26 11:55:00  112.23  112.14  112.20    4370 -0.000178  112.1232   \n",
       "2017-09-26 12:00:00  112.19  112.16  112.18    3617  0.000089  112.1282   \n",
       "2017-09-26 12:05:00  112.20  112.17  112.19    3742  0.000000  112.1330   \n",
       "2017-09-26 12:10:00  112.21  112.19  112.19    3384  0.000000  112.1378   \n",
       "...                     ...     ...     ...     ...       ...       ...   \n",
       "2017-12-29 14:30:00  112.64  112.63  112.64    2594 -0.000089  112.6194   \n",
       "2017-12-29 14:35:00  112.66  112.63  112.63    2724  0.000266  112.6220   \n",
       "2017-12-29 14:40:00  112.66  112.65  112.66    2499 -0.000089  112.6244   \n",
       "2017-12-29 14:45:00  112.66  112.62  112.65    2728 -0.000266  112.6264   \n",
       "2017-12-29 14:50:00  112.62  112.61  112.62    2724  0.000000  112.6282   \n",
       "2017-12-29 14:55:00  112.65  112.62  112.62    3097  0.000266  112.6310   \n",
       "2017-12-29 15:00:00  112.65  112.63  112.65    3151 -0.000089  112.6336   \n",
       "2017-12-29 15:05:00  112.65  112.64  112.64    2826  0.000000  112.6338   \n",
       "2017-12-29 15:10:00  112.65  112.64  112.64    2538  0.000000  112.6336   \n",
       "2017-12-29 15:15:00  112.65  112.63  112.64    2779  0.000000  112.6340   \n",
       "2017-12-29 15:20:00  112.64  112.63  112.64    2804 -0.000089  112.6350   \n",
       "2017-12-29 15:25:00  112.64  112.63  112.63    2776  0.000089  112.6354   \n",
       "2017-12-29 15:30:00  112.66  112.64  112.64    3281  0.000178  112.6356   \n",
       "2017-12-29 15:35:00  112.67  112.65  112.66    3894 -0.000089  112.6360   \n",
       "2017-12-29 15:40:00  112.66  112.65  112.65    3615  0.000089  112.6362   \n",
       "2017-12-29 15:45:00  112.66  112.64  112.66    3430 -0.000178  112.6362   \n",
       "2017-12-29 15:50:00  112.65  112.63  112.64    3394 -0.000089  112.6354   \n",
       "2017-12-29 15:55:00  112.69  112.63  112.63    4163  0.000533  112.6354   \n",
       "2017-12-29 16:00:00  112.69  112.65  112.69    3530 -0.000177  112.6348   \n",
       "2017-12-29 16:05:00  112.67  112.65  112.67    2977 -0.000089  112.6342   \n",
       "2017-12-29 16:10:00  112.68  112.66  112.66    3193  0.000089  112.6342   \n",
       "2017-12-29 16:15:00  112.68  112.66  112.67    3143  0.000089  112.6348   \n",
       "2017-12-29 16:20:00  112.68  112.65  112.68    3276 -0.000089  112.6352   \n",
       "2017-12-29 16:25:00  112.68  112.67  112.67    3389  0.000089  112.6360   \n",
       "2017-12-29 16:30:00  112.69  112.67  112.68    3273 -0.000089  112.6366   \n",
       "2017-12-29 16:35:00  112.68  112.67  112.67    4063  0.000089  112.6382   \n",
       "2017-12-29 16:40:00  112.69  112.68  112.68    3437  0.000089  112.6396   \n",
       "2017-12-29 16:45:00  112.69  112.67  112.69    2354 -0.000177  112.6402   \n",
       "2017-12-29 16:50:00  112.72  112.67  112.67    2473  0.000089  112.6412   \n",
       "2017-12-29 16:55:00  112.70  112.68  112.68    2431  0.000089  112.6434   \n",
       "\n",
       "                        100ma      200ma  Adj Close  \n",
       "Date                                                 \n",
       "2017-09-26 09:45:00  111.7835  111.71785     112.19  \n",
       "2017-09-26 09:50:00  111.7891  111.72000     112.15  \n",
       "2017-09-26 09:55:00  111.7956  111.72235     112.19  \n",
       "2017-09-26 10:00:00  111.8018  111.72440     112.14  \n",
       "2017-09-26 10:05:00  111.8078  111.72650     112.15  \n",
       "2017-09-26 10:10:00  111.8141  111.72865     112.18  \n",
       "2017-09-26 10:15:00  111.8200  111.73070     112.15  \n",
       "2017-09-26 10:20:00  111.8251  111.73275     112.14  \n",
       "2017-09-26 10:25:00  111.8305  111.73485     112.14  \n",
       "2017-09-26 10:30:00  111.8362  111.73675     112.11  \n",
       "2017-09-26 10:35:00  111.8425  111.73890     112.15  \n",
       "2017-09-26 10:40:00  111.8490  111.74135     112.18  \n",
       "2017-09-26 10:45:00  111.8549  111.74350     112.12  \n",
       "2017-09-26 10:50:00  111.8624  111.74620     112.27  \n",
       "2017-09-26 10:55:00  111.8696  111.74885     112.28  \n",
       "2017-09-26 11:00:00  111.8760  111.75125     112.25  \n",
       "2017-09-26 11:05:00  111.8820  111.75355     112.23  \n",
       "2017-09-26 11:10:00  111.8877  111.75565     112.18  \n",
       "2017-09-26 11:15:00  111.8939  111.75785     112.19  \n",
       "2017-09-26 11:20:00  111.8993  111.75995     112.17  \n",
       "2017-09-26 11:25:00  111.9039  111.76190     112.13  \n",
       "2017-09-26 11:30:00  111.9089  111.76415     112.18  \n",
       "2017-09-26 11:35:00  111.9144  111.76650     112.19  \n",
       "2017-09-26 11:40:00  111.9202  111.76860     112.16  \n",
       "2017-09-26 11:45:00  111.9258  111.77080     112.18  \n",
       "2017-09-26 11:50:00  111.9316  111.77305     112.20  \n",
       "2017-09-26 11:55:00  111.9369  111.77530     112.18  \n",
       "2017-09-26 12:00:00  111.9421  111.77765     112.19  \n",
       "2017-09-26 12:05:00  111.9465  111.77990     112.19  \n",
       "2017-09-26 12:10:00  111.9512  111.78200     112.19  \n",
       "...                       ...        ...        ...  \n",
       "2017-12-29 14:30:00  112.5767  112.62345     112.63  \n",
       "2017-12-29 14:35:00  112.5781  112.62305     112.66  \n",
       "2017-12-29 14:40:00  112.5795  112.62250     112.65  \n",
       "2017-12-29 14:45:00  112.5806  112.62180     112.62  \n",
       "2017-12-29 14:50:00  112.5817  112.62110     112.62  \n",
       "2017-12-29 14:55:00  112.5830  112.62040     112.65  \n",
       "2017-12-29 15:00:00  112.5842  112.61975     112.64  \n",
       "2017-12-29 15:05:00  112.5852  112.61910     112.64  \n",
       "2017-12-29 15:10:00  112.5865  112.61850     112.64  \n",
       "2017-12-29 15:15:00  112.5877  112.61780     112.64  \n",
       "2017-12-29 15:20:00  112.5888  112.61715     112.63  \n",
       "2017-12-29 15:25:00  112.5902  112.61650     112.64  \n",
       "2017-12-29 15:30:00  112.5920  112.61590     112.66  \n",
       "2017-12-29 15:35:00  112.5936  112.61530     112.65  \n",
       "2017-12-29 15:40:00  112.5953  112.61475     112.66  \n",
       "2017-12-29 15:45:00  112.5968  112.61425     112.64  \n",
       "2017-12-29 15:50:00  112.5977  112.61360     112.63  \n",
       "2017-12-29 15:55:00  112.5993  112.61330     112.69  \n",
       "2017-12-29 16:00:00  112.6005  112.61290     112.67  \n",
       "2017-12-29 16:05:00  112.6015  112.61240     112.66  \n",
       "2017-12-29 16:10:00  112.6026  112.61195     112.67  \n",
       "2017-12-29 16:15:00  112.6038  112.61160     112.68  \n",
       "2017-12-29 16:20:00  112.6046  112.61120     112.67  \n",
       "2017-12-29 16:25:00  112.6058  112.61080     112.68  \n",
       "2017-12-29 16:30:00  112.6069  112.61035     112.67  \n",
       "2017-12-29 16:35:00  112.6082  112.61005     112.68  \n",
       "2017-12-29 16:40:00  112.6094  112.60975     112.69  \n",
       "2017-12-29 16:45:00  112.6107  112.60935     112.67  \n",
       "2017-12-29 16:50:00  112.6120  112.60895     112.68  \n",
       "2017-12-29 16:55:00  112.6135  112.60860     112.69  \n",
       "\n",
       "[19606 rows x 9 columns]"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_JPY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stock(df):\n",
    "    print(df.head())\n",
    "    plt.subplot(211)\n",
    "    plt.plot(df['Adj Close'], color='red', label='Adj Close')\n",
    "    plt.legend(loc='best')\n",
    "    plt.subplot(212)\n",
    "    plt.plot(df['Pct'], color='blue', label='Percentage change')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Open    High     Low  Volume       Pct      50ma  \\\n",
      "Date                                                                      \n",
      "2017-09-26 09:45:00  112.19  112.16  112.17    4393  0.000178  111.9248   \n",
      "2017-09-26 09:50:00  112.19  112.14  112.19    4507 -0.000357  111.9336   \n",
      "2017-09-26 09:55:00  112.19  112.14  112.15    3889  0.000357  111.9438   \n",
      "2017-09-26 10:00:00  112.19  112.10  112.19    5530 -0.000446  111.9528   \n",
      "2017-09-26 10:05:00  112.18  112.13  112.14    5066  0.000089  111.9622   \n",
      "\n",
      "                        100ma      200ma  Adj Close  \n",
      "Date                                                 \n",
      "2017-09-26 09:45:00  111.7835  111.71785     112.19  \n",
      "2017-09-26 09:50:00  111.7891  111.72000     112.15  \n",
      "2017-09-26 09:55:00  111.7956  111.72235     112.19  \n",
      "2017-09-26 10:00:00  111.8018  111.72440     112.14  \n",
      "2017-09-26 10:05:00  111.8078  111.72650     112.15  \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a44167cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_stock(df_JPY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Training/Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(stock,normalize,seq_len,split,ma):\n",
    "    amount_of_features = len(stock.columns)\n",
    "    print (\"Amount of features = {}\".format(amount_of_features))\n",
    "    sequence_length = seq_len+1\n",
    "    result_train = []\n",
    "    result_test= []\n",
    "    row = round(split * stock.shape[0]) \n",
    "    df_train=stock[0:row].copy()\n",
    "    print (\"Amount of training data = {}\".format(df_train.shape[0]))\n",
    "    df_test=stock[row:len(stock)].copy()\n",
    "    print (\"Amount of testing data = {}\".format(df_test.shape[0]))\n",
    "\n",
    "    \n",
    "    if normalize:\n",
    "        #Training\n",
    "        min_max_scaler = preprocessing.MinMaxScaler()\n",
    "        df_train['Open'] = min_max_scaler.fit_transform(df_train.Open.values.reshape(-1,1))\n",
    "        df_train['High'] = min_max_scaler.fit_transform(df_train.High.values.reshape(-1,1))\n",
    "        df_train['Low'] = min_max_scaler.fit_transform(df_train.Low.values.reshape(-1,1))\n",
    "        df_train['Volume'] = min_max_scaler.fit_transform(df_train.Volume.values.reshape(-1,1))\n",
    "        df_train['Adj Close'] = min_max_scaler.fit_transform(df_train['Adj Close'].values.reshape(-1,1))\n",
    "        df_train['Pct'] = min_max_scaler.fit_transform(df_train['Pct'].values.reshape(-1,1))\n",
    "        if ma != []:\n",
    "            for moving in ma:\n",
    "                df_train['{}ma'.format(moving)] = min_max_scaler.fit_transform(df_train['{}ma'.format(moving)].values.reshape(-1,1))  \n",
    "        #Test\n",
    "        df_test['Open'] = min_max_scaler.fit_transform(df_test.Open.values.reshape(-1,1))\n",
    "        df_test['High'] = min_max_scaler.fit_transform(df_test.High.values.reshape(-1,1))\n",
    "        df_test['Low'] = min_max_scaler.fit_transform(df_test.Low.values.reshape(-1,1))\n",
    "        df_test['Volume'] = min_max_scaler.fit_transform(df_test.Volume.values.reshape(-1,1))\n",
    "        df_test['Adj Close'] = min_max_scaler.fit_transform(df_test['Adj Close'].values.reshape(-1,1))\n",
    "        df_test['Pct'] = min_max_scaler.fit_transform(df_test['Pct'].values.reshape(-1,1))\n",
    "        if ma != []:\n",
    "            for moving in ma:\n",
    "                df_test['{}ma'.format(moving)] = min_max_scaler.fit_transform(df_test['{}ma'.format(moving)].values.reshape(-1,1))  \n",
    "     \n",
    "    #Training\n",
    "    data_train = df_train.as_matrix()\n",
    "    for index in range(len(data_train) - sequence_length): \n",
    "        result_train.append(data_train[index: index + sequence_length]) \n",
    "    train = np.array(result_train)\n",
    "    X_train = train[:, :-1].copy() # all data until day m\n",
    "    y_train = train[:, -1][:,-1].copy() # day m + 1 adjusted close price\n",
    "\n",
    "    #Test\n",
    "    data_test = df_test.as_matrix()\n",
    "    for index in range(len(data_test) - sequence_length): \n",
    "        result_test.append(data_test[index: index + sequence_length]) \n",
    "    test = np.array(result_train)\n",
    "    X_test = test[:, :-1].copy()\n",
    "    y_test = test[:, -1][:,-1].copy()\n",
    "\n",
    "    \n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], amount_of_features))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], amount_of_features))\n",
    "\n",
    "    return [X_train, y_train, X_test, y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of features = 9\n",
      "Amount of training data = 13724\n",
      "Amount of testing data = 5882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = load_data(df_JPY,True,seq_len,split=0.7,ma=[50, 100, 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22, 9)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[1,].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.75633528, 0.77165354, 0.76162791, 0.15806337, 0.6798997 ,\n",
       "       0.79357185, 0.83705747, 0.92625056, 0.76744186])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[6,][21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7674418604651194"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(shape, neurons, dropout, decay):\n",
    "    model = Sequential()\n",
    "\n",
    "    #model.add(Dense(neurons[0],activation=\"relu\", input_shape=(shape[0], shape[1])))\n",
    "    \n",
    "    model.add(LSTM(neurons[0], input_shape=(shape[0], shape[1]), return_sequences=True))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(LSTM(neurons[1], input_shape=(shape[0], shape[1]), return_sequences=False))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Dense(neurons[2],kernel_initializer=\"uniform\",activation='relu'))\n",
    "    model.add(Dense(neurons[3],kernel_initializer=\"uniform\",activation='linear'))\n",
    "    \n",
    "    adam = keras.optimizers.Adam(decay=decay)\n",
    "    model.compile(loss='mse',optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_CNN(shape, neurons, dropout, decay):\n",
    "    model = Sequential()\n",
    "    model.add(Convolution1D(input_shape = (shape[0], shape[1]), \n",
    "                        nb_filter=64,\n",
    "                        filter_length=2,\n",
    "                        border_mode='valid',\n",
    "                        activation='relu',\n",
    "                        subsample_length=1))\n",
    "    model.add(MaxPooling1D(pool_length=2))\n",
    "\n",
    "    model.add(Convolution1D(input_shape = (shape[0], shape[1]), \n",
    "                        nb_filter=64,\n",
    "                        filter_length=2,\n",
    "                        border_mode='valid',\n",
    "                        activation='relu',\n",
    "                        subsample_length=1))\n",
    "    model.add(MaxPooling1D(pool_length=2))\n",
    "\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(250))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('linear'))\n",
    "    adam = keras.optimizers.Adam(decay=decay)\n",
    "    model.compile(loss='mse',optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_63 (Conv1D)           (None, 21, 64)            1216      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 10, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 9, 64)             8256      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 4, 64)             0         \n",
      "_________________________________________________________________\n",
      "dropout_63 (Dropout)         (None, 4, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_32 (Flatten)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (None, 250)               64250     \n",
      "_________________________________________________________________\n",
      "dropout_64 (Dropout)         (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 1)                 251       \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 73,973\n",
      "Trainable params: 73,973\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(input_shape=(22, 9), activation=\"relu\", filters=64, kernel_size=2, strides=1, padding=\"valid\")`\n",
      "  \n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=2)`\n",
      "  if __name__ == '__main__':\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(input_shape=(22, 9), activation=\"relu\", filters=64, kernel_size=2, strides=1, padding=\"valid\")`\n",
      "  app.launch_new_instance()\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=2)`\n"
     ]
    }
   ],
   "source": [
    "model = build_model_CNN(shape, neurons, dropout, decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9590 samples, validate on 4111 samples\n",
      "Epoch 1/90\n",
      "9590/9590 [==============================] - 2s 247us/step - loss: 0.0440 - acc: 1.0428e-04 - val_loss: 0.0022 - val_acc: 2.4325e-04\n",
      "Epoch 2/90\n",
      "9590/9590 [==============================] - 0s 45us/step - loss: 0.0123 - acc: 1.0428e-04 - val_loss: 0.0021 - val_acc: 2.4325e-04\n",
      "Epoch 3/90\n",
      "9590/9590 [==============================] - 0s 46us/step - loss: 0.0080 - acc: 1.0428e-04 - val_loss: 0.0019 - val_acc: 2.4325e-04\n",
      "Epoch 4/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0058 - acc: 1.0428e-04 - val_loss: 0.0019 - val_acc: 2.4325e-04\n",
      "Epoch 5/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0051 - acc: 1.0428e-04 - val_loss: 0.0013 - val_acc: 2.4325e-04\n",
      "Epoch 6/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0046 - acc: 1.0428e-04 - val_loss: 0.0011 - val_acc: 2.4325e-04\n",
      "Epoch 7/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0041 - acc: 1.0428e-04 - val_loss: 0.0012 - val_acc: 2.4325e-04\n",
      "Epoch 8/90\n",
      "9590/9590 [==============================] - 0s 45us/step - loss: 0.0036 - acc: 1.0428e-04 - val_loss: 0.0016 - val_acc: 2.4325e-04\n",
      "Epoch 9/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0034 - acc: 1.0428e-04 - val_loss: 0.0022 - val_acc: 2.4325e-04\n",
      "Epoch 10/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0032 - acc: 1.0428e-04 - val_loss: 0.0029 - val_acc: 2.4325e-04\n",
      "Epoch 11/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0030 - acc: 1.0428e-04 - val_loss: 0.0028 - val_acc: 2.4325e-04\n",
      "Epoch 12/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0027 - acc: 1.0428e-04 - val_loss: 0.0026 - val_acc: 2.4325e-04\n",
      "Epoch 13/90\n",
      "9590/9590 [==============================] - 0s 45us/step - loss: 0.0025 - acc: 1.0428e-04 - val_loss: 0.0029 - val_acc: 2.4325e-04\n",
      "Epoch 14/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0025 - acc: 1.0428e-04 - val_loss: 0.0033 - val_acc: 2.4325e-04\n",
      "Epoch 15/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0023 - acc: 1.0428e-04 - val_loss: 0.0030 - val_acc: 2.4325e-04\n",
      "Epoch 16/90\n",
      "9590/9590 [==============================] - 0s 45us/step - loss: 0.0024 - acc: 1.0428e-04 - val_loss: 0.0031 - val_acc: 2.4325e-04\n",
      "Epoch 17/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0023 - acc: 1.0428e-04 - val_loss: 0.0032 - val_acc: 2.4325e-04\n",
      "Epoch 18/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0024 - acc: 1.0428e-04 - val_loss: 0.0030 - val_acc: 2.4325e-04\n",
      "Epoch 19/90\n",
      "9590/9590 [==============================] - 0s 46us/step - loss: 0.0021 - acc: 1.0428e-04 - val_loss: 0.0033 - val_acc: 2.4325e-04\n",
      "Epoch 20/90\n",
      "9590/9590 [==============================] - 0s 46us/step - loss: 0.0021 - acc: 1.0428e-04 - val_loss: 0.0029 - val_acc: 2.4325e-04\n",
      "Epoch 21/90\n",
      "9590/9590 [==============================] - 0s 45us/step - loss: 0.0020 - acc: 1.0428e-04 - val_loss: 0.0031 - val_acc: 2.4325e-04\n",
      "Epoch 22/90\n",
      "9590/9590 [==============================] - 0s 46us/step - loss: 0.0020 - acc: 1.0428e-04 - val_loss: 0.0036 - val_acc: 2.4325e-04\n",
      "Epoch 23/90\n",
      "9590/9590 [==============================] - 0s 52us/step - loss: 0.0019 - acc: 1.0428e-04 - val_loss: 0.0034 - val_acc: 2.4325e-04\n",
      "Epoch 24/90\n",
      "9590/9590 [==============================] - 1s 62us/step - loss: 0.0019 - acc: 1.0428e-04 - val_loss: 0.0040 - val_acc: 2.4325e-04\n",
      "Epoch 25/90\n",
      "9590/9590 [==============================] - 1s 54us/step - loss: 0.0020 - acc: 1.0428e-04 - val_loss: 0.0033 - val_acc: 2.4325e-04\n",
      "Epoch 26/90\n",
      "9590/9590 [==============================] - 0s 52us/step - loss: 0.0020 - acc: 1.0428e-04 - val_loss: 0.0028 - val_acc: 2.4325e-04\n",
      "Epoch 27/90\n",
      "9590/9590 [==============================] - 0s 49us/step - loss: 0.0019 - acc: 1.0428e-04 - val_loss: 0.0037 - val_acc: 2.4325e-04\n",
      "Epoch 28/90\n",
      "9590/9590 [==============================] - 0s 45us/step - loss: 0.0018 - acc: 1.0428e-04 - val_loss: 0.0033 - val_acc: 2.4325e-04\n",
      "Epoch 29/90\n",
      "9590/9590 [==============================] - 0s 50us/step - loss: 0.0018 - acc: 1.0428e-04 - val_loss: 0.0036 - val_acc: 2.4325e-04\n",
      "Epoch 30/90\n",
      "9590/9590 [==============================] - 0s 46us/step - loss: 0.0019 - acc: 1.0428e-04 - val_loss: 0.0038 - val_acc: 2.4325e-04\n",
      "Epoch 31/90\n",
      "9590/9590 [==============================] - 0s 48us/step - loss: 0.0019 - acc: 1.0428e-04 - val_loss: 0.0039 - val_acc: 2.4325e-04\n",
      "Epoch 32/90\n",
      "9590/9590 [==============================] - 0s 46us/step - loss: 0.0018 - acc: 1.0428e-04 - val_loss: 0.0036 - val_acc: 2.4325e-04\n",
      "Epoch 33/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0018 - acc: 1.0428e-04 - val_loss: 0.0045 - val_acc: 2.4325e-04\n",
      "Epoch 34/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0017 - acc: 1.0428e-04 - val_loss: 0.0047 - val_acc: 2.4325e-04\n",
      "Epoch 35/90\n",
      "9590/9590 [==============================] - 0s 46us/step - loss: 0.0018 - acc: 1.0428e-04 - val_loss: 0.0048 - val_acc: 2.4325e-04\n",
      "Epoch 36/90\n",
      "9590/9590 [==============================] - 0s 46us/step - loss: 0.0018 - acc: 1.0428e-04 - val_loss: 0.0047 - val_acc: 2.4325e-04\n",
      "Epoch 37/90\n",
      "9590/9590 [==============================] - 0s 48us/step - loss: 0.0018 - acc: 1.0428e-04 - val_loss: 0.0052 - val_acc: 2.4325e-04\n",
      "Epoch 38/90\n",
      "9590/9590 [==============================] - 0s 47us/step - loss: 0.0017 - acc: 1.0428e-04 - val_loss: 0.0040 - val_acc: 2.4325e-04\n",
      "Epoch 39/90\n",
      "9590/9590 [==============================] - 0s 47us/step - loss: 0.0018 - acc: 1.0428e-04 - val_loss: 0.0058 - val_acc: 2.4325e-04\n",
      "Epoch 40/90\n",
      "9590/9590 [==============================] - 0s 49us/step - loss: 0.0017 - acc: 1.0428e-04 - val_loss: 0.0051 - val_acc: 2.4325e-04\n",
      "Epoch 41/90\n",
      "9590/9590 [==============================] - 0s 50us/step - loss: 0.0018 - acc: 1.0428e-04 - val_loss: 0.0053 - val_acc: 2.4325e-04\n",
      "Epoch 42/90\n",
      "9590/9590 [==============================] - 0s 46us/step - loss: 0.0015 - acc: 1.0428e-04 - val_loss: 0.0063 - val_acc: 2.4325e-04\n",
      "Epoch 43/90\n",
      "9590/9590 [==============================] - 0s 45us/step - loss: 0.0016 - acc: 1.0428e-04 - val_loss: 0.0052 - val_acc: 2.4325e-04\n",
      "Epoch 44/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0015 - acc: 1.0428e-04 - val_loss: 0.0052 - val_acc: 2.4325e-04\n",
      "Epoch 45/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0016 - acc: 1.0428e-04 - val_loss: 0.0063 - val_acc: 2.4325e-04\n",
      "Epoch 46/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0016 - acc: 1.0428e-04 - val_loss: 0.0063 - val_acc: 2.4325e-04\n",
      "Epoch 47/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0016 - acc: 1.0428e-04 - val_loss: 0.0069 - val_acc: 2.4325e-04\n",
      "Epoch 48/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0015 - acc: 1.0428e-04 - val_loss: 0.0066 - val_acc: 2.4325e-04\n",
      "Epoch 49/90\n",
      "9590/9590 [==============================] - 0s 45us/step - loss: 0.0016 - acc: 1.0428e-04 - val_loss: 0.0064 - val_acc: 2.4325e-04\n",
      "Epoch 50/90\n",
      "9590/9590 [==============================] - 0s 45us/step - loss: 0.0015 - acc: 1.0428e-04 - val_loss: 0.0066 - val_acc: 2.4325e-04\n",
      "Epoch 51/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0015 - acc: 1.0428e-04 - val_loss: 0.0063 - val_acc: 2.4325e-04\n",
      "Epoch 52/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0015 - acc: 1.0428e-04 - val_loss: 0.0049 - val_acc: 2.4325e-04\n",
      "Epoch 53/90\n",
      "9590/9590 [==============================] - 0s 45us/step - loss: 0.0015 - acc: 1.0428e-04 - val_loss: 0.0060 - val_acc: 2.4325e-04\n",
      "Epoch 54/90\n",
      "9590/9590 [==============================] - 0s 50us/step - loss: 0.0015 - acc: 1.0428e-04 - val_loss: 0.0067 - val_acc: 2.4325e-04\n",
      "Epoch 55/90\n",
      "9590/9590 [==============================] - 0s 49us/step - loss: 0.0015 - acc: 1.0428e-04 - val_loss: 0.0066 - val_acc: 2.4325e-04\n",
      "Epoch 56/90\n",
      "9590/9590 [==============================] - 0s 49us/step - loss: 0.0014 - acc: 1.0428e-04 - val_loss: 0.0050 - val_acc: 2.4325e-04\n",
      "Epoch 57/90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9590/9590 [==============================] - 0s 45us/step - loss: 0.0015 - acc: 1.0428e-04 - val_loss: 0.0056 - val_acc: 2.4325e-04\n",
      "Epoch 58/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0015 - acc: 1.0428e-04 - val_loss: 0.0062 - val_acc: 2.4325e-04\n",
      "Epoch 59/90\n",
      "9590/9590 [==============================] - 0s 50us/step - loss: 0.0014 - acc: 1.0428e-04 - val_loss: 0.0057 - val_acc: 2.4325e-04\n",
      "Epoch 60/90\n",
      "9590/9590 [==============================] - 0s 48us/step - loss: 0.0014 - acc: 1.0428e-04 - val_loss: 0.0058 - val_acc: 2.4325e-04\n",
      "Epoch 61/90\n",
      "9590/9590 [==============================] - 0s 45us/step - loss: 0.0014 - acc: 1.0428e-04 - val_loss: 0.0058 - val_acc: 2.4325e-04\n",
      "Epoch 62/90\n",
      "9590/9590 [==============================] - 0s 49us/step - loss: 0.0014 - acc: 1.0428e-04 - val_loss: 0.0056 - val_acc: 2.4325e-04\n",
      "Epoch 63/90\n",
      "9590/9590 [==============================] - 0s 51us/step - loss: 0.0014 - acc: 1.0428e-04 - val_loss: 0.0056 - val_acc: 2.4325e-04\n",
      "Epoch 64/90\n",
      "9590/9590 [==============================] - 0s 49us/step - loss: 0.0014 - acc: 1.0428e-04 - val_loss: 0.0064 - val_acc: 2.4325e-04\n",
      "Epoch 65/90\n",
      "9590/9590 [==============================] - 0s 47us/step - loss: 0.0014 - acc: 1.0428e-04 - val_loss: 0.0058 - val_acc: 2.4325e-04\n",
      "Epoch 66/90\n",
      "9590/9590 [==============================] - 0s 45us/step - loss: 0.0014 - acc: 1.0428e-04 - val_loss: 0.0060 - val_acc: 2.4325e-04\n",
      "Epoch 67/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0014 - acc: 1.0428e-04 - val_loss: 0.0065 - val_acc: 2.4325e-04\n",
      "Epoch 68/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0014 - acc: 1.0428e-04 - val_loss: 0.0067 - val_acc: 2.4325e-04\n",
      "Epoch 69/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0015 - acc: 1.0428e-04 - val_loss: 0.0050 - val_acc: 2.4325e-04\n",
      "Epoch 70/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0014 - acc: 1.0428e-04 - val_loss: 0.0055 - val_acc: 2.4325e-04\n",
      "Epoch 71/90\n",
      "9590/9590 [==============================] - 0s 45us/step - loss: 0.0013 - acc: 1.0428e-04 - val_loss: 0.0061 - val_acc: 2.4325e-04\n",
      "Epoch 72/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0013 - acc: 1.0428e-04 - val_loss: 0.0053 - val_acc: 2.4325e-04\n",
      "Epoch 73/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0013 - acc: 1.0428e-04 - val_loss: 0.0057 - val_acc: 2.4325e-04\n",
      "Epoch 74/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0013 - acc: 1.0428e-04 - val_loss: 0.0059 - val_acc: 2.4325e-04\n",
      "Epoch 75/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0013 - acc: 1.0428e-04 - val_loss: 0.0067 - val_acc: 2.4325e-04\n",
      "Epoch 76/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0013 - acc: 1.0428e-04 - val_loss: 0.0064 - val_acc: 2.4325e-04\n",
      "Epoch 77/90\n",
      "9590/9590 [==============================] - 0s 45us/step - loss: 0.0013 - acc: 1.0428e-04 - val_loss: 0.0062 - val_acc: 2.4325e-04\n",
      "Epoch 78/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0013 - acc: 1.0428e-04 - val_loss: 0.0062 - val_acc: 2.4325e-04\n",
      "Epoch 79/90\n",
      "9590/9590 [==============================] - 0s 47us/step - loss: 0.0013 - acc: 1.0428e-04 - val_loss: 0.0065 - val_acc: 2.4325e-04\n",
      "Epoch 80/90\n",
      "9590/9590 [==============================] - 0s 48us/step - loss: 0.0014 - acc: 1.0428e-04 - val_loss: 0.0062 - val_acc: 2.4325e-04\n",
      "Epoch 81/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0013 - acc: 1.0428e-04 - val_loss: 0.0058 - val_acc: 2.4325e-04\n",
      "Epoch 82/90\n",
      "9590/9590 [==============================] - 0s 45us/step - loss: 0.0013 - acc: 1.0428e-04 - val_loss: 0.0075 - val_acc: 2.4325e-04\n",
      "Epoch 83/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0013 - acc: 1.0428e-04 - val_loss: 0.0061 - val_acc: 2.4325e-04\n",
      "Epoch 84/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0012 - acc: 1.0428e-04 - val_loss: 0.0056 - val_acc: 2.4325e-04\n",
      "Epoch 85/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0012 - acc: 1.0428e-04 - val_loss: 0.0058 - val_acc: 2.4325e-04\n",
      "Epoch 86/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0012 - acc: 1.0428e-04 - val_loss: 0.0065 - val_acc: 2.4325e-04\n",
      "Epoch 87/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0012 - acc: 1.0428e-04 - val_loss: 0.0063 - val_acc: 2.4325e-04\n",
      "Epoch 88/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0011 - acc: 1.0428e-04 - val_loss: 0.0064 - val_acc: 2.4325e-04\n",
      "Epoch 89/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0012 - acc: 1.0428e-04 - val_loss: 0.0066 - val_acc: 2.4325e-04\n",
      "Epoch 90/90\n",
      "9590/9590 [==============================] - 0s 50us/step - loss: 0.0012 - acc: 1.0428e-04 - val_loss: 0.0059 - val_acc: 2.4325e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a416a7400>"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train,batch_size=512,epochs=epochs,validation_split=0.3,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_score(model, X_train, y_train, X_test, y_test):\n",
    "    trainScore = model.evaluate(X_train, y_train, verbose=0)\n",
    "    print('Train Score: %.5f MSE (%.2f RMSE)' % (trainScore[0], math.sqrt(trainScore[0])))\n",
    "\n",
    "    testScore = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print('Test Score: %.5f MSE (%.2f RMSE)' % (testScore[0], math.sqrt(testScore[0])))\n",
    "    return trainScore[0], testScore[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.00202 MSE (0.04 RMSE)\n",
      "Test Score: 0.00202 MSE (0.04 RMSE)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0020164902503767386, 0.0020164902503767386)"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_score(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage_difference(model, X_test, y_test):\n",
    "    percentage_diff=[]\n",
    "\n",
    "    p = model.predict(X_test)\n",
    "    for u in range(len(y_test)): # for each data index in test data\n",
    "        pr = p[u][0] # pr = prediction on day u\n",
    "\n",
    "        percentage_diff.append((pr-y_test[u]/pr)*100)\n",
    "    print(mean(percentage_diff))\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-41.19116979815146\n"
     ]
    }
   ],
   "source": [
    "p = percentage_difference(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result_norm(stock_name, normalized_value_p, normalized_value_y_test):\n",
    "    newp=normalized_value_p\n",
    "    newy_test=normalized_value_y_test\n",
    "    plt2.plot(newp, color='red', label='Prediction')\n",
    "    plt2.plot(newy_test,color='blue', label='Actual')\n",
    "    plt2.legend(loc='best')\n",
    "    plt2.title('The test result for {}'.format(stock_name))\n",
    "    plt2.xlabel('5 Min ahead Forecast')\n",
    "    plt2.ylabel('Price')\n",
    "    plt2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a466df4a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_result_norm(\"JPY Curncy\", p, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.34464752, 0.33681462, 0.34203655, ..., 0.43342037, 0.4464752 ,\n",
       "       0.44386423])"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(stock_name, normalized_value,split=0.7,predict=True):\n",
    "    \"\"\"\n",
    "    Return a dataframe of that stock and normalize all the values. \n",
    "    (Optional: create moving average)\n",
    "    \"\"\"\n",
    "    df = xl.parse(stock_name)\n",
    "    df.set_index('Date', inplace=True)\n",
    "    \n",
    "    # Renaming all the columns so that we can use the old version code\n",
    "    df.rename(columns={'OPEN': 'Open', 'HIGH': 'High', 'LOW': 'Low', 'NUMBER_TICKS': 'Volume', 'LAST_PRICE': 'Adj Close'}, inplace=True)\n",
    "\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "    df = df['Adj Close'].values.reshape(-1,1)\n",
    "    normalized_value = normalized_value.reshape(-1,1)\n",
    "    \n",
    "    row = round(split * df.shape[0]) \n",
    "    if predict:\n",
    "        df_p=df[0:row].copy()\n",
    "    else:\n",
    "        df_p=df[row:len(df)].copy()\n",
    "    \n",
    "    #return df.shape, p.shape\n",
    "    mean_df=np.mean(df_p)\n",
    "    std_df=np.std(df_p)\n",
    "    new=normalized_value*mean_df+std_df\n",
    "      \n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "JPY=denormalize(\"JPY Curncy\", p,predict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def portfolio(currency_list,file = 'FX-5.xlsx',seq_len = 22,shape = [seq_len, 9, 1],neurons = [256, 256, 32, 1],dropout = 0.3,decay = 0.5,\n",
    "              epochs = 90,ma=[50, 100, 200],split=0.7):\n",
    "    i=0\n",
    "    mini=99999999\n",
    "    for currency in currency_list:\n",
    "        df=get_stock_data(currency,  ma)\n",
    "        X_train, y_train, X_test, y_test = load_data(df,True,seq_len,split,ma)\n",
    "        model = build_model_CNN(shape, neurons, dropout, decay)\n",
    "        model.fit(X_train,y_train,batch_size=512,epochs=epochs,validation_split=0.3,verbose=1)\n",
    "        p = percentage_difference(model, X_test, y_test)\n",
    "        newp = denormalize(currency, p,predict=True)\n",
    "        if mini>p.size:\n",
    "            mini=p.size\n",
    "        if i==0:\n",
    "            predict=newp.copy()\n",
    "        else:\n",
    "            predict=np.hstack((predict[0:mini],newp[0:mini]))\n",
    "        i+=1\n",
    "    return predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of features = 9\n",
      "Amount of training data = 13724\n",
      "Amount of testing data = 5882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(input_shape=(22, 9), activation=\"relu\", filters=64, kernel_size=2, strides=1, padding=\"valid\")`\n",
      "  \n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=2)`\n",
      "  if __name__ == '__main__':\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(input_shape=(22, 9), activation=\"relu\", filters=64, kernel_size=2, strides=1, padding=\"valid\")`\n",
      "  app.launch_new_instance()\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=2)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_43 (Conv1D)           (None, 21, 64)            1216      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 10, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 9, 64)             8256      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 4, 64)             0         \n",
      "_________________________________________________________________\n",
      "dropout_43 (Dropout)         (None, 4, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_22 (Flatten)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 250)               64250     \n",
      "_________________________________________________________________\n",
      "dropout_44 (Dropout)         (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 1)                 251       \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 73,973\n",
      "Trainable params: 73,973\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 9590 samples, validate on 4111 samples\n",
      "Epoch 1/90\n",
      "9590/9590 [==============================] - 3s 312us/step - loss: 0.0208 - acc: 1.0428e-04 - val_loss: 0.0015 - val_acc: 2.4325e-04\n",
      "Epoch 2/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0049 - acc: 1.0428e-04 - val_loss: 0.0029 - val_acc: 2.4325e-04\n",
      "Epoch 3/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0033 - acc: 1.0428e-04 - val_loss: 0.0011 - val_acc: 2.4325e-04\n",
      "Epoch 4/90\n",
      "9590/9590 [==============================] - 0s 45us/step - loss: 0.0026 - acc: 1.0428e-04 - val_loss: 0.0016 - val_acc: 2.4325e-04\n",
      "Epoch 5/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0024 - acc: 1.0428e-04 - val_loss: 0.0011 - val_acc: 2.4325e-04\n",
      "Epoch 6/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0021 - acc: 1.0428e-04 - val_loss: 0.0017 - val_acc: 2.4325e-04\n",
      "Epoch 7/90\n",
      "9590/9590 [==============================] - 0s 41us/step - loss: 0.0018 - acc: 1.0428e-04 - val_loss: 0.0014 - val_acc: 2.4325e-04\n",
      "Epoch 8/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0017 - acc: 1.0428e-04 - val_loss: 8.7621e-04 - val_acc: 2.4325e-04\n",
      "Epoch 9/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0016 - acc: 1.0428e-04 - val_loss: 8.9474e-04 - val_acc: 2.4325e-04\n",
      "Epoch 10/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0015 - acc: 1.0428e-04 - val_loss: 9.4185e-04 - val_acc: 2.4325e-04\n",
      "Epoch 11/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0014 - acc: 1.0428e-04 - val_loss: 9.3223e-04 - val_acc: 2.4325e-04\n",
      "Epoch 12/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0014 - acc: 1.0428e-04 - val_loss: 7.4191e-04 - val_acc: 2.4325e-04\n",
      "Epoch 13/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0013 - acc: 1.0428e-04 - val_loss: 6.1633e-04 - val_acc: 2.4325e-04\n",
      "Epoch 14/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0013 - acc: 1.0428e-04 - val_loss: 9.7638e-04 - val_acc: 2.4325e-04\n",
      "Epoch 15/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0012 - acc: 1.0428e-04 - val_loss: 6.6083e-04 - val_acc: 2.4325e-04\n",
      "Epoch 16/90\n",
      "9590/9590 [==============================] - 0s 41us/step - loss: 0.0012 - acc: 1.0428e-04 - val_loss: 6.2846e-04 - val_acc: 2.4325e-04\n",
      "Epoch 17/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0012 - acc: 1.0428e-04 - val_loss: 0.0010 - val_acc: 2.4325e-04\n",
      "Epoch 18/90\n",
      "9590/9590 [==============================] - 0s 41us/step - loss: 0.0012 - acc: 1.0428e-04 - val_loss: 5.5394e-04 - val_acc: 2.4325e-04\n",
      "Epoch 19/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0011 - acc: 1.0428e-04 - val_loss: 5.7733e-04 - val_acc: 2.4325e-04\n",
      "Epoch 20/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0011 - acc: 1.0428e-04 - val_loss: 7.9138e-04 - val_acc: 2.4325e-04\n",
      "Epoch 21/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0011 - acc: 1.0428e-04 - val_loss: 5.9297e-04 - val_acc: 2.4325e-04\n",
      "Epoch 22/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0010 - acc: 1.0428e-04 - val_loss: 5.5290e-04 - val_acc: 2.4325e-04\n",
      "Epoch 23/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0010 - acc: 1.0428e-04 - val_loss: 5.1125e-04 - val_acc: 2.4325e-04\n",
      "Epoch 24/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0011 - acc: 1.0428e-04 - val_loss: 4.3363e-04 - val_acc: 2.4325e-04\n",
      "Epoch 25/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0011 - acc: 1.0428e-04 - val_loss: 0.0011 - val_acc: 2.4325e-04\n",
      "Epoch 26/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0011 - acc: 1.0428e-04 - val_loss: 4.8425e-04 - val_acc: 2.4325e-04\n",
      "Epoch 27/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 9.6516e-04 - acc: 1.0428e-04 - val_loss: 6.7052e-04 - val_acc: 2.4325e-04\n",
      "Epoch 28/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 9.6378e-04 - acc: 1.0428e-04 - val_loss: 5.3335e-04 - val_acc: 2.4325e-04\n",
      "Epoch 29/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 9.6928e-04 - acc: 1.0428e-04 - val_loss: 4.6876e-04 - val_acc: 2.4325e-04\n",
      "Epoch 30/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 9.7307e-04 - acc: 1.0428e-04 - val_loss: 5.8714e-04 - val_acc: 2.4325e-04\n",
      "Epoch 31/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 9.2709e-04 - acc: 1.0428e-04 - val_loss: 5.0406e-04 - val_acc: 2.4325e-04\n",
      "Epoch 32/90\n",
      "9590/9590 [==============================] - 0s 47us/step - loss: 9.4187e-04 - acc: 1.0428e-04 - val_loss: 5.6243e-04 - val_acc: 2.4325e-04\n",
      "Epoch 33/90\n",
      "9590/9590 [==============================] - 0s 48us/step - loss: 9.0688e-04 - acc: 1.0428e-04 - val_loss: 5.4683e-04 - val_acc: 2.4325e-04\n",
      "Epoch 34/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 9.1950e-04 - acc: 1.0428e-04 - val_loss: 4.6732e-04 - val_acc: 2.4325e-04\n",
      "Epoch 35/90\n",
      "9590/9590 [==============================] - 0s 47us/step - loss: 9.1912e-04 - acc: 1.0428e-04 - val_loss: 5.4144e-04 - val_acc: 2.4325e-04\n",
      "Epoch 36/90\n",
      "9590/9590 [==============================] - 0s 52us/step - loss: 8.8595e-04 - acc: 1.0428e-04 - val_loss: 4.4374e-04 - val_acc: 2.4325e-04\n",
      "Epoch 37/90\n",
      "9590/9590 [==============================] - 0s 49us/step - loss: 8.6507e-04 - acc: 1.0428e-04 - val_loss: 4.5665e-04 - val_acc: 2.4325e-04\n",
      "Epoch 38/90\n",
      "9590/9590 [==============================] - 0s 52us/step - loss: 8.6827e-04 - acc: 1.0428e-04 - val_loss: 6.6756e-04 - val_acc: 2.4325e-04\n",
      "Epoch 39/90\n",
      "9590/9590 [==============================] - 0s 52us/step - loss: 8.7319e-04 - acc: 1.0428e-04 - val_loss: 5.1257e-04 - val_acc: 2.4325e-04\n",
      "Epoch 40/90\n",
      "9590/9590 [==============================] - 0s 51us/step - loss: 8.5129e-04 - acc: 1.0428e-04 - val_loss: 4.0621e-04 - val_acc: 2.4325e-04\n",
      "Epoch 41/90\n",
      "9590/9590 [==============================] - 1s 52us/step - loss: 8.4811e-04 - acc: 1.0428e-04 - val_loss: 4.9685e-04 - val_acc: 2.4325e-04\n",
      "Epoch 42/90\n",
      "9590/9590 [==============================] - 1s 53us/step - loss: 8.3119e-04 - acc: 1.0428e-04 - val_loss: 4.6362e-04 - val_acc: 2.4325e-04\n",
      "Epoch 43/90\n",
      "9590/9590 [==============================] - 1s 53us/step - loss: 8.0646e-04 - acc: 1.0428e-04 - val_loss: 5.0074e-04 - val_acc: 2.4325e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/90\n",
      "9590/9590 [==============================] - 0s 52us/step - loss: 8.3868e-04 - acc: 1.0428e-04 - val_loss: 4.7226e-04 - val_acc: 2.4325e-04\n",
      "Epoch 45/90\n",
      "9590/9590 [==============================] - 0s 49us/step - loss: 8.2570e-04 - acc: 1.0428e-04 - val_loss: 5.5805e-04 - val_acc: 2.4325e-04\n",
      "Epoch 46/90\n",
      "9590/9590 [==============================] - 0s 47us/step - loss: 8.4088e-04 - acc: 1.0428e-04 - val_loss: 4.7950e-04 - val_acc: 2.4325e-04\n",
      "Epoch 47/90\n",
      "9590/9590 [==============================] - 0s 52us/step - loss: 8.2561e-04 - acc: 1.0428e-04 - val_loss: 7.5379e-04 - val_acc: 2.4325e-04\n",
      "Epoch 48/90\n",
      "9590/9590 [==============================] - 0s 47us/step - loss: 8.1647e-04 - acc: 1.0428e-04 - val_loss: 7.7535e-04 - val_acc: 2.4325e-04\n",
      "Epoch 49/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 8.6922e-04 - acc: 1.0428e-04 - val_loss: 8.2138e-04 - val_acc: 2.4325e-04\n",
      "Epoch 50/90\n",
      "9590/9590 [==============================] - 0s 46us/step - loss: 8.4207e-04 - acc: 1.0428e-04 - val_loss: 4.4375e-04 - val_acc: 2.4325e-04\n",
      "Epoch 51/90\n",
      "9590/9590 [==============================] - 0s 46us/step - loss: 8.4078e-04 - acc: 1.0428e-04 - val_loss: 4.6283e-04 - val_acc: 2.4325e-04\n",
      "Epoch 52/90\n",
      "9590/9590 [==============================] - 0s 47us/step - loss: 8.5695e-04 - acc: 1.0428e-04 - val_loss: 7.7774e-04 - val_acc: 2.4325e-04\n",
      "Epoch 53/90\n",
      "9590/9590 [==============================] - 1s 54us/step - loss: 8.8653e-04 - acc: 1.0428e-04 - val_loss: 6.1554e-04 - val_acc: 2.4325e-04\n",
      "Epoch 54/90\n",
      "9590/9590 [==============================] - 0s 52us/step - loss: 8.5416e-04 - acc: 1.0428e-04 - val_loss: 5.0110e-04 - val_acc: 2.4325e-04\n",
      "Epoch 55/90\n",
      "9590/9590 [==============================] - 1s 58us/step - loss: 7.9748e-04 - acc: 1.0428e-04 - val_loss: 4.6672e-04 - val_acc: 2.4325e-04\n",
      "Epoch 56/90\n",
      "9590/9590 [==============================] - 0s 47us/step - loss: 8.0523e-04 - acc: 1.0428e-04 - val_loss: 4.7254e-04 - val_acc: 2.4325e-04\n",
      "Epoch 57/90\n",
      "9590/9590 [==============================] - 0s 49us/step - loss: 7.6175e-04 - acc: 1.0428e-04 - val_loss: 6.3018e-04 - val_acc: 2.4325e-04\n",
      "Epoch 58/90\n",
      "9590/9590 [==============================] - 0s 48us/step - loss: 7.6361e-04 - acc: 1.0428e-04 - val_loss: 4.2444e-04 - val_acc: 2.4325e-04\n",
      "Epoch 59/90\n",
      "9590/9590 [==============================] - 1s 55us/step - loss: 7.7326e-04 - acc: 1.0428e-04 - val_loss: 4.1337e-04 - val_acc: 2.4325e-04\n",
      "Epoch 60/90\n",
      "9590/9590 [==============================] - 1s 52us/step - loss: 7.4857e-04 - acc: 1.0428e-04 - val_loss: 3.9944e-04 - val_acc: 2.4325e-04\n",
      "Epoch 61/90\n",
      "9590/9590 [==============================] - 0s 51us/step - loss: 7.3682e-04 - acc: 1.0428e-04 - val_loss: 4.8683e-04 - val_acc: 2.4325e-04\n",
      "Epoch 62/90\n",
      "9590/9590 [==============================] - 0s 50us/step - loss: 7.5094e-04 - acc: 1.0428e-04 - val_loss: 4.0399e-04 - val_acc: 2.4325e-04\n",
      "Epoch 63/90\n",
      "9590/9590 [==============================] - 0s 49us/step - loss: 7.2362e-04 - acc: 1.0428e-04 - val_loss: 5.1235e-04 - val_acc: 2.4325e-04\n",
      "Epoch 64/90\n",
      "9590/9590 [==============================] - 1s 53us/step - loss: 7.6539e-04 - acc: 1.0428e-04 - val_loss: 4.3461e-04 - val_acc: 2.4325e-04\n",
      "Epoch 65/90\n",
      "9590/9590 [==============================] - 0s 51us/step - loss: 7.1963e-04 - acc: 1.0428e-04 - val_loss: 4.2642e-04 - val_acc: 2.4325e-04\n",
      "Epoch 66/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 7.3061e-04 - acc: 1.0428e-04 - val_loss: 4.1470e-04 - val_acc: 2.4325e-04\n",
      "Epoch 67/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 7.3325e-04 - acc: 1.0428e-04 - val_loss: 8.3674e-04 - val_acc: 2.4325e-04\n",
      "Epoch 68/90\n",
      "9590/9590 [==============================] - 0s 46us/step - loss: 7.6864e-04 - acc: 1.0428e-04 - val_loss: 4.2263e-04 - val_acc: 2.4325e-04\n",
      "Epoch 69/90\n",
      "9590/9590 [==============================] - 0s 48us/step - loss: 7.3346e-04 - acc: 1.0428e-04 - val_loss: 4.7687e-04 - val_acc: 2.4325e-04\n",
      "Epoch 70/90\n",
      "9590/9590 [==============================] - 0s 46us/step - loss: 7.2313e-04 - acc: 1.0428e-04 - val_loss: 5.1828e-04 - val_acc: 2.4325e-04\n",
      "Epoch 71/90\n",
      "9590/9590 [==============================] - 0s 47us/step - loss: 7.0010e-04 - acc: 1.0428e-04 - val_loss: 5.0132e-04 - val_acc: 2.4325e-04\n",
      "Epoch 72/90\n",
      "9590/9590 [==============================] - 0s 50us/step - loss: 7.1818e-04 - acc: 1.0428e-04 - val_loss: 6.2432e-04 - val_acc: 2.4325e-04\n",
      "Epoch 73/90\n",
      "9590/9590 [==============================] - 0s 46us/step - loss: 7.0980e-04 - acc: 1.0428e-04 - val_loss: 5.0095e-04 - val_acc: 2.4325e-04\n",
      "Epoch 74/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 6.9874e-04 - acc: 1.0428e-04 - val_loss: 3.9783e-04 - val_acc: 2.4325e-04\n",
      "Epoch 75/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 6.8168e-04 - acc: 1.0428e-04 - val_loss: 3.8505e-04 - val_acc: 2.4325e-04\n",
      "Epoch 76/90\n",
      "9590/9590 [==============================] - 0s 45us/step - loss: 7.2307e-04 - acc: 1.0428e-04 - val_loss: 3.7033e-04 - val_acc: 2.4325e-04\n",
      "Epoch 77/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 7.0558e-04 - acc: 1.0428e-04 - val_loss: 4.0406e-04 - val_acc: 2.4325e-04\n",
      "Epoch 78/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 7.0951e-04 - acc: 1.0428e-04 - val_loss: 5.0272e-04 - val_acc: 2.4325e-04\n",
      "Epoch 79/90\n",
      "9590/9590 [==============================] - 0s 48us/step - loss: 7.0326e-04 - acc: 1.0428e-04 - val_loss: 3.8410e-04 - val_acc: 2.4325e-04\n",
      "Epoch 80/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 6.7310e-04 - acc: 1.0428e-04 - val_loss: 4.3094e-04 - val_acc: 2.4325e-04\n",
      "Epoch 81/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 7.0493e-04 - acc: 1.0428e-04 - val_loss: 0.0010 - val_acc: 2.4325e-04\n",
      "Epoch 82/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 6.9236e-04 - acc: 1.0428e-04 - val_loss: 6.8622e-04 - val_acc: 2.4325e-04\n",
      "Epoch 83/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 6.8934e-04 - acc: 1.0428e-04 - val_loss: 5.2471e-04 - val_acc: 2.4325e-04\n",
      "Epoch 84/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 6.8602e-04 - acc: 1.0428e-04 - val_loss: 5.8942e-04 - val_acc: 2.4325e-04\n",
      "Epoch 85/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 6.5411e-04 - acc: 1.0428e-04 - val_loss: 3.8870e-04 - val_acc: 2.4325e-04\n",
      "Epoch 86/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 6.6786e-04 - acc: 1.0428e-04 - val_loss: 5.0362e-04 - val_acc: 2.4325e-04\n",
      "Epoch 87/90\n",
      "9590/9590 [==============================] - 0s 45us/step - loss: 6.8536e-04 - acc: 1.0428e-04 - val_loss: 8.1101e-04 - val_acc: 2.4325e-04\n",
      "Epoch 88/90\n",
      "9590/9590 [==============================] - 0s 45us/step - loss: 6.7366e-04 - acc: 1.0428e-04 - val_loss: 0.0011 - val_acc: 2.4325e-04\n",
      "Epoch 89/90\n",
      "9590/9590 [==============================] - 0s 46us/step - loss: 7.2900e-04 - acc: 1.0428e-04 - val_loss: 7.1949e-04 - val_acc: 2.4325e-04\n",
      "Epoch 90/90\n",
      "9590/9590 [==============================] - 0s 46us/step - loss: 7.4223e-04 - acc: 1.0428e-04 - val_loss: 8.7351e-04 - val_acc: 2.4325e-04\n",
      "-62.8955280020437\n",
      "Amount of features = 9\n",
      "Amount of training data = 13724\n",
      "Amount of testing data = 5882\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_45 (Conv1D)           (None, 21, 64)            1216      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 10, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 9, 64)             8256      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 4, 64)             0         \n",
      "_________________________________________________________________\n",
      "dropout_45 (Dropout)         (None, 4, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_23 (Flatten)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 250)               64250     \n",
      "_________________________________________________________________\n",
      "dropout_46 (Dropout)         (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 1)                 251       \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 73,973\n",
      "Trainable params: 73,973\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9590 samples, validate on 4111 samples\n",
      "Epoch 1/90\n",
      "9590/9590 [==============================] - 2s 210us/step - loss: 0.0624 - acc: 1.0428e-04 - val_loss: 0.0052 - val_acc: 2.4325e-04\n",
      "Epoch 2/90\n",
      "9590/9590 [==============================] - 0s 49us/step - loss: 0.0125 - acc: 1.0428e-04 - val_loss: 0.0020 - val_acc: 2.4325e-04\n",
      "Epoch 3/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0078 - acc: 1.0428e-04 - val_loss: 0.0021 - val_acc: 2.4325e-04\n",
      "Epoch 4/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0061 - acc: 1.0428e-04 - val_loss: 0.0022 - val_acc: 2.4325e-04\n",
      "Epoch 5/90\n",
      "9590/9590 [==============================] - 0s 46us/step - loss: 0.0049 - acc: 1.0428e-04 - val_loss: 0.0034 - val_acc: 2.4325e-04\n",
      "Epoch 6/90\n",
      "9590/9590 [==============================] - 0s 45us/step - loss: 0.0042 - acc: 1.0428e-04 - val_loss: 0.0038 - val_acc: 2.4325e-04\n",
      "Epoch 7/90\n",
      "9590/9590 [==============================] - 0s 45us/step - loss: 0.0036 - acc: 1.0428e-04 - val_loss: 0.0037 - val_acc: 2.4325e-04\n",
      "Epoch 8/90\n",
      "9590/9590 [==============================] - 0s 46us/step - loss: 0.0032 - acc: 1.0428e-04 - val_loss: 0.0031 - val_acc: 2.4325e-04\n",
      "Epoch 9/90\n",
      "9590/9590 [==============================] - 0s 46us/step - loss: 0.0030 - acc: 1.0428e-04 - val_loss: 0.0043 - val_acc: 2.4325e-04\n",
      "Epoch 10/90\n",
      "9590/9590 [==============================] - 0s 47us/step - loss: 0.0028 - acc: 1.0428e-04 - val_loss: 0.0047 - val_acc: 2.4325e-04\n",
      "Epoch 11/90\n",
      "9590/9590 [==============================] - 0s 46us/step - loss: 0.0027 - acc: 1.0428e-04 - val_loss: 0.0045 - val_acc: 2.4325e-04\n",
      "Epoch 12/90\n",
      "9590/9590 [==============================] - 0s 47us/step - loss: 0.0027 - acc: 1.0428e-04 - val_loss: 0.0048 - val_acc: 2.4325e-04\n",
      "Epoch 13/90\n",
      "9590/9590 [==============================] - 0s 46us/step - loss: 0.0026 - acc: 1.0428e-04 - val_loss: 0.0050 - val_acc: 2.4325e-04\n",
      "Epoch 14/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0025 - acc: 1.0428e-04 - val_loss: 0.0054 - val_acc: 2.4325e-04\n",
      "Epoch 15/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0024 - acc: 1.0428e-04 - val_loss: 0.0058 - val_acc: 2.4325e-04\n",
      "Epoch 16/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0024 - acc: 1.0428e-04 - val_loss: 0.0053 - val_acc: 2.4325e-04\n",
      "Epoch 17/90\n",
      "9590/9590 [==============================] - 0s 45us/step - loss: 0.0023 - acc: 1.0428e-04 - val_loss: 0.0048 - val_acc: 2.4325e-04\n",
      "Epoch 18/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0023 - acc: 1.0428e-04 - val_loss: 0.0055 - val_acc: 2.4325e-04\n",
      "Epoch 19/90\n",
      "9590/9590 [==============================] - 0s 45us/step - loss: 0.0022 - acc: 1.0428e-04 - val_loss: 0.0048 - val_acc: 2.4325e-04\n",
      "Epoch 20/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0022 - acc: 1.0428e-04 - val_loss: 0.0056 - val_acc: 2.4325e-04\n",
      "Epoch 21/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0022 - acc: 1.0428e-04 - val_loss: 0.0060 - val_acc: 2.4325e-04\n",
      "Epoch 22/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0021 - acc: 1.0428e-04 - val_loss: 0.0056 - val_acc: 2.4325e-04\n",
      "Epoch 23/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0021 - acc: 1.0428e-04 - val_loss: 0.0053 - val_acc: 2.4325e-04\n",
      "Epoch 24/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0021 - acc: 1.0428e-04 - val_loss: 0.0052 - val_acc: 2.4325e-04\n",
      "Epoch 25/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0020 - acc: 1.0428e-04 - val_loss: 0.0062 - val_acc: 2.4325e-04\n",
      "Epoch 26/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0021 - acc: 1.0428e-04 - val_loss: 0.0064 - val_acc: 2.4325e-04\n",
      "Epoch 27/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0020 - acc: 1.0428e-04 - val_loss: 0.0068 - val_acc: 2.4325e-04\n",
      "Epoch 28/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0020 - acc: 1.0428e-04 - val_loss: 0.0067 - val_acc: 2.4325e-04\n",
      "Epoch 29/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0020 - acc: 1.0428e-04 - val_loss: 0.0061 - val_acc: 2.4325e-04\n",
      "Epoch 30/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0020 - acc: 1.0428e-04 - val_loss: 0.0070 - val_acc: 2.4325e-04\n",
      "Epoch 31/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0019 - acc: 1.0428e-04 - val_loss: 0.0064 - val_acc: 2.4325e-04\n",
      "Epoch 32/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0020 - acc: 1.0428e-04 - val_loss: 0.0062 - val_acc: 2.4325e-04\n",
      "Epoch 33/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0019 - acc: 1.0428e-04 - val_loss: 0.0069 - val_acc: 2.4325e-04\n",
      "Epoch 34/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0019 - acc: 1.0428e-04 - val_loss: 0.0065 - val_acc: 2.4325e-04\n",
      "Epoch 35/90\n",
      "9590/9590 [==============================] - 0s 45us/step - loss: 0.0019 - acc: 1.0428e-04 - val_loss: 0.0068 - val_acc: 2.4325e-04\n",
      "Epoch 36/90\n",
      "9590/9590 [==============================] - 0s 47us/step - loss: 0.0019 - acc: 1.0428e-04 - val_loss: 0.0064 - val_acc: 2.4325e-04\n",
      "Epoch 37/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0018 - acc: 1.0428e-04 - val_loss: 0.0064 - val_acc: 2.4325e-04\n",
      "Epoch 38/90\n",
      "9590/9590 [==============================] - 0s 45us/step - loss: 0.0018 - acc: 1.0428e-04 - val_loss: 0.0064 - val_acc: 2.4325e-04\n",
      "Epoch 39/90\n",
      "9590/9590 [==============================] - 0s 47us/step - loss: 0.0018 - acc: 1.0428e-04 - val_loss: 0.0058 - val_acc: 2.4325e-04\n",
      "Epoch 40/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0018 - acc: 1.0428e-04 - val_loss: 0.0067 - val_acc: 2.4325e-04\n",
      "Epoch 41/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0018 - acc: 1.0428e-04 - val_loss: 0.0056 - val_acc: 2.4325e-04\n",
      "Epoch 42/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0018 - acc: 1.0428e-04 - val_loss: 0.0056 - val_acc: 2.4325e-04\n",
      "Epoch 43/90\n",
      "9590/9590 [==============================] - 0s 47us/step - loss: 0.0017 - acc: 1.0428e-04 - val_loss: 0.0055 - val_acc: 2.4325e-04\n",
      "Epoch 44/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0017 - acc: 1.0428e-04 - val_loss: 0.0064 - val_acc: 2.4325e-04\n",
      "Epoch 45/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0016 - acc: 1.0428e-04 - val_loss: 0.0064 - val_acc: 2.4325e-04\n",
      "Epoch 46/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0017 - acc: 1.0428e-04 - val_loss: 0.0061 - val_acc: 2.4325e-04\n",
      "Epoch 47/90\n",
      "9590/9590 [==============================] - 0s 41us/step - loss: 0.0017 - acc: 1.0428e-04 - val_loss: 0.0063 - val_acc: 2.4325e-04\n",
      "Epoch 48/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0017 - acc: 1.0428e-04 - val_loss: 0.0073 - val_acc: 2.4325e-04\n",
      "Epoch 49/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0016 - acc: 1.0428e-04 - val_loss: 0.0070 - val_acc: 2.4325e-04\n",
      "Epoch 50/90\n",
      "9590/9590 [==============================] - 0s 41us/step - loss: 0.0017 - acc: 1.0428e-04 - val_loss: 0.0072 - val_acc: 2.4325e-04\n",
      "Epoch 51/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0017 - acc: 1.0428e-04 - val_loss: 0.0075 - val_acc: 2.4325e-04\n",
      "Epoch 52/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0016 - acc: 1.0428e-04 - val_loss: 0.0080 - val_acc: 2.4325e-04\n",
      "Epoch 53/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0016 - acc: 1.0428e-04 - val_loss: 0.0072 - val_acc: 2.4325e-04\n",
      "Epoch 54/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0016 - acc: 1.0428e-04 - val_loss: 0.0072 - val_acc: 2.4325e-04\n",
      "Epoch 55/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0017 - acc: 1.0428e-04 - val_loss: 0.0076 - val_acc: 2.4325e-04\n",
      "Epoch 56/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0016 - acc: 1.0428e-04 - val_loss: 0.0080 - val_acc: 2.4325e-04\n",
      "Epoch 57/90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9590/9590 [==============================] - 0s 41us/step - loss: 0.0016 - acc: 1.0428e-04 - val_loss: 0.0075 - val_acc: 2.4325e-04\n",
      "Epoch 58/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0015 - acc: 1.0428e-04 - val_loss: 0.0075 - val_acc: 2.4325e-04\n",
      "Epoch 59/90\n",
      "9590/9590 [==============================] - 0s 41us/step - loss: 0.0015 - acc: 1.0428e-04 - val_loss: 0.0078 - val_acc: 2.4325e-04\n",
      "Epoch 60/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0015 - acc: 1.0428e-04 - val_loss: 0.0070 - val_acc: 2.4325e-04\n",
      "Epoch 61/90\n",
      "9590/9590 [==============================] - 0s 41us/step - loss: 0.0016 - acc: 1.0428e-04 - val_loss: 0.0082 - val_acc: 2.4325e-04\n",
      "Epoch 62/90\n",
      "9590/9590 [==============================] - 0s 41us/step - loss: 0.0015 - acc: 1.0428e-04 - val_loss: 0.0076 - val_acc: 2.4325e-04\n",
      "Epoch 63/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0015 - acc: 1.0428e-04 - val_loss: 0.0086 - val_acc: 2.4325e-04\n",
      "Epoch 64/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0015 - acc: 1.0428e-04 - val_loss: 0.0068 - val_acc: 2.4325e-04\n",
      "Epoch 65/90\n",
      "9590/9590 [==============================] - 0s 41us/step - loss: 0.0015 - acc: 1.0428e-04 - val_loss: 0.0073 - val_acc: 2.4325e-04\n",
      "Epoch 66/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0015 - acc: 1.0428e-04 - val_loss: 0.0074 - val_acc: 2.4325e-04\n",
      "Epoch 67/90\n",
      "9590/9590 [==============================] - 0s 41us/step - loss: 0.0015 - acc: 1.0428e-04 - val_loss: 0.0076 - val_acc: 2.4325e-04\n",
      "Epoch 68/90\n",
      "9590/9590 [==============================] - 0s 40us/step - loss: 0.0014 - acc: 1.0428e-04 - val_loss: 0.0077 - val_acc: 2.4325e-04\n",
      "Epoch 69/90\n",
      "9590/9590 [==============================] - 0s 41us/step - loss: 0.0015 - acc: 1.0428e-04 - val_loss: 0.0088 - val_acc: 2.4325e-04\n",
      "Epoch 70/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0014 - acc: 1.0428e-04 - val_loss: 0.0074 - val_acc: 2.4325e-04\n",
      "Epoch 71/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0014 - acc: 1.0428e-04 - val_loss: 0.0081 - val_acc: 2.4325e-04\n",
      "Epoch 72/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0014 - acc: 1.0428e-04 - val_loss: 0.0083 - val_acc: 2.4325e-04\n",
      "Epoch 73/90\n",
      "9590/9590 [==============================] - 0s 41us/step - loss: 0.0014 - acc: 1.0428e-04 - val_loss: 0.0093 - val_acc: 2.4325e-04\n",
      "Epoch 74/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0014 - acc: 1.0428e-04 - val_loss: 0.0088 - val_acc: 2.4325e-04\n",
      "Epoch 75/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0015 - acc: 1.0428e-04 - val_loss: 0.0096 - val_acc: 2.4325e-04\n",
      "Epoch 76/90\n",
      "9590/9590 [==============================] - 0s 41us/step - loss: 0.0014 - acc: 1.0428e-04 - val_loss: 0.0096 - val_acc: 2.4325e-04\n",
      "Epoch 77/90\n",
      "9590/9590 [==============================] - 0s 41us/step - loss: 0.0013 - acc: 1.0428e-04 - val_loss: 0.0099 - val_acc: 2.4325e-04\n",
      "Epoch 78/90\n",
      "9590/9590 [==============================] - 0s 41us/step - loss: 0.0014 - acc: 1.0428e-04 - val_loss: 0.0099 - val_acc: 2.4325e-04\n",
      "Epoch 79/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0014 - acc: 1.0428e-04 - val_loss: 0.0090 - val_acc: 2.4325e-04\n",
      "Epoch 80/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0013 - acc: 1.0428e-04 - val_loss: 0.0100 - val_acc: 2.4325e-04\n",
      "Epoch 81/90\n",
      "9590/9590 [==============================] - 0s 41us/step - loss: 0.0014 - acc: 1.0428e-04 - val_loss: 0.0098 - val_acc: 2.4325e-04\n",
      "Epoch 82/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0014 - acc: 1.0428e-04 - val_loss: 0.0111 - val_acc: 2.4325e-04\n",
      "Epoch 83/90\n",
      "9590/9590 [==============================] - 0s 41us/step - loss: 0.0013 - acc: 1.0428e-04 - val_loss: 0.0117 - val_acc: 2.4325e-04\n",
      "Epoch 84/90\n",
      "9590/9590 [==============================] - 0s 41us/step - loss: 0.0014 - acc: 1.0428e-04 - val_loss: 0.0121 - val_acc: 2.4325e-04\n",
      "Epoch 85/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0014 - acc: 1.0428e-04 - val_loss: 0.0121 - val_acc: 2.4325e-04\n",
      "Epoch 86/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0013 - acc: 1.0428e-04 - val_loss: 0.0105 - val_acc: 2.4325e-04\n",
      "Epoch 87/90\n",
      "9590/9590 [==============================] - 0s 41us/step - loss: 0.0013 - acc: 1.0428e-04 - val_loss: 0.0101 - val_acc: 2.4325e-04\n",
      "Epoch 88/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0013 - acc: 1.0428e-04 - val_loss: 0.0101 - val_acc: 2.4325e-04\n",
      "Epoch 89/90\n",
      "9590/9590 [==============================] - 0s 41us/step - loss: 0.0013 - acc: 1.0428e-04 - val_loss: 0.0113 - val_acc: 2.4325e-04\n",
      "Epoch 90/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0013 - acc: 1.0428e-04 - val_loss: 0.0116 - val_acc: 2.4325e-04\n",
      "-38.45882440662738\n",
      "Amount of features = 9\n",
      "Amount of training data = 13724\n",
      "Amount of testing data = 5882\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_47 (Conv1D)           (None, 21, 64)            1216      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 10, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 9, 64)             8256      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 4, 64)             0         \n",
      "_________________________________________________________________\n",
      "dropout_47 (Dropout)         (None, 4, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_24 (Flatten)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 250)               64250     \n",
      "_________________________________________________________________\n",
      "dropout_48 (Dropout)         (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 1)                 251       \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 73,973\n",
      "Trainable params: 73,973\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 9590 samples, validate on 4111 samples\n",
      "Epoch 1/90\n",
      "9590/9590 [==============================] - 2s 198us/step - loss: 0.0375 - acc: 1.0428e-04 - val_loss: 0.0055 - val_acc: 2.4325e-04\n",
      "Epoch 2/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0080 - acc: 1.0428e-04 - val_loss: 8.6920e-04 - val_acc: 2.4325e-04\n",
      "Epoch 3/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0044 - acc: 1.0428e-04 - val_loss: 0.0014 - val_acc: 2.4325e-04\n",
      "Epoch 4/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0033 - acc: 1.0428e-04 - val_loss: 7.3724e-04 - val_acc: 2.4325e-04\n",
      "Epoch 5/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0027 - acc: 1.0428e-04 - val_loss: 0.0011 - val_acc: 2.4325e-04\n",
      "Epoch 6/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0025 - acc: 1.0428e-04 - val_loss: 9.9232e-04 - val_acc: 2.4325e-04\n",
      "Epoch 7/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0023 - acc: 1.0428e-04 - val_loss: 9.6515e-04 - val_acc: 2.4325e-04\n",
      "Epoch 8/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0021 - acc: 1.0428e-04 - val_loss: 0.0012 - val_acc: 2.4325e-04\n",
      "Epoch 9/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0020 - acc: 1.0428e-04 - val_loss: 0.0012 - val_acc: 2.4325e-04\n",
      "Epoch 10/90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0019 - acc: 1.0428e-04 - val_loss: 0.0011 - val_acc: 2.4325e-04\n",
      "Epoch 11/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0018 - acc: 1.0428e-04 - val_loss: 0.0026 - val_acc: 2.4325e-04\n",
      "Epoch 12/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0018 - acc: 1.0428e-04 - val_loss: 0.0011 - val_acc: 2.4325e-04\n",
      "Epoch 13/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0016 - acc: 1.0428e-04 - val_loss: 0.0012 - val_acc: 2.4325e-04\n",
      "Epoch 14/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0015 - acc: 1.0428e-04 - val_loss: 0.0015 - val_acc: 2.4325e-04\n",
      "Epoch 15/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0014 - acc: 1.0428e-04 - val_loss: 0.0015 - val_acc: 2.4325e-04\n",
      "Epoch 16/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0015 - acc: 1.0428e-04 - val_loss: 0.0025 - val_acc: 2.4325e-04\n",
      "Epoch 17/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0014 - acc: 1.0428e-04 - val_loss: 0.0013 - val_acc: 2.4325e-04\n",
      "Epoch 18/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0014 - acc: 1.0428e-04 - val_loss: 0.0024 - val_acc: 2.4325e-04\n",
      "Epoch 19/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0014 - acc: 1.0428e-04 - val_loss: 0.0014 - val_acc: 2.4325e-04\n",
      "Epoch 20/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0014 - acc: 1.0428e-04 - val_loss: 0.0012 - val_acc: 2.4325e-04\n",
      "Epoch 21/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0013 - acc: 1.0428e-04 - val_loss: 0.0017 - val_acc: 2.4325e-04\n",
      "Epoch 22/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0013 - acc: 1.0428e-04 - val_loss: 0.0016 - val_acc: 2.4325e-04\n",
      "Epoch 23/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0013 - acc: 1.0428e-04 - val_loss: 0.0022 - val_acc: 2.4325e-04\n",
      "Epoch 24/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0013 - acc: 1.0428e-04 - val_loss: 0.0011 - val_acc: 2.4325e-04\n",
      "Epoch 25/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0012 - acc: 1.0428e-04 - val_loss: 0.0014 - val_acc: 2.4325e-04\n",
      "Epoch 26/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0012 - acc: 1.0428e-04 - val_loss: 0.0011 - val_acc: 2.4325e-04\n",
      "Epoch 27/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0012 - acc: 1.0428e-04 - val_loss: 0.0023 - val_acc: 2.4325e-04\n",
      "Epoch 28/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0012 - acc: 1.0428e-04 - val_loss: 0.0015 - val_acc: 2.4325e-04\n",
      "Epoch 29/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0012 - acc: 1.0428e-04 - val_loss: 0.0013 - val_acc: 2.4325e-04\n",
      "Epoch 30/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0012 - acc: 1.0428e-04 - val_loss: 0.0019 - val_acc: 2.4325e-04\n",
      "Epoch 31/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0012 - acc: 1.0428e-04 - val_loss: 0.0014 - val_acc: 2.4325e-04\n",
      "Epoch 32/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0012 - acc: 1.0428e-04 - val_loss: 0.0016 - val_acc: 2.4325e-04\n",
      "Epoch 33/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0011 - acc: 1.0428e-04 - val_loss: 0.0012 - val_acc: 2.4325e-04\n",
      "Epoch 34/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0011 - acc: 1.0428e-04 - val_loss: 0.0017 - val_acc: 2.4325e-04\n",
      "Epoch 35/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0011 - acc: 1.0428e-04 - val_loss: 0.0013 - val_acc: 2.4325e-04\n",
      "Epoch 36/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0011 - acc: 1.0428e-04 - val_loss: 7.4018e-04 - val_acc: 2.4325e-04\n",
      "Epoch 37/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0011 - acc: 1.0428e-04 - val_loss: 6.0027e-04 - val_acc: 2.4325e-04\n",
      "Epoch 38/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0011 - acc: 1.0428e-04 - val_loss: 6.4872e-04 - val_acc: 2.4325e-04\n",
      "Epoch 39/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0011 - acc: 1.0428e-04 - val_loss: 0.0011 - val_acc: 2.4325e-04\n",
      "Epoch 40/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0011 - acc: 1.0428e-04 - val_loss: 5.4497e-04 - val_acc: 2.4325e-04\n",
      "Epoch 41/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0010 - acc: 1.0428e-04 - val_loss: 6.8852e-04 - val_acc: 2.4325e-04\n",
      "Epoch 42/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0011 - acc: 1.0428e-04 - val_loss: 0.0011 - val_acc: 2.4325e-04\n",
      "Epoch 43/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0011 - acc: 1.0428e-04 - val_loss: 0.0016 - val_acc: 2.4325e-04\n",
      "Epoch 44/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0011 - acc: 1.0428e-04 - val_loss: 8.7156e-04 - val_acc: 2.4325e-04\n",
      "Epoch 45/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 9.8715e-04 - acc: 1.0428e-04 - val_loss: 7.4855e-04 - val_acc: 2.4325e-04\n",
      "Epoch 46/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0010 - acc: 1.0428e-04 - val_loss: 9.2358e-04 - val_acc: 2.4325e-04\n",
      "Epoch 47/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0010 - acc: 1.0428e-04 - val_loss: 4.8233e-04 - val_acc: 2.4325e-04\n",
      "Epoch 48/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 9.7807e-04 - acc: 1.0428e-04 - val_loss: 6.1636e-04 - val_acc: 2.4325e-04\n",
      "Epoch 49/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 9.5615e-04 - acc: 1.0428e-04 - val_loss: 7.5251e-04 - val_acc: 2.4325e-04\n",
      "Epoch 50/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 9.7366e-04 - acc: 1.0428e-04 - val_loss: 5.1792e-04 - val_acc: 2.4325e-04\n",
      "Epoch 51/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 9.8931e-04 - acc: 1.0428e-04 - val_loss: 7.1157e-04 - val_acc: 2.4325e-04\n",
      "Epoch 52/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0010 - acc: 1.0428e-04 - val_loss: 7.4383e-04 - val_acc: 2.4325e-04\n",
      "Epoch 53/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 9.9130e-04 - acc: 1.0428e-04 - val_loss: 5.0754e-04 - val_acc: 2.4325e-04\n",
      "Epoch 54/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 9.5946e-04 - acc: 1.0428e-04 - val_loss: 6.3776e-04 - val_acc: 2.4325e-04\n",
      "Epoch 55/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 9.8298e-04 - acc: 1.0428e-04 - val_loss: 4.7587e-04 - val_acc: 2.4325e-04\n",
      "Epoch 56/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 9.2568e-04 - acc: 1.0428e-04 - val_loss: 7.9424e-04 - val_acc: 2.4325e-04\n",
      "Epoch 57/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 9.2900e-04 - acc: 1.0428e-04 - val_loss: 0.0010 - val_acc: 2.4325e-04\n",
      "Epoch 58/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 9.2959e-04 - acc: 1.0428e-04 - val_loss: 6.6614e-04 - val_acc: 2.4325e-04\n",
      "Epoch 59/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 9.0663e-04 - acc: 1.0428e-04 - val_loss: 5.8702e-04 - val_acc: 2.4325e-04\n",
      "Epoch 60/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 8.8676e-04 - acc: 1.0428e-04 - val_loss: 4.3188e-04 - val_acc: 2.4325e-04\n",
      "Epoch 61/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 8.8432e-04 - acc: 1.0428e-04 - val_loss: 5.4372e-04 - val_acc: 2.4325e-04\n",
      "Epoch 62/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 8.6431e-04 - acc: 1.0428e-04 - val_loss: 8.6569e-04 - val_acc: 2.4325e-04\n",
      "Epoch 63/90\n",
      "9590/9590 [==============================] - 0s 41us/step - loss: 9.0701e-04 - acc: 1.0428e-04 - val_loss: 4.4885e-04 - val_acc: 2.4325e-04\n",
      "Epoch 64/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 9.4261e-04 - acc: 1.0428e-04 - val_loss: 0.0010 - val_acc: 2.4325e-04\n",
      "Epoch 65/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 9.0574e-04 - acc: 1.0428e-04 - val_loss: 7.3178e-04 - val_acc: 2.4325e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 8.7594e-04 - acc: 1.0428e-04 - val_loss: 5.1555e-04 - val_acc: 2.4325e-04\n",
      "Epoch 67/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 8.3370e-04 - acc: 1.0428e-04 - val_loss: 5.1059e-04 - val_acc: 2.4325e-04\n",
      "Epoch 68/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 8.6401e-04 - acc: 1.0428e-04 - val_loss: 4.9165e-04 - val_acc: 2.4325e-04\n",
      "Epoch 69/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 8.2602e-04 - acc: 1.0428e-04 - val_loss: 5.3527e-04 - val_acc: 2.4325e-04\n",
      "Epoch 70/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 8.3358e-04 - acc: 1.0428e-04 - val_loss: 6.3843e-04 - val_acc: 2.4325e-04\n",
      "Epoch 71/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 8.2450e-04 - acc: 1.0428e-04 - val_loss: 6.6560e-04 - val_acc: 2.4325e-04\n",
      "Epoch 72/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 8.2400e-04 - acc: 1.0428e-04 - val_loss: 6.7921e-04 - val_acc: 2.4325e-04\n",
      "Epoch 73/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 8.4989e-04 - acc: 1.0428e-04 - val_loss: 5.7973e-04 - val_acc: 2.4325e-04\n",
      "Epoch 74/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 8.2046e-04 - acc: 1.0428e-04 - val_loss: 5.3261e-04 - val_acc: 2.4325e-04\n",
      "Epoch 75/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 8.4143e-04 - acc: 1.0428e-04 - val_loss: 4.5951e-04 - val_acc: 2.4325e-04\n",
      "Epoch 76/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 8.2620e-04 - acc: 1.0428e-04 - val_loss: 5.9096e-04 - val_acc: 2.4325e-04\n",
      "Epoch 77/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 7.9344e-04 - acc: 1.0428e-04 - val_loss: 8.1140e-04 - val_acc: 2.4325e-04\n",
      "Epoch 78/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 8.1442e-04 - acc: 1.0428e-04 - val_loss: 3.9778e-04 - val_acc: 2.4325e-04\n",
      "Epoch 79/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 8.0249e-04 - acc: 1.0428e-04 - val_loss: 5.1499e-04 - val_acc: 2.4325e-04\n",
      "Epoch 80/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 8.0088e-04 - acc: 1.0428e-04 - val_loss: 4.6758e-04 - val_acc: 2.4325e-04\n",
      "Epoch 81/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 7.6358e-04 - acc: 1.0428e-04 - val_loss: 6.0887e-04 - val_acc: 2.4325e-04\n",
      "Epoch 82/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 7.9933e-04 - acc: 1.0428e-04 - val_loss: 4.4363e-04 - val_acc: 2.4325e-04\n",
      "Epoch 83/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 8.1828e-04 - acc: 1.0428e-04 - val_loss: 6.0476e-04 - val_acc: 2.4325e-04\n",
      "Epoch 84/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 7.5118e-04 - acc: 1.0428e-04 - val_loss: 4.4656e-04 - val_acc: 2.4325e-04\n",
      "Epoch 85/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 7.5939e-04 - acc: 1.0428e-04 - val_loss: 7.9885e-04 - val_acc: 2.4325e-04\n",
      "Epoch 86/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 8.2394e-04 - acc: 1.0428e-04 - val_loss: 5.1089e-04 - val_acc: 2.4325e-04\n",
      "Epoch 87/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 7.7962e-04 - acc: 1.0428e-04 - val_loss: 4.6649e-04 - val_acc: 2.4325e-04\n",
      "Epoch 88/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 7.9218e-04 - acc: 1.0428e-04 - val_loss: 5.5518e-04 - val_acc: 2.4325e-04\n",
      "Epoch 89/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 7.6210e-04 - acc: 1.0428e-04 - val_loss: 3.6779e-04 - val_acc: 2.4325e-04\n",
      "Epoch 90/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 7.7564e-04 - acc: 1.0428e-04 - val_loss: 8.6342e-04 - val_acc: 2.4325e-04\n",
      "-53.03384464966998\n",
      "Amount of features = 9\n",
      "Amount of training data = 13724\n",
      "Amount of testing data = 5882\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_49 (Conv1D)           (None, 21, 64)            1216      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 10, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 9, 64)             8256      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 4, 64)             0         \n",
      "_________________________________________________________________\n",
      "dropout_49 (Dropout)         (None, 4, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_25 (Flatten)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 250)               64250     \n",
      "_________________________________________________________________\n",
      "dropout_50 (Dropout)         (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 1)                 251       \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 73,973\n",
      "Trainable params: 73,973\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 9590 samples, validate on 4111 samples\n",
      "Epoch 1/90\n",
      "9590/9590 [==============================] - 2s 200us/step - loss: 0.0664 - acc: 2.0855e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 2/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0106 - acc: 2.0855e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 3/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0066 - acc: 2.0855e-04 - val_loss: 7.5823e-04 - val_acc: 0.0000e+00\n",
      "Epoch 4/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0059 - acc: 2.0855e-04 - val_loss: 9.6321e-04 - val_acc: 0.0000e+00\n",
      "Epoch 5/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0049 - acc: 2.0855e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 6/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0042 - acc: 2.0855e-04 - val_loss: 8.0543e-04 - val_acc: 0.0000e+00\n",
      "Epoch 7/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0040 - acc: 2.0855e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 8/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0035 - acc: 2.0855e-04 - val_loss: 9.3650e-04 - val_acc: 0.0000e+00\n",
      "Epoch 9/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0031 - acc: 2.0855e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 10/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0029 - acc: 2.0855e-04 - val_loss: 7.3091e-04 - val_acc: 0.0000e+00\n",
      "Epoch 11/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0028 - acc: 2.0855e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 12/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0025 - acc: 2.0855e-04 - val_loss: 5.4203e-04 - val_acc: 0.0000e+00\n",
      "Epoch 13/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0025 - acc: 2.0855e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 14/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0023 - acc: 2.0855e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 15/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0023 - acc: 2.0855e-04 - val_loss: 3.5537e-04 - val_acc: 0.0000e+00\n",
      "Epoch 16/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0022 - acc: 2.0855e-04 - val_loss: 8.7793e-04 - val_acc: 0.0000e+00\n",
      "Epoch 17/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0022 - acc: 2.0855e-04 - val_loss: 4.4138e-04 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0021 - acc: 2.0855e-04 - val_loss: 3.1951e-04 - val_acc: 0.0000e+00\n",
      "Epoch 19/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0021 - acc: 2.0855e-04 - val_loss: 4.6544e-04 - val_acc: 0.0000e+00\n",
      "Epoch 20/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0019 - acc: 2.0855e-04 - val_loss: 5.7316e-04 - val_acc: 0.0000e+00\n",
      "Epoch 21/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0019 - acc: 2.0855e-04 - val_loss: 6.3580e-04 - val_acc: 0.0000e+00\n",
      "Epoch 22/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0019 - acc: 2.0855e-04 - val_loss: 6.1257e-04 - val_acc: 0.0000e+00\n",
      "Epoch 23/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0019 - acc: 2.0855e-04 - val_loss: 3.8178e-04 - val_acc: 0.0000e+00\n",
      "Epoch 24/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0018 - acc: 2.0855e-04 - val_loss: 4.2456e-04 - val_acc: 0.0000e+00\n",
      "Epoch 25/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0018 - acc: 2.0855e-04 - val_loss: 4.8026e-04 - val_acc: 0.0000e+00\n",
      "Epoch 26/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0018 - acc: 2.0855e-04 - val_loss: 4.8072e-04 - val_acc: 0.0000e+00\n",
      "Epoch 27/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0018 - acc: 2.0855e-04 - val_loss: 7.9795e-04 - val_acc: 0.0000e+00\n",
      "Epoch 28/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0017 - acc: 2.0855e-04 - val_loss: 5.9275e-04 - val_acc: 0.0000e+00\n",
      "Epoch 29/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0017 - acc: 2.0855e-04 - val_loss: 2.9540e-04 - val_acc: 0.0000e+00\n",
      "Epoch 30/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0017 - acc: 2.0855e-04 - val_loss: 5.6518e-04 - val_acc: 0.0000e+00\n",
      "Epoch 31/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0017 - acc: 2.0855e-04 - val_loss: 3.2123e-04 - val_acc: 0.0000e+00\n",
      "Epoch 32/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0016 - acc: 2.0855e-04 - val_loss: 4.7257e-04 - val_acc: 0.0000e+00\n",
      "Epoch 33/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0016 - acc: 2.0855e-04 - val_loss: 4.8614e-04 - val_acc: 0.0000e+00\n",
      "Epoch 34/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0015 - acc: 2.0855e-04 - val_loss: 3.5984e-04 - val_acc: 0.0000e+00\n",
      "Epoch 35/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0015 - acc: 2.0855e-04 - val_loss: 5.0748e-04 - val_acc: 0.0000e+00\n",
      "Epoch 36/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0016 - acc: 2.0855e-04 - val_loss: 3.1615e-04 - val_acc: 0.0000e+00\n",
      "Epoch 37/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0015 - acc: 2.0855e-04 - val_loss: 5.8611e-04 - val_acc: 0.0000e+00\n",
      "Epoch 38/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0015 - acc: 2.0855e-04 - val_loss: 4.9941e-04 - val_acc: 0.0000e+00\n",
      "Epoch 39/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0014 - acc: 2.0855e-04 - val_loss: 3.6284e-04 - val_acc: 0.0000e+00\n",
      "Epoch 40/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0015 - acc: 2.0855e-04 - val_loss: 6.1849e-04 - val_acc: 0.0000e+00\n",
      "Epoch 41/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0014 - acc: 2.0855e-04 - val_loss: 3.6927e-04 - val_acc: 0.0000e+00\n",
      "Epoch 42/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0015 - acc: 2.0855e-04 - val_loss: 3.1196e-04 - val_acc: 0.0000e+00\n",
      "Epoch 43/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0014 - acc: 2.0855e-04 - val_loss: 3.6154e-04 - val_acc: 0.0000e+00\n",
      "Epoch 44/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0014 - acc: 2.0855e-04 - val_loss: 3.3352e-04 - val_acc: 0.0000e+00\n",
      "Epoch 45/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0014 - acc: 2.0855e-04 - val_loss: 3.6641e-04 - val_acc: 0.0000e+00\n",
      "Epoch 46/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0014 - acc: 2.0855e-04 - val_loss: 2.9747e-04 - val_acc: 0.0000e+00\n",
      "Epoch 47/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0014 - acc: 2.0855e-04 - val_loss: 3.9329e-04 - val_acc: 0.0000e+00\n",
      "Epoch 48/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0013 - acc: 2.0855e-04 - val_loss: 4.7877e-04 - val_acc: 0.0000e+00\n",
      "Epoch 49/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0014 - acc: 2.0855e-04 - val_loss: 4.9905e-04 - val_acc: 0.0000e+00\n",
      "Epoch 50/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0014 - acc: 2.0855e-04 - val_loss: 3.0779e-04 - val_acc: 0.0000e+00\n",
      "Epoch 51/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0014 - acc: 2.0855e-04 - val_loss: 3.0345e-04 - val_acc: 0.0000e+00\n",
      "Epoch 52/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0013 - acc: 2.0855e-04 - val_loss: 5.1926e-04 - val_acc: 0.0000e+00\n",
      "Epoch 53/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0013 - acc: 2.0855e-04 - val_loss: 3.6772e-04 - val_acc: 0.0000e+00\n",
      "Epoch 54/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0014 - acc: 2.0855e-04 - val_loss: 4.0613e-04 - val_acc: 0.0000e+00\n",
      "Epoch 55/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0013 - acc: 2.0855e-04 - val_loss: 2.8864e-04 - val_acc: 0.0000e+00\n",
      "Epoch 56/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0013 - acc: 2.0855e-04 - val_loss: 3.0113e-04 - val_acc: 0.0000e+00\n",
      "Epoch 57/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0013 - acc: 2.0855e-04 - val_loss: 3.5891e-04 - val_acc: 0.0000e+00\n",
      "Epoch 58/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0014 - acc: 2.0855e-04 - val_loss: 4.6546e-04 - val_acc: 0.0000e+00\n",
      "Epoch 59/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0013 - acc: 2.0855e-04 - val_loss: 2.9814e-04 - val_acc: 0.0000e+00\n",
      "Epoch 60/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0013 - acc: 2.0855e-04 - val_loss: 3.0371e-04 - val_acc: 0.0000e+00\n",
      "Epoch 61/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0012 - acc: 2.0855e-04 - val_loss: 5.3786e-04 - val_acc: 0.0000e+00\n",
      "Epoch 62/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0013 - acc: 2.0855e-04 - val_loss: 3.0469e-04 - val_acc: 0.0000e+00\n",
      "Epoch 63/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0013 - acc: 2.0855e-04 - val_loss: 3.0291e-04 - val_acc: 0.0000e+00\n",
      "Epoch 64/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0012 - acc: 2.0855e-04 - val_loss: 3.4725e-04 - val_acc: 0.0000e+00\n",
      "Epoch 65/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0012 - acc: 2.0855e-04 - val_loss: 3.2340e-04 - val_acc: 0.0000e+00\n",
      "Epoch 66/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0012 - acc: 2.0855e-04 - val_loss: 3.0671e-04 - val_acc: 0.0000e+00\n",
      "Epoch 67/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0012 - acc: 2.0855e-04 - val_loss: 3.8089e-04 - val_acc: 0.0000e+00\n",
      "Epoch 68/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0012 - acc: 2.0855e-04 - val_loss: 3.0568e-04 - val_acc: 0.0000e+00\n",
      "Epoch 69/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0012 - acc: 2.0855e-04 - val_loss: 3.9093e-04 - val_acc: 0.0000e+00\n",
      "Epoch 70/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0012 - acc: 2.0855e-04 - val_loss: 3.8128e-04 - val_acc: 0.0000e+00\n",
      "Epoch 71/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0012 - acc: 2.0855e-04 - val_loss: 8.0567e-04 - val_acc: 0.0000e+00\n",
      "Epoch 72/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0012 - acc: 2.0855e-04 - val_loss: 3.9744e-04 - val_acc: 0.0000e+00\n",
      "Epoch 73/90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0012 - acc: 2.0855e-04 - val_loss: 4.2478e-04 - val_acc: 0.0000e+00\n",
      "Epoch 74/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0012 - acc: 2.0855e-04 - val_loss: 4.7589e-04 - val_acc: 0.0000e+00\n",
      "Epoch 75/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0012 - acc: 2.0855e-04 - val_loss: 3.0654e-04 - val_acc: 0.0000e+00\n",
      "Epoch 76/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0011 - acc: 2.0855e-04 - val_loss: 2.7834e-04 - val_acc: 0.0000e+00\n",
      "Epoch 77/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0011 - acc: 2.0855e-04 - val_loss: 3.3366e-04 - val_acc: 0.0000e+00\n",
      "Epoch 78/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0011 - acc: 2.0855e-04 - val_loss: 2.9892e-04 - val_acc: 0.0000e+00\n",
      "Epoch 79/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0011 - acc: 2.0855e-04 - val_loss: 3.3761e-04 - val_acc: 0.0000e+00\n",
      "Epoch 80/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0012 - acc: 2.0855e-04 - val_loss: 3.2490e-04 - val_acc: 0.0000e+00\n",
      "Epoch 81/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0011 - acc: 2.0855e-04 - val_loss: 5.7737e-04 - val_acc: 0.0000e+00\n",
      "Epoch 82/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0012 - acc: 2.0855e-04 - val_loss: 3.5697e-04 - val_acc: 0.0000e+00\n",
      "Epoch 83/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0011 - acc: 2.0855e-04 - val_loss: 4.2697e-04 - val_acc: 0.0000e+00\n",
      "Epoch 84/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0011 - acc: 2.0855e-04 - val_loss: 2.8352e-04 - val_acc: 0.0000e+00\n",
      "Epoch 85/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0012 - acc: 2.0855e-04 - val_loss: 6.8663e-04 - val_acc: 0.0000e+00\n",
      "Epoch 86/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0011 - acc: 2.0855e-04 - val_loss: 2.7782e-04 - val_acc: 0.0000e+00\n",
      "Epoch 87/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0011 - acc: 2.0855e-04 - val_loss: 3.6164e-04 - val_acc: 0.0000e+00\n",
      "Epoch 88/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0011 - acc: 2.0855e-04 - val_loss: 3.0652e-04 - val_acc: 0.0000e+00\n",
      "Epoch 89/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0011 - acc: 2.0855e-04 - val_loss: 2.9641e-04 - val_acc: 0.0000e+00\n",
      "Epoch 90/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0011 - acc: 2.0855e-04 - val_loss: 3.3865e-04 - val_acc: 0.0000e+00\n",
      "-41.980378121740365\n",
      "Amount of features = 9\n",
      "Amount of training data = 13724\n",
      "Amount of testing data = 5882\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_51 (Conv1D)           (None, 21, 64)            1216      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 10, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 9, 64)             8256      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 4, 64)             0         \n",
      "_________________________________________________________________\n",
      "dropout_51 (Dropout)         (None, 4, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_26 (Flatten)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 250)               64250     \n",
      "_________________________________________________________________\n",
      "dropout_52 (Dropout)         (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 1)                 251       \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 73,973\n",
      "Trainable params: 73,973\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 9590 samples, validate on 4111 samples\n",
      "Epoch 1/90\n",
      "9590/9590 [==============================] - 2s 203us/step - loss: 0.0572 - acc: 2.0855e-04 - val_loss: 0.0016 - val_acc: 7.2975e-04\n",
      "Epoch 2/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0110 - acc: 2.0855e-04 - val_loss: 5.8613e-04 - val_acc: 7.2975e-04\n",
      "Epoch 3/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0067 - acc: 2.0855e-04 - val_loss: 4.9607e-04 - val_acc: 7.2975e-04\n",
      "Epoch 4/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0058 - acc: 2.0855e-04 - val_loss: 4.7249e-04 - val_acc: 7.2975e-04\n",
      "Epoch 5/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0050 - acc: 2.0855e-04 - val_loss: 5.3362e-04 - val_acc: 7.2975e-04\n",
      "Epoch 6/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0044 - acc: 2.0855e-04 - val_loss: 6.2340e-04 - val_acc: 7.2975e-04\n",
      "Epoch 7/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0041 - acc: 2.0855e-04 - val_loss: 5.0783e-04 - val_acc: 7.2975e-04\n",
      "Epoch 8/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0037 - acc: 2.0855e-04 - val_loss: 6.2772e-04 - val_acc: 7.2975e-04\n",
      "Epoch 9/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0036 - acc: 2.0855e-04 - val_loss: 6.6158e-04 - val_acc: 7.2975e-04\n",
      "Epoch 10/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0033 - acc: 2.0855e-04 - val_loss: 7.3300e-04 - val_acc: 7.2975e-04\n",
      "Epoch 11/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0031 - acc: 2.0855e-04 - val_loss: 7.2157e-04 - val_acc: 7.2975e-04\n",
      "Epoch 12/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0029 - acc: 2.0855e-04 - val_loss: 8.8939e-04 - val_acc: 7.2975e-04\n",
      "Epoch 13/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0028 - acc: 2.0855e-04 - val_loss: 0.0010 - val_acc: 7.2975e-04\n",
      "Epoch 14/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0026 - acc: 2.0855e-04 - val_loss: 7.2497e-04 - val_acc: 7.2975e-04\n",
      "Epoch 15/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0026 - acc: 2.0855e-04 - val_loss: 8.2337e-04 - val_acc: 7.2975e-04\n",
      "Epoch 16/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0025 - acc: 2.0855e-04 - val_loss: 7.5218e-04 - val_acc: 7.2975e-04\n",
      "Epoch 17/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0024 - acc: 2.0855e-04 - val_loss: 9.6771e-04 - val_acc: 7.2975e-04\n",
      "Epoch 18/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0023 - acc: 2.0855e-04 - val_loss: 8.6775e-04 - val_acc: 7.2975e-04\n",
      "Epoch 19/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0023 - acc: 2.0855e-04 - val_loss: 9.6868e-04 - val_acc: 7.2975e-04\n",
      "Epoch 20/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0024 - acc: 2.0855e-04 - val_loss: 0.0011 - val_acc: 7.2975e-04\n",
      "Epoch 21/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0022 - acc: 2.0855e-04 - val_loss: 0.0012 - val_acc: 7.2975e-04\n",
      "Epoch 22/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0022 - acc: 2.0855e-04 - val_loss: 8.2651e-04 - val_acc: 7.2975e-04\n",
      "Epoch 23/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0022 - acc: 2.0855e-04 - val_loss: 9.7120e-04 - val_acc: 7.2975e-04\n",
      "Epoch 24/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0020 - acc: 2.0855e-04 - val_loss: 8.4210e-04 - val_acc: 7.2975e-04\n",
      "Epoch 25/90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0021 - acc: 2.0855e-04 - val_loss: 9.0407e-04 - val_acc: 7.2975e-04\n",
      "Epoch 26/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0020 - acc: 2.0855e-04 - val_loss: 8.9063e-04 - val_acc: 7.2975e-04\n",
      "Epoch 27/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0022 - acc: 2.0855e-04 - val_loss: 0.0010 - val_acc: 7.2975e-04\n",
      "Epoch 28/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0021 - acc: 2.0855e-04 - val_loss: 0.0012 - val_acc: 7.2975e-04\n",
      "Epoch 29/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0020 - acc: 2.0855e-04 - val_loss: 0.0012 - val_acc: 7.2975e-04\n",
      "Epoch 30/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0019 - acc: 2.0855e-04 - val_loss: 0.0011 - val_acc: 7.2975e-04\n",
      "Epoch 31/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0020 - acc: 2.0855e-04 - val_loss: 8.6793e-04 - val_acc: 7.2975e-04\n",
      "Epoch 32/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0019 - acc: 2.0855e-04 - val_loss: 9.3136e-04 - val_acc: 7.2975e-04\n",
      "Epoch 33/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0019 - acc: 2.0855e-04 - val_loss: 0.0011 - val_acc: 7.2975e-04\n",
      "Epoch 34/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0019 - acc: 2.0855e-04 - val_loss: 0.0012 - val_acc: 7.2975e-04\n",
      "Epoch 35/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0018 - acc: 2.0855e-04 - val_loss: 9.8454e-04 - val_acc: 7.2975e-04\n",
      "Epoch 36/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0019 - acc: 2.0855e-04 - val_loss: 0.0011 - val_acc: 7.2975e-04\n",
      "Epoch 37/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0018 - acc: 2.0855e-04 - val_loss: 0.0012 - val_acc: 7.2975e-04\n",
      "Epoch 38/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0018 - acc: 2.0855e-04 - val_loss: 0.0011 - val_acc: 7.2975e-04\n",
      "Epoch 39/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0017 - acc: 2.0855e-04 - val_loss: 0.0012 - val_acc: 7.2975e-04\n",
      "Epoch 40/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0017 - acc: 2.0855e-04 - val_loss: 0.0011 - val_acc: 7.2975e-04\n",
      "Epoch 41/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0017 - acc: 2.0855e-04 - val_loss: 0.0013 - val_acc: 7.2975e-04\n",
      "Epoch 42/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0017 - acc: 2.0855e-04 - val_loss: 0.0013 - val_acc: 7.2975e-04\n",
      "Epoch 43/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0018 - acc: 2.0855e-04 - val_loss: 0.0014 - val_acc: 7.2975e-04\n",
      "Epoch 44/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0016 - acc: 2.0855e-04 - val_loss: 0.0013 - val_acc: 7.2975e-04\n",
      "Epoch 45/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0016 - acc: 2.0855e-04 - val_loss: 0.0013 - val_acc: 7.2975e-04\n",
      "Epoch 46/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0017 - acc: 2.0855e-04 - val_loss: 0.0014 - val_acc: 7.2975e-04\n",
      "Epoch 47/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0016 - acc: 2.0855e-04 - val_loss: 0.0015 - val_acc: 7.2975e-04\n",
      "Epoch 48/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0016 - acc: 2.0855e-04 - val_loss: 0.0015 - val_acc: 7.2975e-04\n",
      "Epoch 49/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0015 - acc: 2.0855e-04 - val_loss: 0.0013 - val_acc: 7.2975e-04\n",
      "Epoch 50/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0016 - acc: 2.0855e-04 - val_loss: 0.0013 - val_acc: 7.2975e-04\n",
      "Epoch 51/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0015 - acc: 2.0855e-04 - val_loss: 0.0013 - val_acc: 7.2975e-04\n",
      "Epoch 52/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0015 - acc: 2.0855e-04 - val_loss: 0.0014 - val_acc: 7.2975e-04\n",
      "Epoch 53/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0015 - acc: 2.0855e-04 - val_loss: 0.0014 - val_acc: 7.2975e-04\n",
      "Epoch 54/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0015 - acc: 2.0855e-04 - val_loss: 0.0013 - val_acc: 7.2975e-04\n",
      "Epoch 55/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0015 - acc: 2.0855e-04 - val_loss: 0.0019 - val_acc: 7.2975e-04\n",
      "Epoch 56/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0015 - acc: 2.0855e-04 - val_loss: 0.0011 - val_acc: 7.2975e-04\n",
      "Epoch 57/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0015 - acc: 2.0855e-04 - val_loss: 0.0015 - val_acc: 7.2975e-04\n",
      "Epoch 58/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0014 - acc: 2.0855e-04 - val_loss: 0.0017 - val_acc: 7.2975e-04\n",
      "Epoch 59/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0014 - acc: 2.0855e-04 - val_loss: 0.0013 - val_acc: 7.2975e-04\n",
      "Epoch 60/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0014 - acc: 2.0855e-04 - val_loss: 0.0015 - val_acc: 7.2975e-04\n",
      "Epoch 61/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0014 - acc: 2.0855e-04 - val_loss: 0.0014 - val_acc: 7.2975e-04\n",
      "Epoch 62/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0014 - acc: 2.0855e-04 - val_loss: 0.0016 - val_acc: 7.2975e-04\n",
      "Epoch 63/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0013 - acc: 2.0855e-04 - val_loss: 0.0014 - val_acc: 7.2975e-04\n",
      "Epoch 64/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0013 - acc: 2.0855e-04 - val_loss: 0.0016 - val_acc: 7.2975e-04\n",
      "Epoch 65/90\n",
      "9590/9590 [==============================] - 0s 45us/step - loss: 0.0013 - acc: 2.0855e-04 - val_loss: 0.0016 - val_acc: 7.2975e-04\n",
      "Epoch 66/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0013 - acc: 2.0855e-04 - val_loss: 0.0015 - val_acc: 7.2975e-04\n",
      "Epoch 67/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0013 - acc: 2.0855e-04 - val_loss: 0.0017 - val_acc: 7.2975e-04\n",
      "Epoch 68/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0014 - acc: 2.0855e-04 - val_loss: 0.0013 - val_acc: 7.2975e-04\n",
      "Epoch 69/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0013 - acc: 2.0855e-04 - val_loss: 0.0016 - val_acc: 7.2975e-04\n",
      "Epoch 70/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0013 - acc: 2.0855e-04 - val_loss: 0.0014 - val_acc: 7.2975e-04\n",
      "Epoch 71/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0014 - acc: 2.0855e-04 - val_loss: 0.0014 - val_acc: 7.2975e-04\n",
      "Epoch 72/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0013 - acc: 2.0855e-04 - val_loss: 0.0015 - val_acc: 7.2975e-04\n",
      "Epoch 73/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0013 - acc: 2.0855e-04 - val_loss: 0.0017 - val_acc: 7.2975e-04\n",
      "Epoch 74/90\n",
      "9590/9590 [==============================] - 0s 46us/step - loss: 0.0012 - acc: 2.0855e-04 - val_loss: 0.0015 - val_acc: 7.2975e-04\n",
      "Epoch 75/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0013 - acc: 2.0855e-04 - val_loss: 0.0016 - val_acc: 7.2975e-04\n",
      "Epoch 76/90\n",
      "9590/9590 [==============================] - 0s 48us/step - loss: 0.0013 - acc: 2.0855e-04 - val_loss: 0.0015 - val_acc: 7.2975e-04\n",
      "Epoch 77/90\n",
      "9590/9590 [==============================] - 0s 45us/step - loss: 0.0012 - acc: 2.0855e-04 - val_loss: 0.0017 - val_acc: 7.2975e-04\n",
      "Epoch 78/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0012 - acc: 2.0855e-04 - val_loss: 0.0015 - val_acc: 7.2975e-04\n",
      "Epoch 79/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0012 - acc: 2.0855e-04 - val_loss: 0.0017 - val_acc: 7.2975e-04\n",
      "Epoch 80/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0013 - acc: 2.0855e-04 - val_loss: 0.0016 - val_acc: 7.2975e-04\n",
      "Epoch 81/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0012 - acc: 2.0855e-04 - val_loss: 0.0016 - val_acc: 7.2975e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/90\n",
      "9590/9590 [==============================] - 0s 45us/step - loss: 0.0012 - acc: 2.0855e-04 - val_loss: 0.0016 - val_acc: 7.2975e-04\n",
      "Epoch 83/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0012 - acc: 2.0855e-04 - val_loss: 0.0017 - val_acc: 7.2975e-04\n",
      "Epoch 84/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0012 - acc: 2.0855e-04 - val_loss: 0.0016 - val_acc: 7.2975e-04\n",
      "Epoch 85/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0012 - acc: 2.0855e-04 - val_loss: 0.0016 - val_acc: 7.2975e-04\n",
      "Epoch 86/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0012 - acc: 2.0855e-04 - val_loss: 0.0019 - val_acc: 7.2975e-04\n",
      "Epoch 87/90\n",
      "9590/9590 [==============================] - 0s 45us/step - loss: 0.0012 - acc: 2.0855e-04 - val_loss: 0.0018 - val_acc: 7.2975e-04\n",
      "Epoch 88/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0011 - acc: 2.0855e-04 - val_loss: 0.0016 - val_acc: 7.2975e-04\n",
      "Epoch 89/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0012 - acc: 2.0855e-04 - val_loss: 0.0017 - val_acc: 7.2975e-04\n",
      "Epoch 90/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0012 - acc: 2.0855e-04 - val_loss: 0.0016 - val_acc: 7.2975e-04\n",
      "-48.05216438368434\n",
      "Amount of features = 9\n",
      "Amount of training data = 13724\n",
      "Amount of testing data = 5882\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_53 (Conv1D)           (None, 21, 64)            1216      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 10, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 9, 64)             8256      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 4, 64)             0         \n",
      "_________________________________________________________________\n",
      "dropout_53 (Dropout)         (None, 4, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_27 (Flatten)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 250)               64250     \n",
      "_________________________________________________________________\n",
      "dropout_54 (Dropout)         (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 1)                 251       \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 73,973\n",
      "Trainable params: 73,973\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 9590 samples, validate on 4111 samples\n",
      "Epoch 1/90\n",
      "9590/9590 [==============================] - 2s 227us/step - loss: 0.0311 - acc: 1.0428e-04 - val_loss: 0.0027 - val_acc: 2.4325e-04\n",
      "Epoch 2/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0063 - acc: 1.0428e-04 - val_loss: 6.2930e-04 - val_acc: 2.4325e-04\n",
      "Epoch 3/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0041 - acc: 1.0428e-04 - val_loss: 0.0015 - val_acc: 2.4325e-04\n",
      "Epoch 4/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0032 - acc: 1.0428e-04 - val_loss: 0.0012 - val_acc: 2.4325e-04\n",
      "Epoch 5/90\n",
      "9590/9590 [==============================] - 0s 45us/step - loss: 0.0028 - acc: 1.0428e-04 - val_loss: 0.0019 - val_acc: 2.4325e-04\n",
      "Epoch 6/90\n",
      "9590/9590 [==============================] - 0s 45us/step - loss: 0.0023 - acc: 1.0428e-04 - val_loss: 0.0025 - val_acc: 2.4325e-04\n",
      "Epoch 7/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0020 - acc: 1.0428e-04 - val_loss: 0.0026 - val_acc: 2.4325e-04\n",
      "Epoch 8/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0020 - acc: 1.0428e-04 - val_loss: 0.0013 - val_acc: 2.4325e-04\n",
      "Epoch 9/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0019 - acc: 1.0428e-04 - val_loss: 8.1222e-04 - val_acc: 2.4325e-04\n",
      "Epoch 10/90\n",
      "9590/9590 [==============================] - 0s 45us/step - loss: 0.0017 - acc: 1.0428e-04 - val_loss: 0.0013 - val_acc: 2.4325e-04\n",
      "Epoch 11/90\n",
      "9590/9590 [==============================] - 0s 45us/step - loss: 0.0017 - acc: 1.0428e-04 - val_loss: 0.0015 - val_acc: 2.4325e-04\n",
      "Epoch 12/90\n",
      "9590/9590 [==============================] - 0s 45us/step - loss: 0.0016 - acc: 1.0428e-04 - val_loss: 0.0022 - val_acc: 2.4325e-04\n",
      "Epoch 13/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0016 - acc: 1.0428e-04 - val_loss: 0.0015 - val_acc: 2.4325e-04\n",
      "Epoch 14/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0014 - acc: 1.0428e-04 - val_loss: 0.0011 - val_acc: 2.4325e-04\n",
      "Epoch 15/90\n",
      "9590/9590 [==============================] - 0s 45us/step - loss: 0.0014 - acc: 1.0428e-04 - val_loss: 0.0010 - val_acc: 2.4325e-04\n",
      "Epoch 16/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0014 - acc: 1.0428e-04 - val_loss: 0.0018 - val_acc: 2.4325e-04\n",
      "Epoch 17/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0013 - acc: 1.0428e-04 - val_loss: 6.1732e-04 - val_acc: 2.4325e-04\n",
      "Epoch 18/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0013 - acc: 1.0428e-04 - val_loss: 0.0014 - val_acc: 2.4325e-04\n",
      "Epoch 19/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0013 - acc: 1.0428e-04 - val_loss: 0.0013 - val_acc: 2.4325e-04\n",
      "Epoch 20/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0012 - acc: 1.0428e-04 - val_loss: 0.0012 - val_acc: 2.4325e-04\n",
      "Epoch 21/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0012 - acc: 1.0428e-04 - val_loss: 0.0015 - val_acc: 2.4325e-04\n",
      "Epoch 22/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0012 - acc: 1.0428e-04 - val_loss: 0.0014 - val_acc: 2.4325e-04\n",
      "Epoch 23/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0012 - acc: 1.0428e-04 - val_loss: 0.0011 - val_acc: 2.4325e-04\n",
      "Epoch 24/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0012 - acc: 1.0428e-04 - val_loss: 0.0014 - val_acc: 2.4325e-04\n",
      "Epoch 25/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0012 - acc: 1.0428e-04 - val_loss: 6.4160e-04 - val_acc: 2.4325e-04\n",
      "Epoch 26/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0011 - acc: 1.0428e-04 - val_loss: 0.0019 - val_acc: 2.4325e-04\n",
      "Epoch 27/90\n",
      "9590/9590 [==============================] - 0s 45us/step - loss: 0.0012 - acc: 1.0428e-04 - val_loss: 0.0012 - val_acc: 2.4325e-04\n",
      "Epoch 28/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0011 - acc: 1.0428e-04 - val_loss: 0.0012 - val_acc: 2.4325e-04\n",
      "Epoch 29/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0011 - acc: 1.0428e-04 - val_loss: 7.2578e-04 - val_acc: 2.4325e-04\n",
      "Epoch 30/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0011 - acc: 1.0428e-04 - val_loss: 8.7562e-04 - val_acc: 2.4325e-04\n",
      "Epoch 31/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0011 - acc: 1.0428e-04 - val_loss: 7.6485e-04 - val_acc: 2.4325e-04\n",
      "Epoch 32/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0011 - acc: 1.0428e-04 - val_loss: 0.0011 - val_acc: 2.4325e-04\n",
      "Epoch 33/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0011 - acc: 1.0428e-04 - val_loss: 5.5281e-04 - val_acc: 2.4325e-04\n",
      "Epoch 34/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0011 - acc: 1.0428e-04 - val_loss: 5.4948e-04 - val_acc: 2.4325e-04\n",
      "Epoch 35/90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0011 - acc: 1.0428e-04 - val_loss: 7.7492e-04 - val_acc: 2.4325e-04\n",
      "Epoch 36/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0010 - acc: 1.0428e-04 - val_loss: 7.4676e-04 - val_acc: 2.4325e-04\n",
      "Epoch 37/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0010 - acc: 1.0428e-04 - val_loss: 8.1997e-04 - val_acc: 2.4325e-04\n",
      "Epoch 38/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0011 - acc: 1.0428e-04 - val_loss: 5.8444e-04 - val_acc: 2.4325e-04\n",
      "Epoch 39/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0011 - acc: 1.0428e-04 - val_loss: 0.0012 - val_acc: 2.4325e-04\n",
      "Epoch 40/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0010 - acc: 1.0428e-04 - val_loss: 0.0011 - val_acc: 2.4325e-04\n",
      "Epoch 41/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0011 - acc: 1.0428e-04 - val_loss: 0.0011 - val_acc: 2.4325e-04\n",
      "Epoch 42/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0010 - acc: 1.0428e-04 - val_loss: 0.0013 - val_acc: 2.4325e-04\n",
      "Epoch 43/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0011 - acc: 1.0428e-04 - val_loss: 0.0017 - val_acc: 2.4325e-04\n",
      "Epoch 44/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0011 - acc: 1.0428e-04 - val_loss: 5.2637e-04 - val_acc: 2.4325e-04\n",
      "Epoch 45/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0010 - acc: 1.0428e-04 - val_loss: 6.8428e-04 - val_acc: 2.4325e-04\n",
      "Epoch 46/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 9.7873e-04 - acc: 1.0428e-04 - val_loss: 8.7832e-04 - val_acc: 2.4325e-04\n",
      "Epoch 47/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 9.5580e-04 - acc: 1.0428e-04 - val_loss: 4.8621e-04 - val_acc: 2.4325e-04\n",
      "Epoch 48/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 9.9097e-04 - acc: 1.0428e-04 - val_loss: 0.0016 - val_acc: 2.4325e-04\n",
      "Epoch 49/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 9.1775e-04 - acc: 1.0428e-04 - val_loss: 0.0011 - val_acc: 2.4325e-04\n",
      "Epoch 50/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 9.0748e-04 - acc: 1.0428e-04 - val_loss: 6.4501e-04 - val_acc: 2.4325e-04\n",
      "Epoch 51/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 9.1984e-04 - acc: 1.0428e-04 - val_loss: 9.6133e-04 - val_acc: 2.4325e-04\n",
      "Epoch 52/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 9.5369e-04 - acc: 1.0428e-04 - val_loss: 9.3116e-04 - val_acc: 2.4325e-04\n",
      "Epoch 53/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 9.1835e-04 - acc: 1.0428e-04 - val_loss: 7.1411e-04 - val_acc: 2.4325e-04\n",
      "Epoch 54/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 8.9635e-04 - acc: 1.0428e-04 - val_loss: 0.0013 - val_acc: 2.4325e-04\n",
      "Epoch 55/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 9.4526e-04 - acc: 1.0428e-04 - val_loss: 4.6129e-04 - val_acc: 2.4325e-04\n",
      "Epoch 56/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 9.0670e-04 - acc: 1.0428e-04 - val_loss: 9.8413e-04 - val_acc: 2.4325e-04\n",
      "Epoch 57/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 8.8682e-04 - acc: 1.0428e-04 - val_loss: 4.7699e-04 - val_acc: 2.4325e-04\n",
      "Epoch 58/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 8.6751e-04 - acc: 1.0428e-04 - val_loss: 6.1676e-04 - val_acc: 2.4325e-04\n",
      "Epoch 59/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 8.6280e-04 - acc: 1.0428e-04 - val_loss: 6.7196e-04 - val_acc: 2.4325e-04\n",
      "Epoch 60/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 9.0094e-04 - acc: 1.0428e-04 - val_loss: 0.0010 - val_acc: 2.4325e-04\n",
      "Epoch 61/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 8.7825e-04 - acc: 1.0428e-04 - val_loss: 5.4995e-04 - val_acc: 2.4325e-04\n",
      "Epoch 62/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 8.4582e-04 - acc: 1.0428e-04 - val_loss: 4.5809e-04 - val_acc: 2.4325e-04\n",
      "Epoch 63/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 8.6558e-04 - acc: 1.0428e-04 - val_loss: 5.0034e-04 - val_acc: 2.4325e-04\n",
      "Epoch 64/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 8.6707e-04 - acc: 1.0428e-04 - val_loss: 0.0011 - val_acc: 2.4325e-04\n",
      "Epoch 65/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 8.3078e-04 - acc: 1.0428e-04 - val_loss: 7.8101e-04 - val_acc: 2.4325e-04\n",
      "Epoch 66/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 8.4670e-04 - acc: 1.0428e-04 - val_loss: 7.6575e-04 - val_acc: 2.4325e-04\n",
      "Epoch 67/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 8.3477e-04 - acc: 1.0428e-04 - val_loss: 4.7290e-04 - val_acc: 2.4325e-04\n",
      "Epoch 68/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 8.3877e-04 - acc: 1.0428e-04 - val_loss: 5.0712e-04 - val_acc: 2.4325e-04\n",
      "Epoch 69/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 8.3305e-04 - acc: 1.0428e-04 - val_loss: 5.0633e-04 - val_acc: 2.4325e-04\n",
      "Epoch 70/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 8.3434e-04 - acc: 1.0428e-04 - val_loss: 5.0203e-04 - val_acc: 2.4325e-04\n",
      "Epoch 71/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 7.8903e-04 - acc: 1.0428e-04 - val_loss: 4.7655e-04 - val_acc: 2.4325e-04\n",
      "Epoch 72/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 8.1483e-04 - acc: 1.0428e-04 - val_loss: 7.0577e-04 - val_acc: 2.4325e-04\n",
      "Epoch 73/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 8.1270e-04 - acc: 1.0428e-04 - val_loss: 7.8965e-04 - val_acc: 2.4325e-04\n",
      "Epoch 74/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 7.5793e-04 - acc: 1.0428e-04 - val_loss: 5.4282e-04 - val_acc: 2.4325e-04\n",
      "Epoch 75/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 8.1028e-04 - acc: 1.0428e-04 - val_loss: 4.4791e-04 - val_acc: 2.4325e-04\n",
      "Epoch 76/90\n",
      "9590/9590 [==============================] - 0s 45us/step - loss: 8.2767e-04 - acc: 1.0428e-04 - val_loss: 6.9615e-04 - val_acc: 2.4325e-04\n",
      "Epoch 77/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 8.0886e-04 - acc: 1.0428e-04 - val_loss: 4.5620e-04 - val_acc: 2.4325e-04\n",
      "Epoch 78/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 7.8972e-04 - acc: 1.0428e-04 - val_loss: 9.4539e-04 - val_acc: 2.4325e-04\n",
      "Epoch 79/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 7.3202e-04 - acc: 1.0428e-04 - val_loss: 5.7466e-04 - val_acc: 2.4325e-04\n",
      "Epoch 80/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 7.6444e-04 - acc: 1.0428e-04 - val_loss: 8.9419e-04 - val_acc: 2.4325e-04\n",
      "Epoch 81/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 8.0714e-04 - acc: 1.0428e-04 - val_loss: 5.2147e-04 - val_acc: 2.4325e-04\n",
      "Epoch 82/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 7.1978e-04 - acc: 1.0428e-04 - val_loss: 5.0255e-04 - val_acc: 2.4325e-04\n",
      "Epoch 83/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 7.6388e-04 - acc: 1.0428e-04 - val_loss: 4.0613e-04 - val_acc: 2.4325e-04\n",
      "Epoch 84/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 6.9817e-04 - acc: 1.0428e-04 - val_loss: 3.6772e-04 - val_acc: 2.4325e-04\n",
      "Epoch 85/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 7.1943e-04 - acc: 1.0428e-04 - val_loss: 5.3881e-04 - val_acc: 2.4325e-04\n",
      "Epoch 86/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 7.5066e-04 - acc: 1.0428e-04 - val_loss: 6.2020e-04 - val_acc: 2.4325e-04\n",
      "Epoch 87/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 6.9531e-04 - acc: 1.0428e-04 - val_loss: 3.7724e-04 - val_acc: 2.4325e-04\n",
      "Epoch 88/90\n",
      "9590/9590 [==============================] - 0s 45us/step - loss: 6.9317e-04 - acc: 1.0428e-04 - val_loss: 3.8586e-04 - val_acc: 2.4325e-04\n",
      "Epoch 89/90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9590/9590 [==============================] - 0s 43us/step - loss: 7.1501e-04 - acc: 1.0428e-04 - val_loss: 7.9977e-04 - val_acc: 2.4325e-04\n",
      "Epoch 90/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 7.1592e-04 - acc: 1.0428e-04 - val_loss: 0.0011 - val_acc: 2.4325e-04\n",
      "-53.03891752183146\n",
      "Amount of features = 9\n",
      "Amount of training data = 13724\n",
      "Amount of testing data = 5882\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_55 (Conv1D)           (None, 21, 64)            1216      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 10, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 9, 64)             8256      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 4, 64)             0         \n",
      "_________________________________________________________________\n",
      "dropout_55 (Dropout)         (None, 4, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_28 (Flatten)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 250)               64250     \n",
      "_________________________________________________________________\n",
      "dropout_56 (Dropout)         (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 1)                 251       \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 73,973\n",
      "Trainable params: 73,973\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 9590 samples, validate on 4111 samples\n",
      "Epoch 1/90\n",
      "9590/9590 [==============================] - 2s 213us/step - loss: 0.0850 - acc: 1.0428e-04 - val_loss: 0.0068 - val_acc: 4.8650e-04\n",
      "Epoch 2/90\n",
      "9590/9590 [==============================] - 0s 45us/step - loss: 0.0132 - acc: 1.0428e-04 - val_loss: 0.0070 - val_acc: 4.8650e-04\n",
      "Epoch 3/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0088 - acc: 1.0428e-04 - val_loss: 0.0065 - val_acc: 4.8650e-04\n",
      "Epoch 4/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0068 - acc: 1.0428e-04 - val_loss: 0.0057 - val_acc: 4.8650e-04\n",
      "Epoch 5/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0058 - acc: 1.0428e-04 - val_loss: 0.0090 - val_acc: 4.8650e-04\n",
      "Epoch 6/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0053 - acc: 1.0428e-04 - val_loss: 0.0107 - val_acc: 4.8650e-04\n",
      "Epoch 7/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0047 - acc: 1.0428e-04 - val_loss: 0.0118 - val_acc: 4.8650e-04\n",
      "Epoch 8/90\n",
      "9590/9590 [==============================] - 0s 45us/step - loss: 0.0044 - acc: 1.0428e-04 - val_loss: 0.0137 - val_acc: 4.8650e-04\n",
      "Epoch 9/90\n",
      "9590/9590 [==============================] - 0s 45us/step - loss: 0.0043 - acc: 1.0428e-04 - val_loss: 0.0134 - val_acc: 4.8650e-04\n",
      "Epoch 10/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0040 - acc: 1.0428e-04 - val_loss: 0.0136 - val_acc: 4.8650e-04\n",
      "Epoch 11/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0038 - acc: 1.0428e-04 - val_loss: 0.0140 - val_acc: 4.8650e-04\n",
      "Epoch 12/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0037 - acc: 1.0428e-04 - val_loss: 0.0159 - val_acc: 4.8650e-04\n",
      "Epoch 13/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0037 - acc: 1.0428e-04 - val_loss: 0.0170 - val_acc: 4.8650e-04\n",
      "Epoch 14/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0033 - acc: 1.0428e-04 - val_loss: 0.0179 - val_acc: 4.8650e-04\n",
      "Epoch 15/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0032 - acc: 1.0428e-04 - val_loss: 0.0184 - val_acc: 4.8650e-04\n",
      "Epoch 16/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0030 - acc: 1.0428e-04 - val_loss: 0.0210 - val_acc: 4.8650e-04\n",
      "Epoch 17/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0030 - acc: 1.0428e-04 - val_loss: 0.0182 - val_acc: 4.8650e-04\n",
      "Epoch 18/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0029 - acc: 1.0428e-04 - val_loss: 0.0175 - val_acc: 4.8650e-04\n",
      "Epoch 19/90\n",
      "9590/9590 [==============================] - 0s 45us/step - loss: 0.0028 - acc: 1.0428e-04 - val_loss: 0.0176 - val_acc: 4.8650e-04\n",
      "Epoch 20/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0028 - acc: 1.0428e-04 - val_loss: 0.0127 - val_acc: 4.8650e-04\n",
      "Epoch 21/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0028 - acc: 1.0428e-04 - val_loss: 0.0183 - val_acc: 4.8650e-04\n",
      "Epoch 22/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0027 - acc: 1.0428e-04 - val_loss: 0.0175 - val_acc: 4.8650e-04\n",
      "Epoch 23/90\n",
      "9590/9590 [==============================] - 0s 46us/step - loss: 0.0027 - acc: 1.0428e-04 - val_loss: 0.0189 - val_acc: 4.8650e-04\n",
      "Epoch 24/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0026 - acc: 1.0428e-04 - val_loss: 0.0191 - val_acc: 4.8650e-04\n",
      "Epoch 25/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0024 - acc: 1.0428e-04 - val_loss: 0.0197 - val_acc: 4.8650e-04\n",
      "Epoch 26/90\n",
      "9590/9590 [==============================] - 0s 45us/step - loss: 0.0024 - acc: 1.0428e-04 - val_loss: 0.0209 - val_acc: 4.8650e-04\n",
      "Epoch 27/90\n",
      "9590/9590 [==============================] - 0s 45us/step - loss: 0.0024 - acc: 1.0428e-04 - val_loss: 0.0243 - val_acc: 4.8650e-04\n",
      "Epoch 28/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0024 - acc: 1.0428e-04 - val_loss: 0.0233 - val_acc: 4.8650e-04\n",
      "Epoch 29/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0023 - acc: 1.0428e-04 - val_loss: 0.0184 - val_acc: 4.8650e-04\n",
      "Epoch 30/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0023 - acc: 1.0428e-04 - val_loss: 0.0217 - val_acc: 4.8650e-04\n",
      "Epoch 31/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0024 - acc: 1.0428e-04 - val_loss: 0.0209 - val_acc: 4.8650e-04\n",
      "Epoch 32/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0023 - acc: 1.0428e-04 - val_loss: 0.0221 - val_acc: 4.8650e-04\n",
      "Epoch 33/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0023 - acc: 1.0428e-04 - val_loss: 0.0237 - val_acc: 4.8650e-04\n",
      "Epoch 34/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0023 - acc: 1.0428e-04 - val_loss: 0.0214 - val_acc: 4.8650e-04\n",
      "Epoch 35/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0022 - acc: 1.0428e-04 - val_loss: 0.0171 - val_acc: 4.8650e-04\n",
      "Epoch 36/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0021 - acc: 1.0428e-04 - val_loss: 0.0180 - val_acc: 4.8650e-04\n",
      "Epoch 37/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0021 - acc: 1.0428e-04 - val_loss: 0.0191 - val_acc: 4.8650e-04\n",
      "Epoch 38/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0021 - acc: 1.0428e-04 - val_loss: 0.0216 - val_acc: 4.8650e-04\n",
      "Epoch 39/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0021 - acc: 1.0428e-04 - val_loss: 0.0193 - val_acc: 4.8650e-04\n",
      "Epoch 40/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0020 - acc: 1.0428e-04 - val_loss: 0.0196 - val_acc: 4.8650e-04\n",
      "Epoch 41/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0020 - acc: 1.0428e-04 - val_loss: 0.0200 - val_acc: 4.8650e-04\n",
      "Epoch 42/90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0019 - acc: 1.0428e-04 - val_loss: 0.0182 - val_acc: 4.8650e-04\n",
      "Epoch 43/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0020 - acc: 1.0428e-04 - val_loss: 0.0170 - val_acc: 4.8650e-04\n",
      "Epoch 44/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0019 - acc: 1.0428e-04 - val_loss: 0.0201 - val_acc: 4.8650e-04\n",
      "Epoch 45/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0019 - acc: 1.0428e-04 - val_loss: 0.0192 - val_acc: 4.8650e-04\n",
      "Epoch 46/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0019 - acc: 1.0428e-04 - val_loss: 0.0182 - val_acc: 4.8650e-04\n",
      "Epoch 47/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0018 - acc: 1.0428e-04 - val_loss: 0.0180 - val_acc: 4.8650e-04\n",
      "Epoch 48/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0019 - acc: 1.0428e-04 - val_loss: 0.0219 - val_acc: 4.8650e-04\n",
      "Epoch 49/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0018 - acc: 1.0428e-04 - val_loss: 0.0182 - val_acc: 4.8650e-04\n",
      "Epoch 50/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0019 - acc: 1.0428e-04 - val_loss: 0.0191 - val_acc: 4.8650e-04\n",
      "Epoch 51/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0019 - acc: 1.0428e-04 - val_loss: 0.0167 - val_acc: 4.8650e-04\n",
      "Epoch 52/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0018 - acc: 1.0428e-04 - val_loss: 0.0185 - val_acc: 4.8650e-04\n",
      "Epoch 53/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0018 - acc: 1.0428e-04 - val_loss: 0.0164 - val_acc: 4.8650e-04\n",
      "Epoch 54/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0017 - acc: 1.0428e-04 - val_loss: 0.0193 - val_acc: 4.8650e-04\n",
      "Epoch 55/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0019 - acc: 1.0428e-04 - val_loss: 0.0217 - val_acc: 4.8650e-04\n",
      "Epoch 56/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0019 - acc: 1.0428e-04 - val_loss: 0.0178 - val_acc: 4.8650e-04\n",
      "Epoch 57/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0017 - acc: 1.0428e-04 - val_loss: 0.0216 - val_acc: 4.8650e-04\n",
      "Epoch 58/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0018 - acc: 1.0428e-04 - val_loss: 0.0184 - val_acc: 4.8650e-04\n",
      "Epoch 59/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0018 - acc: 1.0428e-04 - val_loss: 0.0189 - val_acc: 4.8650e-04\n",
      "Epoch 60/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0017 - acc: 1.0428e-04 - val_loss: 0.0193 - val_acc: 4.8650e-04\n",
      "Epoch 61/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0017 - acc: 1.0428e-04 - val_loss: 0.0175 - val_acc: 4.8650e-04\n",
      "Epoch 62/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0017 - acc: 1.0428e-04 - val_loss: 0.0190 - val_acc: 4.8650e-04\n",
      "Epoch 63/90\n",
      "9590/9590 [==============================] - 0s 45us/step - loss: 0.0017 - acc: 1.0428e-04 - val_loss: 0.0151 - val_acc: 4.8650e-04\n",
      "Epoch 64/90\n",
      "9590/9590 [==============================] - 0s 46us/step - loss: 0.0017 - acc: 1.0428e-04 - val_loss: 0.0169 - val_acc: 4.8650e-04\n",
      "Epoch 65/90\n",
      "9590/9590 [==============================] - 0s 46us/step - loss: 0.0018 - acc: 1.0428e-04 - val_loss: 0.0165 - val_acc: 4.8650e-04\n",
      "Epoch 66/90\n",
      "9590/9590 [==============================] - 0s 47us/step - loss: 0.0018 - acc: 1.0428e-04 - val_loss: 0.0181 - val_acc: 4.8650e-04\n",
      "Epoch 67/90\n",
      "9590/9590 [==============================] - 0s 46us/step - loss: 0.0016 - acc: 1.0428e-04 - val_loss: 0.0177 - val_acc: 4.8650e-04\n",
      "Epoch 68/90\n",
      "9590/9590 [==============================] - 0s 46us/step - loss: 0.0016 - acc: 1.0428e-04 - val_loss: 0.0148 - val_acc: 4.8650e-04\n",
      "Epoch 69/90\n",
      "9590/9590 [==============================] - 0s 47us/step - loss: 0.0016 - acc: 1.0428e-04 - val_loss: 0.0213 - val_acc: 4.8650e-04\n",
      "Epoch 70/90\n",
      "9590/9590 [==============================] - 0s 46us/step - loss: 0.0016 - acc: 1.0428e-04 - val_loss: 0.0157 - val_acc: 4.8650e-04\n",
      "Epoch 71/90\n",
      "9590/9590 [==============================] - 0s 46us/step - loss: 0.0015 - acc: 1.0428e-04 - val_loss: 0.0161 - val_acc: 4.8650e-04\n",
      "Epoch 72/90\n",
      "9590/9590 [==============================] - 0s 47us/step - loss: 0.0016 - acc: 1.0428e-04 - val_loss: 0.0187 - val_acc: 4.8650e-04\n",
      "Epoch 73/90\n",
      "9590/9590 [==============================] - 0s 45us/step - loss: 0.0016 - acc: 1.0428e-04 - val_loss: 0.0169 - val_acc: 4.8650e-04\n",
      "Epoch 74/90\n",
      "9590/9590 [==============================] - 0s 45us/step - loss: 0.0016 - acc: 1.0428e-04 - val_loss: 0.0161 - val_acc: 4.8650e-04\n",
      "Epoch 75/90\n",
      "9590/9590 [==============================] - 0s 45us/step - loss: 0.0017 - acc: 1.0428e-04 - val_loss: 0.0166 - val_acc: 4.8650e-04\n",
      "Epoch 76/90\n",
      "9590/9590 [==============================] - 0s 46us/step - loss: 0.0017 - acc: 1.0428e-04 - val_loss: 0.0185 - val_acc: 4.8650e-04\n",
      "Epoch 77/90\n",
      "9590/9590 [==============================] - 0s 46us/step - loss: 0.0016 - acc: 1.0428e-04 - val_loss: 0.0164 - val_acc: 4.8650e-04\n",
      "Epoch 78/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0015 - acc: 1.0428e-04 - val_loss: 0.0138 - val_acc: 4.8650e-04\n",
      "Epoch 79/90\n",
      "9590/9590 [==============================] - 0s 45us/step - loss: 0.0016 - acc: 1.0428e-04 - val_loss: 0.0162 - val_acc: 4.8650e-04\n",
      "Epoch 80/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0015 - acc: 1.0428e-04 - val_loss: 0.0163 - val_acc: 4.8650e-04\n",
      "Epoch 81/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0016 - acc: 1.0428e-04 - val_loss: 0.0150 - val_acc: 4.8650e-04\n",
      "Epoch 82/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0015 - acc: 1.0428e-04 - val_loss: 0.0168 - val_acc: 4.8650e-04\n",
      "Epoch 83/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0015 - acc: 1.0428e-04 - val_loss: 0.0149 - val_acc: 4.8650e-04\n",
      "Epoch 84/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0015 - acc: 1.0428e-04 - val_loss: 0.0149 - val_acc: 4.8650e-04\n",
      "Epoch 85/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0015 - acc: 1.0428e-04 - val_loss: 0.0154 - val_acc: 4.8650e-04\n",
      "Epoch 86/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0015 - acc: 1.0428e-04 - val_loss: 0.0151 - val_acc: 4.8650e-04\n",
      "Epoch 87/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0015 - acc: 1.0428e-04 - val_loss: 0.0155 - val_acc: 4.8650e-04\n",
      "Epoch 88/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0014 - acc: 1.0428e-04 - val_loss: 0.0157 - val_acc: 4.8650e-04\n",
      "Epoch 89/90\n",
      "9590/9590 [==============================] - 0s 45us/step - loss: 0.0015 - acc: 1.0428e-04 - val_loss: 0.0169 - val_acc: 4.8650e-04\n",
      "Epoch 90/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0015 - acc: 1.0428e-04 - val_loss: 0.0140 - val_acc: 4.8650e-04\n",
      "-32.01571796685397\n",
      "Amount of features = 9\n",
      "Amount of training data = 13724\n",
      "Amount of testing data = 5882\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_57 (Conv1D)           (None, 21, 64)            1216      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 10, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 9, 64)             8256      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 4, 64)             0         \n",
      "_________________________________________________________________\n",
      "dropout_57 (Dropout)         (None, 4, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_29 (Flatten)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 250)               64250     \n",
      "_________________________________________________________________\n",
      "dropout_58 (Dropout)         (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 1)                 251       \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 73,973\n",
      "Trainable params: 73,973\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9590 samples, validate on 4111 samples\n",
      "Epoch 1/90\n",
      "9590/9590 [==============================] - 2s 216us/step - loss: 0.1130 - acc: 3.1283e-04 - val_loss: 0.0088 - val_acc: 0.0000e+00\n",
      "Epoch 2/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0138 - acc: 4.1710e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 3/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0094 - acc: 4.1710e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 4/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0075 - acc: 4.1710e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 5/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0059 - acc: 4.1710e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 6/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0051 - acc: 4.1710e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 7/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0048 - acc: 4.1710e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 8/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0043 - acc: 4.1710e-04 - val_loss: 8.9053e-04 - val_acc: 0.0000e+00\n",
      "Epoch 9/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0038 - acc: 4.1710e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 10/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0036 - acc: 4.1710e-04 - val_loss: 5.8368e-04 - val_acc: 0.0000e+00\n",
      "Epoch 11/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0035 - acc: 4.1710e-04 - val_loss: 8.2735e-04 - val_acc: 0.0000e+00\n",
      "Epoch 12/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0031 - acc: 4.1710e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 13/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0031 - acc: 4.1710e-04 - val_loss: 5.8867e-04 - val_acc: 0.0000e+00\n",
      "Epoch 14/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0029 - acc: 4.1710e-04 - val_loss: 6.5297e-04 - val_acc: 0.0000e+00\n",
      "Epoch 15/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0027 - acc: 4.1710e-04 - val_loss: 9.4946e-04 - val_acc: 0.0000e+00\n",
      "Epoch 16/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0026 - acc: 4.1710e-04 - val_loss: 6.8186e-04 - val_acc: 0.0000e+00\n",
      "Epoch 17/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0026 - acc: 4.1710e-04 - val_loss: 4.6982e-04 - val_acc: 0.0000e+00\n",
      "Epoch 18/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0026 - acc: 4.1710e-04 - val_loss: 4.6739e-04 - val_acc: 0.0000e+00\n",
      "Epoch 19/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0024 - acc: 4.1710e-04 - val_loss: 4.9780e-04 - val_acc: 0.0000e+00\n",
      "Epoch 20/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0024 - acc: 4.1710e-04 - val_loss: 4.2755e-04 - val_acc: 0.0000e+00\n",
      "Epoch 21/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0024 - acc: 4.1710e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 22/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0023 - acc: 4.1710e-04 - val_loss: 4.7235e-04 - val_acc: 0.0000e+00\n",
      "Epoch 23/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0022 - acc: 4.1710e-04 - val_loss: 4.0836e-04 - val_acc: 0.0000e+00\n",
      "Epoch 24/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0021 - acc: 4.1710e-04 - val_loss: 4.2336e-04 - val_acc: 0.0000e+00\n",
      "Epoch 25/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0021 - acc: 4.1710e-04 - val_loss: 5.7261e-04 - val_acc: 0.0000e+00\n",
      "Epoch 26/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0022 - acc: 4.1710e-04 - val_loss: 7.1315e-04 - val_acc: 0.0000e+00\n",
      "Epoch 27/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0021 - acc: 4.1710e-04 - val_loss: 5.8565e-04 - val_acc: 0.0000e+00\n",
      "Epoch 28/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0020 - acc: 4.1710e-04 - val_loss: 4.2001e-04 - val_acc: 0.0000e+00\n",
      "Epoch 29/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0022 - acc: 4.1710e-04 - val_loss: 4.1606e-04 - val_acc: 0.0000e+00\n",
      "Epoch 30/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0022 - acc: 4.1710e-04 - val_loss: 4.8038e-04 - val_acc: 0.0000e+00\n",
      "Epoch 31/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0020 - acc: 4.1710e-04 - val_loss: 5.2685e-04 - val_acc: 0.0000e+00\n",
      "Epoch 32/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0019 - acc: 4.1710e-04 - val_loss: 4.1768e-04 - val_acc: 0.0000e+00\n",
      "Epoch 33/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0018 - acc: 4.1710e-04 - val_loss: 4.3170e-04 - val_acc: 0.0000e+00\n",
      "Epoch 34/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 0.0018 - acc: 4.1710e-04 - val_loss: 3.9119e-04 - val_acc: 0.0000e+00\n",
      "Epoch 35/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0018 - acc: 4.1710e-04 - val_loss: 4.5340e-04 - val_acc: 0.0000e+00\n",
      "Epoch 36/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0018 - acc: 4.1710e-04 - val_loss: 4.0113e-04 - val_acc: 0.0000e+00\n",
      "Epoch 37/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0018 - acc: 4.1710e-04 - val_loss: 3.8773e-04 - val_acc: 0.0000e+00\n",
      "Epoch 38/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0019 - acc: 4.1710e-04 - val_loss: 8.2178e-04 - val_acc: 0.0000e+00\n",
      "Epoch 39/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0017 - acc: 4.1710e-04 - val_loss: 6.4893e-04 - val_acc: 0.0000e+00\n",
      "Epoch 40/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0017 - acc: 4.1710e-04 - val_loss: 4.5962e-04 - val_acc: 0.0000e+00\n",
      "Epoch 41/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0019 - acc: 4.1710e-04 - val_loss: 4.4217e-04 - val_acc: 0.0000e+00\n",
      "Epoch 42/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0017 - acc: 4.1710e-04 - val_loss: 4.2333e-04 - val_acc: 0.0000e+00\n",
      "Epoch 43/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0018 - acc: 4.1710e-04 - val_loss: 4.3404e-04 - val_acc: 0.0000e+00\n",
      "Epoch 44/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0018 - acc: 4.1710e-04 - val_loss: 5.1150e-04 - val_acc: 0.0000e+00\n",
      "Epoch 45/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0016 - acc: 4.1710e-04 - val_loss: 5.0781e-04 - val_acc: 0.0000e+00\n",
      "Epoch 46/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0017 - acc: 4.1710e-04 - val_loss: 4.3487e-04 - val_acc: 0.0000e+00\n",
      "Epoch 47/90\n",
      "9590/9590 [==============================] - 0s 45us/step - loss: 0.0017 - acc: 4.1710e-04 - val_loss: 5.2460e-04 - val_acc: 0.0000e+00\n",
      "Epoch 48/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0016 - acc: 4.1710e-04 - val_loss: 4.3746e-04 - val_acc: 0.0000e+00\n",
      "Epoch 49/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0016 - acc: 4.1710e-04 - val_loss: 4.2707e-04 - val_acc: 0.0000e+00\n",
      "Epoch 50/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0016 - acc: 4.1710e-04 - val_loss: 5.1523e-04 - val_acc: 0.0000e+00\n",
      "Epoch 51/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0016 - acc: 4.1710e-04 - val_loss: 4.8829e-04 - val_acc: 0.0000e+00\n",
      "Epoch 52/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0015 - acc: 4.1710e-04 - val_loss: 5.5585e-04 - val_acc: 0.0000e+00\n",
      "Epoch 53/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0016 - acc: 4.1710e-04 - val_loss: 4.8367e-04 - val_acc: 0.0000e+00\n",
      "Epoch 54/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0016 - acc: 4.1710e-04 - val_loss: 4.7879e-04 - val_acc: 0.0000e+00\n",
      "Epoch 55/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0015 - acc: 4.1710e-04 - val_loss: 4.7669e-04 - val_acc: 0.0000e+00\n",
      "Epoch 56/90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0016 - acc: 4.1710e-04 - val_loss: 4.8290e-04 - val_acc: 0.0000e+00\n",
      "Epoch 57/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0015 - acc: 4.1710e-04 - val_loss: 4.6170e-04 - val_acc: 0.0000e+00\n",
      "Epoch 58/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0015 - acc: 4.1710e-04 - val_loss: 4.6642e-04 - val_acc: 0.0000e+00\n",
      "Epoch 59/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0015 - acc: 4.1710e-04 - val_loss: 5.0690e-04 - val_acc: 0.0000e+00\n",
      "Epoch 60/90\n",
      "9590/9590 [==============================] - 0s 46us/step - loss: 0.0015 - acc: 4.1710e-04 - val_loss: 5.2318e-04 - val_acc: 0.0000e+00\n",
      "Epoch 61/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0015 - acc: 4.1710e-04 - val_loss: 4.6602e-04 - val_acc: 0.0000e+00\n",
      "Epoch 62/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0015 - acc: 4.1710e-04 - val_loss: 5.1805e-04 - val_acc: 0.0000e+00\n",
      "Epoch 63/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0015 - acc: 4.1710e-04 - val_loss: 5.6697e-04 - val_acc: 0.0000e+00\n",
      "Epoch 64/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0015 - acc: 4.1710e-04 - val_loss: 6.1595e-04 - val_acc: 0.0000e+00\n",
      "Epoch 65/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0016 - acc: 4.1710e-04 - val_loss: 6.9435e-04 - val_acc: 0.0000e+00\n",
      "Epoch 66/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0014 - acc: 4.1710e-04 - val_loss: 4.7472e-04 - val_acc: 0.0000e+00\n",
      "Epoch 67/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0015 - acc: 4.1710e-04 - val_loss: 5.2811e-04 - val_acc: 0.0000e+00\n",
      "Epoch 68/90\n",
      "9590/9590 [==============================] - 0s 47us/step - loss: 0.0014 - acc: 4.1710e-04 - val_loss: 5.2293e-04 - val_acc: 0.0000e+00\n",
      "Epoch 69/90\n",
      "9590/9590 [==============================] - 0s 48us/step - loss: 0.0014 - acc: 4.1710e-04 - val_loss: 4.3749e-04 - val_acc: 0.0000e+00\n",
      "Epoch 70/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0014 - acc: 4.1710e-04 - val_loss: 4.5160e-04 - val_acc: 0.0000e+00\n",
      "Epoch 71/90\n",
      "9590/9590 [==============================] - 0s 46us/step - loss: 0.0013 - acc: 4.1710e-04 - val_loss: 6.5618e-04 - val_acc: 0.0000e+00\n",
      "Epoch 72/90\n",
      "9590/9590 [==============================] - 0s 47us/step - loss: 0.0014 - acc: 4.1710e-04 - val_loss: 4.2467e-04 - val_acc: 0.0000e+00\n",
      "Epoch 73/90\n",
      "9590/9590 [==============================] - 0s 45us/step - loss: 0.0014 - acc: 4.1710e-04 - val_loss: 6.6358e-04 - val_acc: 0.0000e+00\n",
      "Epoch 74/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0014 - acc: 4.1710e-04 - val_loss: 4.9444e-04 - val_acc: 0.0000e+00\n",
      "Epoch 75/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0014 - acc: 4.1710e-04 - val_loss: 4.6320e-04 - val_acc: 0.0000e+00\n",
      "Epoch 76/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0014 - acc: 4.1710e-04 - val_loss: 5.4666e-04 - val_acc: 0.0000e+00\n",
      "Epoch 77/90\n",
      "9590/9590 [==============================] - 0s 46us/step - loss: 0.0013 - acc: 4.1710e-04 - val_loss: 5.0882e-04 - val_acc: 0.0000e+00\n",
      "Epoch 78/90\n",
      "9590/9590 [==============================] - 0s 45us/step - loss: 0.0014 - acc: 4.1710e-04 - val_loss: 4.3958e-04 - val_acc: 0.0000e+00\n",
      "Epoch 79/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0014 - acc: 4.1710e-04 - val_loss: 6.3078e-04 - val_acc: 0.0000e+00\n",
      "Epoch 80/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0013 - acc: 4.1710e-04 - val_loss: 6.7332e-04 - val_acc: 0.0000e+00\n",
      "Epoch 81/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0013 - acc: 4.1710e-04 - val_loss: 6.3701e-04 - val_acc: 0.0000e+00\n",
      "Epoch 82/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0014 - acc: 4.1710e-04 - val_loss: 6.9732e-04 - val_acc: 0.0000e+00\n",
      "Epoch 83/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0013 - acc: 4.1710e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 84/90\n",
      "9590/9590 [==============================] - 0s 49us/step - loss: 0.0013 - acc: 4.1710e-04 - val_loss: 5.7488e-04 - val_acc: 0.0000e+00\n",
      "Epoch 85/90\n",
      "9590/9590 [==============================] - 2s 249us/step - loss: 0.0014 - acc: 4.1710e-04 - val_loss: 5.6379e-04 - val_acc: 0.0000e+00\n",
      "Epoch 86/90\n",
      "9590/9590 [==============================] - 1s 55us/step - loss: 0.0014 - acc: 4.1710e-04 - val_loss: 7.1294e-04 - val_acc: 0.0000e+00\n",
      "Epoch 87/90\n",
      "9590/9590 [==============================] - 0s 49us/step - loss: 0.0013 - acc: 4.1710e-04 - val_loss: 4.9143e-04 - val_acc: 0.0000e+00\n",
      "Epoch 88/90\n",
      "9590/9590 [==============================] - 0s 49us/step - loss: 0.0013 - acc: 4.1710e-04 - val_loss: 7.0844e-04 - val_acc: 0.0000e+00\n",
      "Epoch 89/90\n",
      "9590/9590 [==============================] - 0s 51us/step - loss: 0.0013 - acc: 4.1710e-04 - val_loss: 4.4413e-04 - val_acc: 0.0000e+00\n",
      "Epoch 90/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0013 - acc: 4.1710e-04 - val_loss: 6.7168e-04 - val_acc: 0.0000e+00\n",
      "-50.233957955472654\n",
      "Amount of features = 9\n",
      "Amount of training data = 13724\n",
      "Amount of testing data = 5882\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_59 (Conv1D)           (None, 21, 64)            1216      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 10, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_60 (Conv1D)           (None, 9, 64)             8256      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 4, 64)             0         \n",
      "_________________________________________________________________\n",
      "dropout_59 (Dropout)         (None, 4, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_30 (Flatten)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 250)               64250     \n",
      "_________________________________________________________________\n",
      "dropout_60 (Dropout)         (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 1)                 251       \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 73,973\n",
      "Trainable params: 73,973\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 9590 samples, validate on 4111 samples\n",
      "Epoch 1/90\n",
      "9590/9590 [==============================] - 2s 227us/step - loss: 0.0293 - acc: 1.0428e-04 - val_loss: 0.0101 - val_acc: 2.4325e-04\n",
      "Epoch 2/90\n",
      "9590/9590 [==============================] - 0s 46us/step - loss: 0.0057 - acc: 1.0428e-04 - val_loss: 9.1727e-04 - val_acc: 2.4325e-04\n",
      "Epoch 3/90\n",
      "9590/9590 [==============================] - 0s 45us/step - loss: 0.0036 - acc: 1.0428e-04 - val_loss: 0.0017 - val_acc: 2.4325e-04\n",
      "Epoch 4/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0031 - acc: 1.0428e-04 - val_loss: 0.0030 - val_acc: 2.4325e-04\n",
      "Epoch 5/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0026 - acc: 1.0428e-04 - val_loss: 0.0020 - val_acc: 2.4325e-04\n",
      "Epoch 6/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0022 - acc: 1.0428e-04 - val_loss: 0.0017 - val_acc: 2.4325e-04\n",
      "Epoch 7/90\n",
      "9590/9590 [==============================] - 0s 45us/step - loss: 0.0020 - acc: 1.0428e-04 - val_loss: 0.0029 - val_acc: 2.4325e-04\n",
      "Epoch 8/90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9590/9590 [==============================] - 0s 45us/step - loss: 0.0018 - acc: 1.0428e-04 - val_loss: 0.0011 - val_acc: 2.4325e-04\n",
      "Epoch 9/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0016 - acc: 1.0428e-04 - val_loss: 0.0015 - val_acc: 2.4325e-04\n",
      "Epoch 10/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0015 - acc: 1.0428e-04 - val_loss: 0.0019 - val_acc: 2.4325e-04\n",
      "Epoch 11/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0014 - acc: 1.0428e-04 - val_loss: 0.0030 - val_acc: 2.4325e-04\n",
      "Epoch 12/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0014 - acc: 1.0428e-04 - val_loss: 0.0017 - val_acc: 2.4325e-04\n",
      "Epoch 13/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0013 - acc: 1.0428e-04 - val_loss: 0.0018 - val_acc: 2.4325e-04\n",
      "Epoch 14/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0013 - acc: 1.0428e-04 - val_loss: 0.0013 - val_acc: 2.4325e-04\n",
      "Epoch 15/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0013 - acc: 1.0428e-04 - val_loss: 0.0016 - val_acc: 2.4325e-04\n",
      "Epoch 16/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0012 - acc: 1.0428e-04 - val_loss: 0.0017 - val_acc: 2.4325e-04\n",
      "Epoch 17/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0012 - acc: 1.0428e-04 - val_loss: 0.0034 - val_acc: 2.4325e-04\n",
      "Epoch 18/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0012 - acc: 1.0428e-04 - val_loss: 0.0024 - val_acc: 2.4325e-04\n",
      "Epoch 19/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0011 - acc: 1.0428e-04 - val_loss: 0.0021 - val_acc: 2.4325e-04\n",
      "Epoch 20/90\n",
      "9590/9590 [==============================] - 0s 45us/step - loss: 0.0011 - acc: 1.0428e-04 - val_loss: 0.0031 - val_acc: 2.4325e-04\n",
      "Epoch 21/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0010 - acc: 1.0428e-04 - val_loss: 0.0027 - val_acc: 2.4325e-04\n",
      "Epoch 22/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0011 - acc: 1.0428e-04 - val_loss: 0.0029 - val_acc: 2.4325e-04\n",
      "Epoch 23/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0010 - acc: 1.0428e-04 - val_loss: 0.0044 - val_acc: 2.4325e-04\n",
      "Epoch 24/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0010 - acc: 1.0428e-04 - val_loss: 0.0025 - val_acc: 2.4325e-04\n",
      "Epoch 25/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 9.9376e-04 - acc: 1.0428e-04 - val_loss: 0.0024 - val_acc: 2.4325e-04\n",
      "Epoch 26/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 9.7033e-04 - acc: 1.0428e-04 - val_loss: 0.0014 - val_acc: 2.4325e-04\n",
      "Epoch 27/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0010 - acc: 1.0428e-04 - val_loss: 0.0020 - val_acc: 2.4325e-04\n",
      "Epoch 28/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0010 - acc: 1.0428e-04 - val_loss: 0.0032 - val_acc: 2.4325e-04\n",
      "Epoch 29/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 9.4967e-04 - acc: 1.0428e-04 - val_loss: 0.0011 - val_acc: 2.4325e-04\n",
      "Epoch 30/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 9.6475e-04 - acc: 1.0428e-04 - val_loss: 0.0016 - val_acc: 2.4325e-04\n",
      "Epoch 31/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 9.2159e-04 - acc: 1.0428e-04 - val_loss: 0.0027 - val_acc: 2.4325e-04\n",
      "Epoch 32/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 9.1847e-04 - acc: 1.0428e-04 - val_loss: 0.0016 - val_acc: 2.4325e-04\n",
      "Epoch 33/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 9.7131e-04 - acc: 1.0428e-04 - val_loss: 0.0014 - val_acc: 2.4325e-04\n",
      "Epoch 34/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 9.0994e-04 - acc: 1.0428e-04 - val_loss: 0.0024 - val_acc: 2.4325e-04\n",
      "Epoch 35/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 8.6748e-04 - acc: 1.0428e-04 - val_loss: 0.0018 - val_acc: 2.4325e-04\n",
      "Epoch 36/90\n",
      "9590/9590 [==============================] - 0s 45us/step - loss: 8.8680e-04 - acc: 1.0428e-04 - val_loss: 0.0029 - val_acc: 2.4325e-04\n",
      "Epoch 37/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 8.8285e-04 - acc: 1.0428e-04 - val_loss: 0.0016 - val_acc: 2.4325e-04\n",
      "Epoch 38/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 8.3277e-04 - acc: 1.0428e-04 - val_loss: 0.0015 - val_acc: 2.4325e-04\n",
      "Epoch 39/90\n",
      "9590/9590 [==============================] - 0s 45us/step - loss: 8.3311e-04 - acc: 1.0428e-04 - val_loss: 0.0011 - val_acc: 2.4325e-04\n",
      "Epoch 40/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 8.5679e-04 - acc: 1.0428e-04 - val_loss: 9.7590e-04 - val_acc: 2.4325e-04\n",
      "Epoch 41/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 8.6287e-04 - acc: 1.0428e-04 - val_loss: 0.0016 - val_acc: 2.4325e-04\n",
      "Epoch 42/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 8.1721e-04 - acc: 1.0428e-04 - val_loss: 0.0016 - val_acc: 2.4325e-04\n",
      "Epoch 43/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 8.5011e-04 - acc: 1.0428e-04 - val_loss: 0.0016 - val_acc: 2.4325e-04\n",
      "Epoch 44/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 7.6445e-04 - acc: 1.0428e-04 - val_loss: 0.0014 - val_acc: 2.4325e-04\n",
      "Epoch 45/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 7.9889e-04 - acc: 1.0428e-04 - val_loss: 0.0013 - val_acc: 2.4325e-04\n",
      "Epoch 46/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 8.4786e-04 - acc: 1.0428e-04 - val_loss: 8.8151e-04 - val_acc: 2.4325e-04\n",
      "Epoch 47/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 8.0266e-04 - acc: 1.0428e-04 - val_loss: 0.0018 - val_acc: 2.4325e-04\n",
      "Epoch 48/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 7.5288e-04 - acc: 1.0428e-04 - val_loss: 0.0012 - val_acc: 2.4325e-04\n",
      "Epoch 49/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 8.1466e-04 - acc: 1.0428e-04 - val_loss: 0.0011 - val_acc: 2.4325e-04\n",
      "Epoch 50/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 7.7836e-04 - acc: 1.0428e-04 - val_loss: 0.0014 - val_acc: 2.4325e-04\n",
      "Epoch 51/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 7.3420e-04 - acc: 1.0428e-04 - val_loss: 0.0019 - val_acc: 2.4325e-04\n",
      "Epoch 52/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 7.4393e-04 - acc: 1.0428e-04 - val_loss: 0.0019 - val_acc: 2.4325e-04\n",
      "Epoch 53/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 7.4772e-04 - acc: 1.0428e-04 - val_loss: 0.0010 - val_acc: 2.4325e-04\n",
      "Epoch 54/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 7.6934e-04 - acc: 1.0428e-04 - val_loss: 0.0024 - val_acc: 2.4325e-04\n",
      "Epoch 55/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 7.7122e-04 - acc: 1.0428e-04 - val_loss: 0.0018 - val_acc: 2.4325e-04\n",
      "Epoch 56/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 7.6765e-04 - acc: 1.0428e-04 - val_loss: 0.0017 - val_acc: 2.4325e-04\n",
      "Epoch 57/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 7.2473e-04 - acc: 1.0428e-04 - val_loss: 0.0014 - val_acc: 2.4325e-04\n",
      "Epoch 58/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 7.3553e-04 - acc: 1.0428e-04 - val_loss: 0.0023 - val_acc: 2.4325e-04\n",
      "Epoch 59/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 7.2848e-04 - acc: 1.0428e-04 - val_loss: 0.0020 - val_acc: 2.4325e-04\n",
      "Epoch 60/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 7.2562e-04 - acc: 1.0428e-04 - val_loss: 0.0010 - val_acc: 2.4325e-04\n",
      "Epoch 61/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 7.1195e-04 - acc: 1.0428e-04 - val_loss: 0.0012 - val_acc: 2.4325e-04\n",
      "Epoch 62/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 7.0646e-04 - acc: 1.0428e-04 - val_loss: 0.0011 - val_acc: 2.4325e-04\n",
      "Epoch 63/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 7.7704e-04 - acc: 1.0428e-04 - val_loss: 8.5865e-04 - val_acc: 2.4325e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 7.0696e-04 - acc: 1.0428e-04 - val_loss: 0.0026 - val_acc: 2.4325e-04\n",
      "Epoch 65/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 7.1339e-04 - acc: 1.0428e-04 - val_loss: 0.0015 - val_acc: 2.4325e-04\n",
      "Epoch 66/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 7.4107e-04 - acc: 1.0428e-04 - val_loss: 9.7140e-04 - val_acc: 2.4325e-04\n",
      "Epoch 67/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 6.8559e-04 - acc: 1.0428e-04 - val_loss: 6.2884e-04 - val_acc: 2.4325e-04\n",
      "Epoch 68/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 7.7865e-04 - acc: 1.0428e-04 - val_loss: 0.0010 - val_acc: 2.4325e-04\n",
      "Epoch 69/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 7.1674e-04 - acc: 1.0428e-04 - val_loss: 7.8564e-04 - val_acc: 2.4325e-04\n",
      "Epoch 70/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 7.0116e-04 - acc: 1.0428e-04 - val_loss: 0.0012 - val_acc: 2.4325e-04\n",
      "Epoch 71/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 6.5931e-04 - acc: 1.0428e-04 - val_loss: 0.0024 - val_acc: 2.4325e-04\n",
      "Epoch 72/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 6.7968e-04 - acc: 1.0428e-04 - val_loss: 0.0025 - val_acc: 2.4325e-04\n",
      "Epoch 73/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 6.3318e-04 - acc: 1.0428e-04 - val_loss: 0.0013 - val_acc: 2.4325e-04\n",
      "Epoch 74/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 6.4375e-04 - acc: 1.0428e-04 - val_loss: 0.0012 - val_acc: 2.4325e-04\n",
      "Epoch 75/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 6.5049e-04 - acc: 1.0428e-04 - val_loss: 0.0013 - val_acc: 2.4325e-04\n",
      "Epoch 76/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 6.3159e-04 - acc: 1.0428e-04 - val_loss: 9.3505e-04 - val_acc: 2.4325e-04\n",
      "Epoch 77/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 6.5090e-04 - acc: 1.0428e-04 - val_loss: 0.0013 - val_acc: 2.4325e-04\n",
      "Epoch 78/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 6.1465e-04 - acc: 1.0428e-04 - val_loss: 9.3717e-04 - val_acc: 2.4325e-04\n",
      "Epoch 79/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 6.1773e-04 - acc: 1.0428e-04 - val_loss: 0.0014 - val_acc: 2.4325e-04\n",
      "Epoch 80/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 6.5585e-04 - acc: 1.0428e-04 - val_loss: 0.0017 - val_acc: 2.4325e-04\n",
      "Epoch 81/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 6.0754e-04 - acc: 1.0428e-04 - val_loss: 9.6721e-04 - val_acc: 2.4325e-04\n",
      "Epoch 82/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 6.0473e-04 - acc: 1.0428e-04 - val_loss: 0.0017 - val_acc: 2.4325e-04\n",
      "Epoch 83/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 6.3574e-04 - acc: 1.0428e-04 - val_loss: 0.0014 - val_acc: 2.4325e-04\n",
      "Epoch 84/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 6.1253e-04 - acc: 1.0428e-04 - val_loss: 9.3808e-04 - val_acc: 2.4325e-04\n",
      "Epoch 85/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 5.7365e-04 - acc: 1.0428e-04 - val_loss: 9.1877e-04 - val_acc: 2.4325e-04\n",
      "Epoch 86/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 5.8317e-04 - acc: 1.0428e-04 - val_loss: 0.0014 - val_acc: 2.4325e-04\n",
      "Epoch 87/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 6.0395e-04 - acc: 1.0428e-04 - val_loss: 0.0011 - val_acc: 2.4325e-04\n",
      "Epoch 88/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 5.6680e-04 - acc: 1.0428e-04 - val_loss: 8.6768e-04 - val_acc: 2.4325e-04\n",
      "Epoch 89/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 5.8977e-04 - acc: 1.0428e-04 - val_loss: 4.9357e-04 - val_acc: 2.4325e-04\n",
      "Epoch 90/90\n",
      "9590/9590 [==============================] - 0s 46us/step - loss: 5.8550e-04 - acc: 1.0428e-04 - val_loss: 8.5894e-04 - val_acc: 2.4325e-04\n",
      "-52.21782355951732\n",
      "Amount of features = 9\n",
      "Amount of training data = 13724\n",
      "Amount of testing data = 5882\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_61 (Conv1D)           (None, 21, 64)            1216      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 10, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 9, 64)             8256      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 4, 64)             0         \n",
      "_________________________________________________________________\n",
      "dropout_61 (Dropout)         (None, 4, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_31 (Flatten)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 250)               64250     \n",
      "_________________________________________________________________\n",
      "dropout_62 (Dropout)         (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 1)                 251       \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 73,973\n",
      "Trainable params: 73,973\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 9590 samples, validate on 4111 samples\n",
      "Epoch 1/90\n",
      "9590/9590 [==============================] - 3s 308us/step - loss: 0.0221 - acc: 1.0428e-04 - val_loss: 0.0048 - val_acc: 2.4325e-04\n",
      "Epoch 2/90\n",
      "9590/9590 [==============================] - 1s 54us/step - loss: 0.0057 - acc: 1.0428e-04 - val_loss: 0.0017 - val_acc: 2.4325e-04\n",
      "Epoch 3/90\n",
      "9590/9590 [==============================] - 1s 57us/step - loss: 0.0040 - acc: 1.0428e-04 - val_loss: 0.0018 - val_acc: 2.4325e-04\n",
      "Epoch 4/90\n",
      "9590/9590 [==============================] - 1s 62us/step - loss: 0.0031 - acc: 1.0428e-04 - val_loss: 0.0011 - val_acc: 2.4325e-04\n",
      "Epoch 5/90\n",
      "9590/9590 [==============================] - 1s 56us/step - loss: 0.0024 - acc: 1.0428e-04 - val_loss: 0.0013 - val_acc: 2.4325e-04\n",
      "Epoch 6/90\n",
      "9590/9590 [==============================] - 1s 57us/step - loss: 0.0022 - acc: 1.0428e-04 - val_loss: 0.0013 - val_acc: 2.4325e-04\n",
      "Epoch 7/90\n",
      "9590/9590 [==============================] - 1s 59us/step - loss: 0.0019 - acc: 1.0428e-04 - val_loss: 0.0013 - val_acc: 2.4325e-04\n",
      "Epoch 8/90\n",
      "9590/9590 [==============================] - 1s 59us/step - loss: 0.0018 - acc: 1.0428e-04 - val_loss: 0.0023 - val_acc: 2.4325e-04\n",
      "Epoch 9/90\n",
      "9590/9590 [==============================] - 1s 53us/step - loss: 0.0017 - acc: 1.0428e-04 - val_loss: 0.0014 - val_acc: 2.4325e-04\n",
      "Epoch 10/90\n",
      "9590/9590 [==============================] - 0s 50us/step - loss: 0.0017 - acc: 1.0428e-04 - val_loss: 8.3417e-04 - val_acc: 2.4325e-04\n",
      "Epoch 11/90\n",
      "9590/9590 [==============================] - 0s 50us/step - loss: 0.0015 - acc: 1.0428e-04 - val_loss: 0.0015 - val_acc: 2.4325e-04\n",
      "Epoch 12/90\n",
      "9590/9590 [==============================] - 0s 49us/step - loss: 0.0015 - acc: 1.0428e-04 - val_loss: 0.0026 - val_acc: 2.4325e-04\n",
      "Epoch 13/90\n",
      "9590/9590 [==============================] - 0s 49us/step - loss: 0.0015 - acc: 1.0428e-04 - val_loss: 4.5870e-04 - val_acc: 2.4325e-04\n",
      "Epoch 14/90\n",
      "9590/9590 [==============================] - 0s 51us/step - loss: 0.0015 - acc: 1.0428e-04 - val_loss: 0.0015 - val_acc: 2.4325e-04\n",
      "Epoch 15/90\n",
      "9590/9590 [==============================] - 0s 49us/step - loss: 0.0013 - acc: 1.0428e-04 - val_loss: 0.0017 - val_acc: 2.4325e-04\n",
      "Epoch 16/90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9590/9590 [==============================] - 0s 47us/step - loss: 0.0013 - acc: 1.0428e-04 - val_loss: 0.0010 - val_acc: 2.4325e-04\n",
      "Epoch 17/90\n",
      "9590/9590 [==============================] - 0s 47us/step - loss: 0.0013 - acc: 1.0428e-04 - val_loss: 0.0018 - val_acc: 2.4325e-04\n",
      "Epoch 18/90\n",
      "9590/9590 [==============================] - 0s 48us/step - loss: 0.0012 - acc: 1.0428e-04 - val_loss: 0.0021 - val_acc: 2.4325e-04\n",
      "Epoch 19/90\n",
      "9590/9590 [==============================] - 0s 48us/step - loss: 0.0012 - acc: 1.0428e-04 - val_loss: 0.0015 - val_acc: 2.4325e-04\n",
      "Epoch 20/90\n",
      "9590/9590 [==============================] - 0s 47us/step - loss: 0.0012 - acc: 1.0428e-04 - val_loss: 0.0016 - val_acc: 2.4325e-04\n",
      "Epoch 21/90\n",
      "9590/9590 [==============================] - 0s 47us/step - loss: 0.0011 - acc: 1.0428e-04 - val_loss: 0.0012 - val_acc: 2.4325e-04\n",
      "Epoch 22/90\n",
      "9590/9590 [==============================] - 0s 46us/step - loss: 0.0011 - acc: 1.0428e-04 - val_loss: 0.0013 - val_acc: 2.4325e-04\n",
      "Epoch 23/90\n",
      "9590/9590 [==============================] - 0s 47us/step - loss: 0.0011 - acc: 1.0428e-04 - val_loss: 0.0028 - val_acc: 2.4325e-04\n",
      "Epoch 24/90\n",
      "9590/9590 [==============================] - 0s 50us/step - loss: 0.0011 - acc: 1.0428e-04 - val_loss: 0.0017 - val_acc: 2.4325e-04\n",
      "Epoch 25/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0011 - acc: 1.0428e-04 - val_loss: 0.0015 - val_acc: 2.4325e-04\n",
      "Epoch 26/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 0.0010 - acc: 1.0428e-04 - val_loss: 0.0013 - val_acc: 2.4325e-04\n",
      "Epoch 27/90\n",
      "9590/9590 [==============================] - 0s 46us/step - loss: 0.0011 - acc: 1.0428e-04 - val_loss: 0.0013 - val_acc: 2.4325e-04\n",
      "Epoch 28/90\n",
      "9590/9590 [==============================] - 0s 46us/step - loss: 0.0011 - acc: 1.0428e-04 - val_loss: 0.0019 - val_acc: 2.4325e-04\n",
      "Epoch 29/90\n",
      "9590/9590 [==============================] - 0s 47us/step - loss: 0.0011 - acc: 1.0428e-04 - val_loss: 0.0013 - val_acc: 2.4325e-04\n",
      "Epoch 30/90\n",
      "9590/9590 [==============================] - 0s 48us/step - loss: 0.0011 - acc: 1.0428e-04 - val_loss: 0.0019 - val_acc: 2.4325e-04\n",
      "Epoch 31/90\n",
      "9590/9590 [==============================] - 0s 45us/step - loss: 0.0011 - acc: 1.0428e-04 - val_loss: 8.1057e-04 - val_acc: 2.4325e-04\n",
      "Epoch 32/90\n",
      "9590/9590 [==============================] - 0s 45us/step - loss: 0.0011 - acc: 1.0428e-04 - val_loss: 0.0016 - val_acc: 2.4325e-04\n",
      "Epoch 33/90\n",
      "9590/9590 [==============================] - 0s 45us/step - loss: 0.0010 - acc: 1.0428e-04 - val_loss: 0.0015 - val_acc: 2.4325e-04\n",
      "Epoch 34/90\n",
      "9590/9590 [==============================] - 0s 45us/step - loss: 9.9140e-04 - acc: 1.0428e-04 - val_loss: 0.0020 - val_acc: 2.4325e-04\n",
      "Epoch 35/90\n",
      "9590/9590 [==============================] - 0s 46us/step - loss: 9.7555e-04 - acc: 1.0428e-04 - val_loss: 0.0011 - val_acc: 2.4325e-04\n",
      "Epoch 36/90\n",
      "9590/9590 [==============================] - 0s 46us/step - loss: 9.6667e-04 - acc: 1.0428e-04 - val_loss: 0.0014 - val_acc: 2.4325e-04\n",
      "Epoch 37/90\n",
      "9590/9590 [==============================] - 0s 48us/step - loss: 9.6286e-04 - acc: 1.0428e-04 - val_loss: 0.0011 - val_acc: 2.4325e-04\n",
      "Epoch 38/90\n",
      "9590/9590 [==============================] - 0s 46us/step - loss: 9.2880e-04 - acc: 1.0428e-04 - val_loss: 0.0011 - val_acc: 2.4325e-04\n",
      "Epoch 39/90\n",
      "9590/9590 [==============================] - 0s 47us/step - loss: 9.5116e-04 - acc: 1.0428e-04 - val_loss: 8.3057e-04 - val_acc: 2.4325e-04\n",
      "Epoch 40/90\n",
      "9590/9590 [==============================] - 0s 46us/step - loss: 9.4089e-04 - acc: 1.0428e-04 - val_loss: 0.0018 - val_acc: 2.4325e-04\n",
      "Epoch 41/90\n",
      "9590/9590 [==============================] - 0s 48us/step - loss: 9.4573e-04 - acc: 1.0428e-04 - val_loss: 0.0015 - val_acc: 2.4325e-04\n",
      "Epoch 42/90\n",
      "9590/9590 [==============================] - 0s 46us/step - loss: 9.4395e-04 - acc: 1.0428e-04 - val_loss: 0.0012 - val_acc: 2.4325e-04\n",
      "Epoch 43/90\n",
      "9590/9590 [==============================] - 0s 47us/step - loss: 9.1562e-04 - acc: 1.0428e-04 - val_loss: 0.0015 - val_acc: 2.4325e-04\n",
      "Epoch 44/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 8.8386e-04 - acc: 1.0428e-04 - val_loss: 7.1441e-04 - val_acc: 2.4325e-04\n",
      "Epoch 45/90\n",
      "9590/9590 [==============================] - 0s 46us/step - loss: 9.1768e-04 - acc: 1.0428e-04 - val_loss: 0.0014 - val_acc: 2.4325e-04\n",
      "Epoch 46/90\n",
      "9590/9590 [==============================] - 0s 45us/step - loss: 8.7449e-04 - acc: 1.0428e-04 - val_loss: 8.4164e-04 - val_acc: 2.4325e-04\n",
      "Epoch 47/90\n",
      "9590/9590 [==============================] - 0s 47us/step - loss: 8.6385e-04 - acc: 1.0428e-04 - val_loss: 0.0013 - val_acc: 2.4325e-04\n",
      "Epoch 48/90\n",
      "9590/9590 [==============================] - 0s 45us/step - loss: 8.8619e-04 - acc: 1.0428e-04 - val_loss: 0.0010 - val_acc: 2.4325e-04\n",
      "Epoch 49/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 8.7273e-04 - acc: 1.0428e-04 - val_loss: 4.8737e-04 - val_acc: 2.4325e-04\n",
      "Epoch 50/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 0.0010 - acc: 1.0428e-04 - val_loss: 6.0326e-04 - val_acc: 2.4325e-04\n",
      "Epoch 51/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 8.5430e-04 - acc: 1.0428e-04 - val_loss: 4.6888e-04 - val_acc: 2.4325e-04\n",
      "Epoch 52/90\n",
      "9590/9590 [==============================] - 0s 49us/step - loss: 8.3729e-04 - acc: 1.0428e-04 - val_loss: 5.0279e-04 - val_acc: 2.4325e-04\n",
      "Epoch 53/90\n",
      "9590/9590 [==============================] - 0s 49us/step - loss: 8.7001e-04 - acc: 1.0428e-04 - val_loss: 9.4936e-04 - val_acc: 2.4325e-04\n",
      "Epoch 54/90\n",
      "9590/9590 [==============================] - 1s 54us/step - loss: 8.9458e-04 - acc: 1.0428e-04 - val_loss: 5.7304e-04 - val_acc: 2.4325e-04\n",
      "Epoch 55/90\n",
      "9590/9590 [==============================] - 1s 58us/step - loss: 8.5305e-04 - acc: 1.0428e-04 - val_loss: 5.6862e-04 - val_acc: 2.4325e-04\n",
      "Epoch 56/90\n",
      "9590/9590 [==============================] - 1s 54us/step - loss: 8.7611e-04 - acc: 1.0428e-04 - val_loss: 5.5079e-04 - val_acc: 2.4325e-04\n",
      "Epoch 57/90\n",
      "9590/9590 [==============================] - 1s 55us/step - loss: 8.4796e-04 - acc: 1.0428e-04 - val_loss: 8.9971e-04 - val_acc: 2.4325e-04\n",
      "Epoch 58/90\n",
      "9590/9590 [==============================] - 1s 58us/step - loss: 8.3943e-04 - acc: 1.0428e-04 - val_loss: 6.6235e-04 - val_acc: 2.4325e-04\n",
      "Epoch 59/90\n",
      "9590/9590 [==============================] - 1s 55us/step - loss: 7.9080e-04 - acc: 1.0428e-04 - val_loss: 4.5015e-04 - val_acc: 2.4325e-04\n",
      "Epoch 60/90\n",
      "9590/9590 [==============================] - 1s 61us/step - loss: 8.1382e-04 - acc: 1.0428e-04 - val_loss: 0.0011 - val_acc: 2.4325e-04\n",
      "Epoch 61/90\n",
      "9590/9590 [==============================] - 1s 58us/step - loss: 8.2062e-04 - acc: 1.0428e-04 - val_loss: 6.7153e-04 - val_acc: 2.4325e-04\n",
      "Epoch 62/90\n",
      "9590/9590 [==============================] - 1s 53us/step - loss: 8.4672e-04 - acc: 1.0428e-04 - val_loss: 4.6658e-04 - val_acc: 2.4325e-04\n",
      "Epoch 63/90\n",
      "9590/9590 [==============================] - 0s 50us/step - loss: 7.9382e-04 - acc: 1.0428e-04 - val_loss: 4.5222e-04 - val_acc: 2.4325e-04\n",
      "Epoch 64/90\n",
      "9590/9590 [==============================] - 0s 49us/step - loss: 7.9166e-04 - acc: 1.0428e-04 - val_loss: 4.4528e-04 - val_acc: 2.4325e-04\n",
      "Epoch 65/90\n",
      "9590/9590 [==============================] - 0s 48us/step - loss: 8.3466e-04 - acc: 1.0428e-04 - val_loss: 6.5253e-04 - val_acc: 2.4325e-04\n",
      "Epoch 66/90\n",
      "9590/9590 [==============================] - 0s 47us/step - loss: 8.4690e-04 - acc: 1.0428e-04 - val_loss: 7.1750e-04 - val_acc: 2.4325e-04\n",
      "Epoch 67/90\n",
      "9590/9590 [==============================] - 0s 47us/step - loss: 7.7280e-04 - acc: 1.0428e-04 - val_loss: 4.3944e-04 - val_acc: 2.4325e-04\n",
      "Epoch 68/90\n",
      "9590/9590 [==============================] - 1s 57us/step - loss: 7.7922e-04 - acc: 1.0428e-04 - val_loss: 7.1890e-04 - val_acc: 2.4325e-04\n",
      "Epoch 69/90\n",
      "9590/9590 [==============================] - 0s 51us/step - loss: 7.7346e-04 - acc: 1.0428e-04 - val_loss: 6.1565e-04 - val_acc: 2.4325e-04\n",
      "Epoch 70/90\n",
      "9590/9590 [==============================] - 0s 48us/step - loss: 8.0738e-04 - acc: 1.0428e-04 - val_loss: 5.0437e-04 - val_acc: 2.4325e-04\n",
      "Epoch 71/90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9590/9590 [==============================] - 0s 44us/step - loss: 7.3299e-04 - acc: 1.0428e-04 - val_loss: 5.4836e-04 - val_acc: 2.4325e-04\n",
      "Epoch 72/90\n",
      "9590/9590 [==============================] - 0s 45us/step - loss: 7.1764e-04 - acc: 1.0428e-04 - val_loss: 7.9305e-04 - val_acc: 2.4325e-04\n",
      "Epoch 73/90\n",
      "9590/9590 [==============================] - 0s 45us/step - loss: 7.2407e-04 - acc: 1.0428e-04 - val_loss: 5.2437e-04 - val_acc: 2.4325e-04\n",
      "Epoch 74/90\n",
      "9590/9590 [==============================] - 0s 47us/step - loss: 7.7271e-04 - acc: 1.0428e-04 - val_loss: 4.7028e-04 - val_acc: 2.4325e-04\n",
      "Epoch 75/90\n",
      "9590/9590 [==============================] - 0s 47us/step - loss: 7.5136e-04 - acc: 1.0428e-04 - val_loss: 4.5790e-04 - val_acc: 2.4325e-04\n",
      "Epoch 76/90\n",
      "9590/9590 [==============================] - 0s 46us/step - loss: 7.3353e-04 - acc: 1.0428e-04 - val_loss: 4.8940e-04 - val_acc: 2.4325e-04\n",
      "Epoch 77/90\n",
      "9590/9590 [==============================] - 0s 46us/step - loss: 7.0677e-04 - acc: 1.0428e-04 - val_loss: 4.2076e-04 - val_acc: 2.4325e-04\n",
      "Epoch 78/90\n",
      "9590/9590 [==============================] - 0s 45us/step - loss: 7.1221e-04 - acc: 1.0428e-04 - val_loss: 4.0157e-04 - val_acc: 2.4325e-04\n",
      "Epoch 79/90\n",
      "9590/9590 [==============================] - 0s 50us/step - loss: 6.7821e-04 - acc: 1.0428e-04 - val_loss: 4.7560e-04 - val_acc: 2.4325e-04\n",
      "Epoch 80/90\n",
      "9590/9590 [==============================] - 0s 49us/step - loss: 6.9301e-04 - acc: 1.0428e-04 - val_loss: 3.5688e-04 - val_acc: 2.4325e-04\n",
      "Epoch 81/90\n",
      "9590/9590 [==============================] - 0s 45us/step - loss: 7.1434e-04 - acc: 1.0428e-04 - val_loss: 4.8518e-04 - val_acc: 2.4325e-04\n",
      "Epoch 82/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 6.9127e-04 - acc: 1.0428e-04 - val_loss: 4.3042e-04 - val_acc: 2.4325e-04\n",
      "Epoch 83/90\n",
      "9590/9590 [==============================] - 0s 45us/step - loss: 6.9756e-04 - acc: 1.0428e-04 - val_loss: 3.5228e-04 - val_acc: 2.4325e-04\n",
      "Epoch 84/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 6.5369e-04 - acc: 1.0428e-04 - val_loss: 5.5573e-04 - val_acc: 2.4325e-04\n",
      "Epoch 85/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 6.9819e-04 - acc: 1.0428e-04 - val_loss: 3.5925e-04 - val_acc: 2.4325e-04\n",
      "Epoch 86/90\n",
      "9590/9590 [==============================] - 0s 43us/step - loss: 6.7203e-04 - acc: 1.0428e-04 - val_loss: 3.5171e-04 - val_acc: 2.4325e-04\n",
      "Epoch 87/90\n",
      "9590/9590 [==============================] - 0s 42us/step - loss: 6.6817e-04 - acc: 1.0428e-04 - val_loss: 4.9946e-04 - val_acc: 2.4325e-04\n",
      "Epoch 88/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 6.8082e-04 - acc: 1.0428e-04 - val_loss: 3.7165e-04 - val_acc: 2.4325e-04\n",
      "Epoch 89/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 6.4417e-04 - acc: 1.0428e-04 - val_loss: 4.4958e-04 - val_acc: 2.4325e-04\n",
      "Epoch 90/90\n",
      "9590/9590 [==============================] - 0s 44us/step - loss: 6.9873e-04 - acc: 1.0428e-04 - val_loss: 8.9931e-04 - val_acc: 2.4325e-04\n",
      "-54.444766688121646\n"
     ]
    }
   ],
   "source": [
    "currency_list=[ 'GBP Curncy',\n",
    " 'JPY Curncy',\n",
    " 'EUR Curncy',\n",
    " 'CAD Curncy',\n",
    " 'NZD Curncy',\n",
    " 'SEK Curncy',\n",
    " 'AUD Curncy',\n",
    " 'CHF Curncy',\n",
    " 'NOK Curncy',\n",
    " 'ZAR Curncy']\n",
    "#currency_list=['JPY Curncy']\n",
    "predictcur=portfolio(currency_list,file = 'FX-5-merg.xlsx',seq_len = 22,shape = [seq_len, 9, 1],neurons = [256, 256, 32, 1],dropout = 0.3,decay = 0.5,\n",
    "              epochs = 90,ma=[50, 100, 200],split=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13701, 3)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictcur.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Description:\n",
    "        Extends the basic Markowitz model with a market cost term.\n",
    "\n",
    "    Input:\n",
    "        n: Number of assets\n",
    "        mu: An n dimensional vector of expected returns\n",
    "        GT: A matrix with n columns so (GT')*GT  = covariance matrix\n",
    "        x0: Initial holdings \n",
    "        w: Initial cash holding\n",
    "        gamma: Maximum risk (=std. dev) accepted\n",
    "        f: If asset j is traded then a fixed cost f_j must be paid\n",
    "        g: If asset j is traded then a cost g_j must be paid for each unit traded\n",
    "\n",
    "    Output:\n",
    "       Optimal expected return and the optimal portfolio     \n",
    "\n",
    "\"\"\"\n",
    "def MarkowitzWithTransactionsCost(n,mu,GT,x0,w,gamma,f,g):\n",
    "    # Upper bound on the traded amount\n",
    "    w0 = w+sum(x0)\n",
    "    u = n*[w0]\n",
    "\n",
    "    with Model(\"Markowitz portfolio with transaction costs\") as M:\n",
    "        #M.setLogHandler(sys.stdout)\n",
    "\n",
    "        # Defines the variables. No shortselling is allowed.\n",
    "        x = M.variable(\"x\", n, Domain.greaterThan(0.0))\n",
    "\n",
    "        # Additional \"helper\" variables \n",
    "        z = M.variable(\"z\", n, Domain.unbounded())   \n",
    "        # Binary variables\n",
    "        y = M.variable(\"y\", n, Domain.binary())\n",
    "\n",
    "        #  Maximize expected return\n",
    "        M.objective('obj', ObjectiveSense.Maximize, Expr.dot(mu,x))\n",
    "\n",
    "        # Invest amount + transactions costs = initial wealth\n",
    "        M.constraint('budget', Expr.add([ Expr.sum(x), Expr.dot(f,y),Expr.dot(g,z)] ), Domain.equalsTo(w0))\n",
    "\n",
    "        # Imposes a bound on the risk\n",
    "        M.constraint('risk', Expr.vstack( gamma,Expr.mul(GT,x)), Domain.inQCone())\n",
    "\n",
    "        # z >= |x-x0| \n",
    "        M.constraint('buy', Expr.sub(z,Expr.sub(x,x0)),Domain.greaterThan(0.0))\n",
    "        M.constraint('sell', Expr.sub(z,Expr.sub(x0,x)),Domain.greaterThan(0.0))\n",
    "        # Alternatively, formulate the two constraints as\n",
    "        #M.constraint('trade', Expr.hstack(z,Expr.sub(x,x0)), Domain.inQcone())\n",
    "\n",
    "        # Constraints for turning y off and on. z-diag(u)*y<=0 i.e. z_j <= u_j*y_j\n",
    "        M.constraint('y_on_off', Expr.sub(z,Expr.mulElm(u,y)), Domain.lessThan(0.0))\n",
    "\n",
    "        # Integer optimization problems can be very hard to solve so limiting the \n",
    "        # maximum amount of time is a valuable safe guard\n",
    "        M.setSolverParam('mioMaxTime', 1000.0) \n",
    "        M.solve()\n",
    "\n",
    "        #print(\"\\n-----------------------------------------------------------------------------------\");\n",
    "        #print('Markowitz portfolio optimization with transactions cost')\n",
    "        #print(\"-----------------------------------------------------------------------------------\\n\");\n",
    "        #print('Expected return: %.4e Std. deviation: %.4e Transactions cost: %.4e' % \\\n",
    "              #(np.dot(mu,x.level()),gamma,np.dot(f,y.level())+np.dot(g,z.level())))\n",
    "\n",
    "        return (np.dot(mu,x.level()), x.level())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebalance_y(n,previous_prices,x0,w,mu,gamma=1):\n",
    "    GT=np.cov(previous_prices)\n",
    "    f = n*[0.0001]\n",
    "    g = n*[0.005]\n",
    "    _,weights=MarkowitzWithTransactionsCost(n,mu,GT,x0,w,gamma,f,g)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "close = pd.ExcelFile('close.xlsx')\n",
    "dq=np.array(close.parse(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19806, 10)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qq=denormalize(stock_name, normalized_value,split=0.7,predict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.3465, 111.72  ,   1.1849])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dq[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1)\n",
    "prices = np.array([random.gauss(100, 1) for _ in range(11*300)])\n",
    "prices = np.reshape(prices,(11,300))\n",
    "predictions = np.array([random.gauss(100, 1) for _ in range(11*100)])\n",
    "predictions = np.reshape(predictions,(11,100))\n",
    "initial_weights = np.repeat(1/11,11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backtesting using rebalancing function for weights\n",
    "random.seed(1)\n",
    "prices = np.array([random.gauss(100, 1) for _ in range(11*300)])\n",
    "prices = np.reshape(prices,(11,300))\n",
    "predictions = np.array([random.gauss(100, 1) for _ in range(11*100)])\n",
    "predictions = np.reshape(predictions,(11,100))\n",
    "initial_weights = np.repeat(1/11,11)\n",
    "\n",
    "def log_diff(data):\n",
    "    return np.diff(np.log(data))\n",
    "\n",
    "t_prices = len(prices[1,:])\n",
    "t_predictions = len(predictions[1,:])\n",
    "length_past = t_prices - t_predictions\n",
    "prediction_return = []\n",
    "for k in range(t_predictions):\n",
    "    prediction_return.append(np.log(predictions[:,k]/prices[:,length_past+k]))\n",
    "\n",
    "\n",
    "def backtest(prices, predictions, initial_weights):\n",
    "    t_prices = len(prices[1,:])\n",
    "    t_predictions = len(predictions[1,:])\n",
    "    length_past = t_prices - t_predictions\n",
    "    returns = np.apply_along_axis(log_diff, 1, prices)\n",
    "    prediction_return = []\n",
    "    for k in range(t_predictions):\n",
    "        prediction_return.append(np.log(predictions[:,k]/prices[:,length_past+k]))\n",
    "    prediction_return = np.asarray(prediction_return).T    \n",
    "    weights = initial_weights\n",
    "    portfolio_return = []\n",
    "    for i in range(0,t_predictions-1):\n",
    "        predicted_price = predictions[:,i]\n",
    "        previous_price = prices[:,length_past+i]\n",
    "        previous_prices = prices[:,0:length_past+i]\n",
    "        prev_weight = weights\n",
    "        new_weight = rebalance_y(3,previous_prices,mu=predicted_price,x0=prev_weight,w=1,gamma=0.5)\n",
    "        period_return = np.log((new_weight*prices[:,length_past+i+1])/(prev_weight*prices[:,length_past+i]))\n",
    "        portfolio_return.append(np.sum(period_return))\n",
    "        prev_weight = new_weight\n",
    "    return portfolio_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_diff(data):\n",
    "    return np.diff(np.log(data))\n",
    "def backtest(prices, predictions, initial_weights):\n",
    "    t_prices = len(prices[1,:])\n",
    "    t_predictions = len(predictions[:,1])\n",
    "    length_past = t_prices - t_predictions\n",
    "    returns = np.apply_along_axis(log_diff, 1, prices)\n",
    "    prediction_return = []\n",
    "    port_return=1\n",
    "    for k in range(t_predictions):\n",
    "        prediction_return.append(np.log(predictions[k]/prices[:,length_past+k]))\n",
    "    weights = initial_weights\n",
    "    portfolio_return = []\n",
    "    prev_weight = weights\n",
    "    for i in range(0,74):\n",
    "        print(i)\n",
    "        predicted_return = prediction_return[i]\n",
    "        previous_return = returns[:,length_past+i]\n",
    "        previous_returns = returns[:,0:length_past+i]\n",
    "        if i==0:\n",
    "            new_weight = rebalance_y(10,previous_returns,mu=predicted_return.tolist(),x0=prev_weight,w=1,gamma=0.05)\n",
    "        else:\n",
    "            new_weight = rebalance_y(10,previous_returns,mu=predicted_return.tolist(),x0=prev_weight,w=np.sum(period_return),gamma=0.05)\n",
    "        period_return = new_weight*np.log(prices[:,length_past+i+1]/prices[:,length_past+i])\n",
    "        port_return=port_return*(1+np.sum(period_return))\n",
    "        portfolio_return.append(port_return)\n",
    "        prev_weight = new_weight\n",
    "        print(new_weight)\n",
    "    return portfolio_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[ 3.71182211e-13 -1.04976885e-13 -1.21993725e-14 -2.04769427e-13\n",
      "  1.25653269e-11 -1.67476715e-13  9.49000000e-01 -2.01524208e-13\n",
      " -2.01021840e-13 -1.98243411e-13]\n",
      "1\n",
      "[ 5.65633677e-11 -6.87018011e-12 -1.01526319e-11 -2.11804242e-11\n",
      "  2.14302627e-09 -1.75098054e-11  9.00550004e-01 -2.17524010e-11\n",
      " -2.24877917e-11 -2.27875807e-11]\n",
      "2\n",
      "[ 7.73867547e-13 -9.30071243e-14 -1.61991744e-13 -3.12441556e-13\n",
      "  7.20191189e-11 -2.38052834e-13  8.54522505e-01 -3.18516451e-13\n",
      " -3.31155793e-13 -3.35120903e-13]\n",
      "3\n",
      "[ 8.20688459e-10 -1.37552143e-10 -2.42036735e-10 -3.92855525e-10\n",
      "  2.67178916e-08 -2.84269428e-10  8.10900845e-01 -3.95068835e-10\n",
      " -4.16177555e-10 -4.20185242e-10]\n",
      "4\n",
      "[ 0.00000000e+00 -2.22217722e-09 -2.85530641e-09 -5.89183607e-09\n",
      "  0.00000000e+00 -4.11433920e-09  7.69257923e-01 -5.98432097e-09\n",
      " -6.09742920e-09  0.00000000e+00]\n",
      "5\n",
      "[ 2.07762508e-11 -8.97681519e-12 -1.08385935e-11 -1.71674218e-11\n",
      "  9.22505308e-10 -1.22860451e-11  7.29701018e-01 -1.72688091e-11\n",
      " -1.75368054e-11 -1.74469793e-11]\n",
      "6\n",
      "[ 3.40096422e-13 -1.80702360e-13 -1.80347417e-13 -3.18083907e-13\n",
      "  3.82561097e-11 -2.30765789e-13  6.92305122e-01 -3.18890961e-13\n",
      " -3.20212510e-13 -3.17797362e-13]\n",
      "7\n",
      "[ 2.30607150e-09 -9.44544873e-10 -1.39873904e-09 -1.97141747e-09\n",
      "  9.20843631e-08 -1.22790953e-09  6.56859343e-01 -1.98484761e-09\n",
      " -2.01566751e-09 -1.99984352e-09]\n",
      "8\n",
      "[ 0.00000000e+00  0.00000000e+00 -8.53529840e-09 -1.62268785e-08\n",
      "  0.00000000e+00 -1.10755355e-08  6.23260889e-01 -1.62752182e-08\n",
      " -1.62955138e-08 -1.61747331e-08]\n",
      "9\n",
      "[ 0.00000000e+00 -1.10568552e-09 -1.44098400e-09 -1.94562281e-09\n",
      "  0.00000000e+00 -1.23800622e-09  5.91098145e-01 -1.95394600e-09\n",
      " -1.94134305e-09 -1.92689540e-09]\n",
      "10\n",
      "[ 3.97944997e-10 -3.55508494e-10 -4.19245966e-10 -5.89554114e-10\n",
      "  4.28315897e-08 -3.81054291e-10  5.60543311e-01 -5.91965420e-10\n",
      " -5.82416691e-10 -5.74857677e-10]\n",
      "11\n",
      "[ 6.63145782e-15 -1.13391042e-14 -9.85980186e-15 -1.34384376e-14\n",
      "  2.60885627e-13 -1.14378222e-14  5.31584617e-01 -1.34384622e-14\n",
      " -1.32298596e-14 -1.31459315e-14]\n",
      "12\n",
      "[-1.56350148e-10 -2.60017936e-09 -2.25562187e-09 -2.26716548e-09\n",
      "  0.00000000e+00 -2.26722517e-09  5.03876152e-01 -2.27027860e-09\n",
      " -2.24998726e-09 -2.24110740e-09]\n",
      "13\n",
      "[ 4.05793663e-10 -6.66241901e-10 -5.80620571e-10 -9.86618294e-10\n",
      "  1.05242866e-08 -7.58470876e-10  4.75338724e-01 -9.87234119e-10\n",
      " -9.36256067e-10 -9.27911748e-10]\n",
      "14\n",
      "[ 8.13557619e-14 -1.43762006e-13 -6.03909537e-14 -1.17635952e-13\n",
      "  5.44055097e-12 -1.42769124e-13  4.50863336e-01 -1.18136666e-13\n",
      " -1.24134115e-13 -1.25636635e-13]\n",
      "15\n",
      "[ 0.00000000e+00 -4.43898222e-09  0.00000000e+00 -3.69893593e-09\n",
      "  0.00000000e+00 -4.39610981e-09  4.26989271e-01 -3.66386241e-09\n",
      " -3.88359366e-09 -3.92646851e-09]\n",
      "16\n",
      "[ 3.61314328e-14 -1.50101594e-13 -4.49850467e-14 -1.19506824e-13\n",
      "  2.85764545e-12 -1.48935392e-13  4.04639785e-01 -1.12838864e-13\n",
      " -1.22419302e-13 -1.28530006e-13]\n",
      "17\n",
      "[ 0.00000000e+00 -1.17724257e-09 -6.61189568e-10 -1.80201461e-09\n",
      "  0.00000000e+00 -1.58016721e-09  3.83308825e-01 -1.82540154e-09\n",
      " -1.80942014e-09 -1.74904087e-09]\n",
      "18\n",
      "[ 0.00000000e+00 -1.57837435e-09 -8.59051032e-10 -2.26232129e-09\n",
      "  0.00000000e+00 -2.01163357e-09  3.63096666e-01 -2.31381469e-09\n",
      " -2.27805885e-09 -2.20037135e-09]\n",
      "19\n",
      "[ 0.00000000e+00 -1.74344642e-08  0.00000000e+00 -2.00665855e-08\n",
      "  0.00000000e+00 -2.27324171e-08  3.43543742e-01 -1.93349844e-08\n",
      " -1.99269951e-08 -2.10601613e-08]\n",
      "20\n",
      "[ 2.97639810e-13 -3.80908527e-13 -1.86979590e-13 -4.67001093e-13\n",
      "  4.49280915e-12 -5.12988721e-13  3.25408664e-01 -4.60065081e-13\n",
      " -4.64485768e-13 -4.88418996e-13]\n",
      "21\n",
      "[ 8.09981036e-14 -8.25242892e-14 -5.30654112e-14 -1.02479027e-13\n",
      "  1.12608361e-12 -1.11706926e-13  3.08218205e-01 -1.02494324e-13\n",
      " -1.03094202e-13 -1.07971968e-13]\n",
      "22\n",
      "[ 5.66204239e-14 -5.81071183e-14 -4.63629201e-14 -7.51325223e-14\n",
      "  6.96808958e-13 -8.07878431e-14  2.91617885e-01 -7.40212812e-14\n",
      " -7.55402994e-14 -7.87029311e-14]\n",
      "23\n",
      "[ 0.00000000e+00 -1.84223088e-09 -9.99148143e-10 -2.57560095e-09\n",
      "  0.00000000e+00 -2.27929330e-09  2.76001768e-01 -2.56396828e-09\n",
      " -2.55746692e-09 -2.45341709e-09]\n",
      "24\n",
      "[ 2.14705709e-10 -1.23169655e-10 -1.49453784e-10 -1.71977429e-10\n",
      "  1.34510213e-09 -2.05124571e-10  2.61303504e-01 -1.74867468e-10\n",
      " -1.78525534e-10 -1.93978073e-10]\n",
      "25\n",
      "[ 2.68986833e-10 -1.68562166e-10 -1.73651221e-10 -2.44216376e-10\n",
      "  3.04890046e-09 -2.66983335e-10  2.47142042e-01 -2.53739477e-10\n",
      " -2.52758650e-10 -2.66536735e-10]\n",
      "26\n",
      "[ 1.30484728e-11 -6.67973402e-12 -4.47336099e-12 -1.00939925e-11\n",
      "  9.76874787e-11 -9.91273026e-12  2.33693762e-01 -1.01357753e-11\n",
      " -1.02179335e-11 -1.02791604e-11]\n",
      "27\n",
      "[ 3.60414611e-10 -1.62084262e-10 -1.51040986e-10 -2.60394138e-10\n",
      "  3.35115278e-09 -2.66949148e-10  2.21037902e-01 -2.66767784e-10\n",
      " -2.64331222e-10 -2.74431865e-10]\n",
      "28\n",
      "[ 0.00000000e+00 -9.97298724e-10 -1.04714696e-09 -1.65551979e-09\n",
      "  0.00000000e+00 -1.76200115e-09  2.09013710e-01 -1.66104498e-09\n",
      " -1.70471189e-09 -1.78385591e-09]\n",
      "29\n",
      "[ 0.00000000e+00 -1.66862718e-09 -1.79049515e-09 -2.86695559e-09\n",
      "  0.00000000e+00 -2.95043731e-09  1.97538611e-01 -2.90399861e-09\n",
      " -2.90709449e-09 -3.04639961e-09]\n",
      "30\n",
      "[ 0.00000000e+00 -3.82148831e-09 -4.58050551e-09 -6.65415885e-09\n",
      "  0.00000000e+00 -7.04503363e-09  1.86713129e-01 -6.84067199e-09\n",
      " -7.10745676e-09 -7.36480590e-09]\n",
      "31\n",
      "[ 0.00000000e+00 -2.44863251e-09 -2.83867494e-09 -4.35233788e-09\n",
      "  0.00000000e+00 -4.50936646e-09  1.76402396e-01 -4.56081637e-09\n",
      " -4.59007419e-09 -4.77679857e-09]\n",
      "32\n",
      "[ 0.00000000e+00 -2.93593768e-09 -3.66837695e-09 -5.72442006e-09\n",
      "  0.00000000e+00 -5.81297412e-09  1.66541872e-01 -5.89643348e-09\n",
      " -6.06531901e-09 -6.20519705e-09]\n",
      "33\n",
      "[ 0.00000000e+00 -3.25115253e-10 -3.77998087e-10 -6.11245683e-10\n",
      "  0.00000000e+00 -6.13703892e-10  1.57215836e-01 -6.29206113e-10\n",
      " -6.43679570e-10 -6.58475370e-10]\n",
      "34\n",
      "[ 0.00000000e+00 -1.09341633e-08 -9.91645415e-09 -1.97602331e-08\n",
      "  0.00000000e+00 -1.86441703e-08  1.48344767e-01 -2.01916700e-08\n",
      " -2.03810175e-08 -2.04421085e-08]\n",
      "35\n",
      "[ 0.00000000e+00 -1.01262456e-08 -8.52442613e-09 -1.76845585e-08\n",
      "  0.00000000e+00 -1.68750164e-08  1.39954432e-01 -1.80891935e-08\n",
      " -1.82769649e-08 -1.83556976e-08]\n",
      "36\n",
      "[ 0.00000000e+00 -8.83966014e-09 -7.25201697e-09 -1.51524796e-08\n",
      "  0.00000000e+00 -1.42601547e-08  1.31981818e-01 -1.53616338e-08\n",
      " -1.55342271e-08 -1.55475737e-08]\n",
      "37\n",
      "[ 1.29663907e-12 -1.72069466e-12 -1.41711366e-12 -2.82710420e-12\n",
      "  5.16394055e-11 -2.65412880e-12  1.24447543e-01 -2.86704214e-12\n",
      " -2.89388818e-12 -2.88983510e-12]\n",
      "38\n",
      "[ 1.47561109e-12 -1.08722481e-12 -7.59005266e-13 -1.67479612e-12\n",
      "  3.31823953e-11 -1.56746334e-12  1.17225169e-01 -1.68245107e-12\n",
      " -1.68474349e-12 -1.67605504e-12]\n",
      "39\n",
      "[ 0.00000000e+00 -5.58171821e-10 -3.36499407e-10 -8.19486378e-10\n",
      "  0.00000000e+00 -7.70128770e-10  1.10379663e-01 -8.20793417e-10\n",
      " -8.19369997e-10 -8.15259109e-10]\n",
      "40\n",
      "[ 0.00000000e+00 -1.06838892e-08 -8.68003517e-09 -1.85316219e-08\n",
      "  0.00000000e+00 -1.72143768e-08  1.03815513e-01 -1.86402665e-08\n",
      " -1.87075226e-08 -1.86203804e-08]\n",
      "41\n",
      "[ 0.00000000e+00 -1.01196616e-08 -1.09860284e-08 -1.85407922e-08\n",
      "  0.00000000e+00 -1.80669499e-08  9.76407553e-02 -1.89237727e-08\n",
      " -1.93384032e-08 -1.94388813e-08]\n",
      "42\n",
      "[ 0.00000000e+00 -1.43482059e-09 -1.32293301e-09 -2.32932572e-09\n",
      "  0.00000000e+00 -2.31649116e-09  9.17732291e-02 -2.34935029e-09\n",
      " -2.39311588e-09 -2.41087988e-09]\n",
      "43\n",
      "[ 8.08790501e-12 -8.70115285e-12 -8.12718350e-12 -1.61746552e-11\n",
      "  2.97185414e-10 -1.46302931e-11  8.61620119e-02 -1.62110933e-11\n",
      " -1.62655856e-11 -1.61715395e-11]\n",
      "44\n",
      "[ 0.00000000e+00 -5.91225412e-09 -6.11214737e-09 -1.09475428e-08\n",
      "  0.00000000e+00 -1.02081476e-08  8.08751033e-02 -1.09657314e-08\n",
      " -1.11151778e-08 -1.11123095e-08]\n",
      "45\n",
      "[ 0.00000000e+00 -3.62062793e-09 -3.08780380e-09 -6.07478307e-09\n",
      "  0.00000000e+00 -5.61697083e-09  7.58461864e-02 -6.08125426e-09\n",
      " -6.10124708e-09 -6.07634626e-09]\n",
      "46\n",
      "[ 0.00000000e+00 -3.02778247e-09 -2.37160110e-09 -4.90342065e-09\n",
      "  0.00000000e+00 -4.49272999e-09  7.10575512e-02 -4.90350629e-09\n",
      " -4.89212182e-09 -4.86354655e-09]\n",
      "47\n",
      "[ 6.47015828e-12 -6.68669176e-12 -5.55609393e-12 -1.17016765e-11\n",
      "  1.96878510e-10 -1.03947981e-11  6.64871901e-02 -1.17013011e-11\n",
      " -1.16277581e-11 -1.15279432e-11]\n",
      "48\n",
      "[ 0.00000000e+00 -9.79306967e-09 -6.84790432e-09 -1.58255633e-08\n",
      "  0.00000000e+00 -1.40815815e-08  6.21615856e-02 -1.58128157e-08\n",
      " -1.55884929e-08 -1.54229218e-08]\n",
      "49\n",
      "[ 1.57447173e-12 -1.31846599e-12 -1.14247006e-12 -2.41035622e-12\n",
      "  4.86310802e-11 -2.06400147e-12  5.80457762e-02 -2.40269108e-12\n",
      " -2.34696448e-12 -2.31310296e-12]\n",
      "50\n",
      "[ 1.29707360e-11 -1.26853690e-11 -1.32841862e-11 -2.57714669e-11\n",
      "  4.08094990e-10 -2.12384335e-11  5.41006698e-02 -2.57080073e-11\n",
      " -2.50153442e-11 -2.45965279e-11]\n",
      "51\n",
      "[ 0.00000000e+00 -1.75614035e-09 -1.48167650e-09 -2.92816996e-09\n",
      "  0.00000000e+00 -2.44538850e-09  5.04043832e-02 -2.92041016e-09\n",
      " -2.83604737e-09 -2.78489493e-09]\n",
      "52\n",
      "[ 0.00000000e+00 -1.27341102e-09 -1.09417987e-09 -1.72491495e-09\n",
      "  0.00000000e+00 -1.34777944e-09  4.68914430e-02 -1.72168341e-09\n",
      " -1.62961680e-09 -1.58212675e-09]\n",
      "53\n",
      "[ 0.00000000e+00 -6.65581472e-10 -3.20146363e-10 -8.20719097e-10\n",
      "  0.00000000e+00 -7.55215440e-10  4.35358120e-02 -8.20401699e-10\n",
      " -7.94985167e-10 -7.86432763e-10]\n",
      "54\n",
      "[ 0.00000000e+00 -1.65984402e-10 -6.16003535e-11 -2.30828521e-10\n",
      "  0.00000000e+00 -2.04042038e-10  4.03591642e-02 -2.30850504e-10\n",
      " -2.22486853e-10 -2.19085500e-10]\n",
      "55\n",
      "[ 7.36096446e-11 -4.17961463e-11 -1.31004839e-11 -5.95053994e-11\n",
      "  5.73745782e-10 -5.33550553e-11  3.73462073e-02 -5.95583953e-11\n",
      " -5.77156656e-11 -5.70392401e-11]\n",
      "56\n",
      "[ 5.57266897e-11 -3.70352157e-11 -1.32021630e-11 -5.14074810e-11\n",
      "  7.11248271e-10 -4.58536609e-11  3.44789219e-02 -5.15649822e-11\n",
      " -4.96744964e-11 -4.90327830e-11]\n",
      "57\n",
      "[ 2.09456487e-11 -1.54827402e-11 -5.61541578e-12 -2.17028399e-11\n",
      "  2.56455325e-10 -1.94164764e-11  3.17592291e-02 -2.17738579e-11\n",
      " -2.09759421e-11 -2.07487333e-11]\n",
      "58\n",
      "[ 0.00000000e+00 -5.97880309e-11 -1.46366356e-11 -8.75449616e-11\n",
      "  0.00000000e+00 -7.82916066e-11  2.91673978e-02 -8.78067376e-11\n",
      " -8.46228324e-11 -8.36942325e-11]\n",
      "59\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.15498579e-07 -9.73314286e-09  8.74017719e-09 -2.82596738e-08\n",
      "  1.19626131e-06 -2.54882655e-08  2.67075090e-02 -2.83312658e-08\n",
      " -2.76360189e-08 -2.73725619e-08]\n",
      "60\n",
      "[ 0.00000000e+00 -1.57826558e-08 -8.62764994e-09 -2.20035368e-08\n",
      "  0.00000000e+00 -1.99738106e-08  2.43791647e-02 -2.20863435e-08\n",
      " -2.14357855e-08 -2.12459092e-08]\n",
      "61\n",
      "[ 2.09999907e-08 -4.49370725e-09  5.14460904e-10 -7.45492974e-09\n",
      "  7.18857389e-08 -6.64948930e-09  2.21610107e-02 -7.47725603e-09\n",
      " -7.26060802e-09 -7.18648958e-09]\n",
      "62\n",
      "[ 5.44866449e-10 -4.04034043e-10 -2.67618875e-10 -9.90210717e-10\n",
      "  1.90264081e-08 -8.45006265e-10  2.00587297e-02 -9.90464130e-10\n",
      " -9.74926043e-10 -9.63930022e-10]\n",
      "63\n",
      "[ 5.98509114e-11 -4.52957514e-11 -1.97926324e-11 -6.87359398e-11\n",
      "  1.42520079e-09 -6.14176515e-11  1.80558288e-02 -6.88414049e-11\n",
      " -6.66515364e-11 -6.59628671e-11]\n",
      "64\n",
      "[ 0.00000000e+00 -6.71020730e-11 -3.07643303e-11 -1.08336750e-10\n",
      "  0.00000000e+00 -9.59106873e-11  1.61508472e-02 -1.08468571e-10\n",
      " -1.04945370e-10 -1.03761067e-10]\n",
      "65\n",
      "[ 5.87207689e-11 -2.86681250e-11 -1.88600790e-11 -4.89511167e-11\n",
      "  1.38385086e-09 -4.24697255e-11  1.43453049e-02 -4.89596526e-11\n",
      " -4.71966261e-11 -4.66265448e-11]\n",
      "66\n",
      "[ 0.00000000e+00 -3.98596270e-09 -2.29969631e-09 -6.31283347e-09\n",
      "  0.00000000e+00 -5.60495676e-09  1.26315362e-02 -6.31699490e-09\n",
      " -6.14578140e-09 -6.08120109e-09]\n",
      "67\n",
      "[ 0.00000000e+00 -3.55807657e-09 -1.73039543e-09 -5.43895411e-09\n",
      "  0.00000000e+00 -4.75363075e-09  1.09999739e-02 -5.43652474e-09\n",
      " -5.24111108e-09 -5.17403459e-09]\n",
      "68\n",
      "[ 0.00000000e+00 -8.43423997e-09 -8.21199429e-09 -1.52733126e-08\n",
      "  0.00000000e+00 -1.29799094e-08  9.45425652e-03 -1.52773329e-08\n",
      " -1.49109126e-08 -1.47234253e-08]\n",
      "69\n",
      "[ 0.00000000e+00 -7.03746898e-09 -4.21434158e-09 -9.73177527e-09\n",
      "  0.00000000e+00 -8.70085747e-09  7.98293470e-03 -9.72325889e-09\n",
      " -9.40534843e-09 -9.29877925e-09]\n",
      "70\n",
      "[ 0.00000000e+00 -1.53447866e-09 -8.99920972e-10 -2.12792459e-09\n",
      "  0.00000000e+00 -1.88759061e-09  6.58547769e-03 -2.12738673e-09\n",
      " -2.05279001e-09 -2.02759750e-09]\n",
      "71\n",
      "[ 2.38535919e-08 -3.20815405e-09 -4.17926702e-09 -8.96791642e-09\n",
      "  5.34064056e-07 -6.03838319e-09  5.25506445e-03 -8.96446046e-09\n",
      " -8.34903994e-09 -8.07951282e-09]\n",
      "72\n",
      "[ 0.00000000e+00 -1.03870355e-09 -6.51708469e-10 -1.36496215e-09\n",
      "  0.00000000e+00 -1.23407798e-09  3.99325531e-03 -1.36360805e-09\n",
      " -1.32139319e-09 -1.30945953e-09]\n",
      "73\n",
      "[ 0.00000000e+00 -1.66264927e-09 -7.89944571e-10 -2.30038149e-09\n",
      "  0.00000000e+00 -2.03986873e-09  2.79551831e-03 -2.29715206e-09\n",
      " -2.20713054e-09 -2.18124958e-09]\n"
     ]
    }
   ],
   "source": [
    "x2=backtest(dq.T, predictcur, [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.4897466 , -5.97431917, -0.65146212, -2.35432795,  0.10476992,\n",
       "        -3.80640489,  0.19753378, -2.50924223, -4.80677026, -4.90781422])]"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_return=[]\n",
    "prediction_return.append(np.log(predictcur[2]/dq[150+2,:]))\n",
    "prediction_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.17056602e-08,  8.98216823e-01, -2.25649748e-09, -1.21618141e-08,\n",
       "        2.29623357e-08, -1.17976974e-08,  1.01283391e-01, -1.24138900e-08,\n",
       "       -1.26400060e-08, -1.29796907e-08])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rebalance_y(10,dq,mu=q,x0=[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],w=1,gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rebalance_y(10,previous_prices,mu=predicted_price.tolist(),x0=prev_weight,w=1,gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9971267771608727"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a43f229e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(x2)\n",
    "x2[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.27569218, -0.97371871, -0.6332695 , -2.23206723, -0.06454975,\n",
       "       -1.47317129, -0.05008349, -2.17911207, -2.08924851, -2.0318682 ])"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(predictcur[1]/dq.T[:,length_past+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.3165, 113.94  ,   1.1767,   1.2684,   0.6886,   8.2352,\n",
       "         0.7709,   0.9916,   8.0194,  13.727 ])"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dq.T[:,length_past+111]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.0345352 , 49.57716   ,  0.6513902 ,  0.11623273,  0.6592041 ,\n",
       "        1.8297249 ,  0.7369523 ,  0.09122836,  1.0056746 ,  1.7670125 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictcur[111]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes current_weights (n,1), previous_prices(n,t), next_price_pred(n,1) outputs updated_weighted (n,1)\n",
    "def rebalance(current_weights, previous_prices, next_price_prediction, transaction_cost=0.0005):\n",
    "    expected_return = np.log(next_price_prediction/previous_prices[:,-1])\n",
    "    sharpe = expected_return/previous_prices.std(axis=1)\n",
    "    w = sharpe/np.sum(sharpe)\n",
    "    delta_w = (w - current_weights)/current_weights\n",
    "    change = np.repeat(0,11)\n",
    "    for i in range(len(current_weights)):\n",
    "        if (delta_w[0,i-1]*(expected_return[0,i-1] - transaction_cost) > 0 ) :\n",
    "            change[i-1] = w[0,i-1]\n",
    "    total_change = (1 - np.sum(current_weights[change == 0]))\n",
    "    upd_weights = change * total_change/np.sum(change[change!=0])\n",
    "    upd_weights[upd_weights==0] = current_weights[upd_weights==0]\n",
    "    return upd_weights\n",
    "\n",
    "            \n",
    "            \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab\n",
    "\n",
    "import random\n",
    "\n",
    "random.seed()\n",
    "previous_prices = np.array([random.gauss(100, 1) for _ in range(11*100)])\n",
    "previous_prices = np.reshape(previous_prices,(11,100))\n",
    "#np.shape(previous_prices)\n",
    "#np.shape(previous_prices[:,-1])\n",
    "#len(current_weights)\n",
    "current_weights = np.repeat(1/11,11)\n",
    "#previous_prices = np.repeat(100,11)\n",
    "next_price_prediction = np.array([[95,96,97,98,99,100,101,102,103,104,105]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.003943449780939109"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([-1.00835304e-08, -1.41057215e-08, -6.31430920e-10, -8.91386841e-09,\n",
    "  1.68087230e-08, -1.39296899e-08,  3.94351517e-03, -6.27512976e-09,\n",
    " -1.40332767e-08, -1.42251363e-08])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9986004594120637"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([ 2.46827783e-09, -3.38387466e-09,  1.14274261e-09, -2.27042248e-09,\n",
    "  1.99860033e+00, -2.84756164e-09,  1.42514222e-07, -1.91787958e-09,\n",
    " -3.07848920e-09, -3.21495098e-09])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.72727273, -0.90909091, -1.36363636, -0.45454545,  0.09090909,\n",
       "        0.45454545,  0.90909091,  0.45454545,  0.90909091,  1.81818182,\n",
       "        1.81818182])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rebalance(current_weights, previous_prices, next_price_prediction, transaction_cost=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-14-13c3a2a4ffff>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-14-13c3a2a4ffff>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "def backtest(prices, predictions, initial_weights, length_past):\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'product' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-82c536c44287>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mproduct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'product' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result(stock_name, normalized_value_p, normalized_value_y_test):\n",
    "    newp = denormalize(stock_name, normalized_value_p,predict=True)\n",
    "    newy_test = denormalize(stock_name, normalized_value_y_test,predict=False)\n",
    "    plt2.plot(newp, color='red', label='Prediction')\n",
    "    plt2.plot(newy_test,color='blue', label='Actual')\n",
    "    plt2.legend(loc='best')\n",
    "    plt2.title('The test result for {}'.format(stock_name))\n",
    "    plt2.xlabel('5 Min ahead Forecast')\n",
    "    plt2.ylabel('Price')\n",
    "    plt2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a4400e400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_result(\"JPY Curncy\", p, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s=denormalize(\"GBP Curncy\", y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORKSPACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a31221a20>]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a35962ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt2.plot(s, color='red', label='Prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    stock=df_GBP\n",
    "    split=0.7\n",
    "    amount_of_features = len(stock.columns)\n",
    "    print (\"Amount of features = {}\".format(amount_of_features))\n",
    "    data = stock.as_matrix()\n",
    "    sequence_length = seq_len + 1 \n",
    "    result = []\n",
    "\n",
    "    for index in range(len(data) - sequence_length): \n",
    "        result.append(data[index: index + sequence_length]) \n",
    "\n",
    "    result = np.array(result)\n",
    "    row = round(split * result.shape[0]) \n",
    "    print (\"Amount of training data = {}\".format(split * result.shape[0]))\n",
    "    print (\"Amount of testing data = {}\".format((1-split) * result.shape[0]))\n",
    "\n",
    "    train = result[:int(row), :] \n",
    "    X_train = train[:, :-1] # all data until day m\n",
    "    y_train = train[:, -1][:,-1] # day m + 1 adjusted close price\n",
    "\n",
    "    X_test = result[int(row):, :-1]\n",
    "    y_test = result[int(row):, -1][:,-1]\n",
    "\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], amount_of_features))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], amount_of_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of features = 9\n"
     ]
    }
   ],
   "source": [
    "    stock=df_GBP\n",
    "    split=0.7\n",
    "    amount_of_features = len(stock.columns)\n",
    "    print (\"Amount of features = {}\".format(amount_of_features))\n",
    "    data = stock.as_matrix()\n",
    "    sequence_length = seq_len + 1 \n",
    "    result = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of training data = 13981.8\n",
      "Amount of testing data = 5992.200000000001\n"
     ]
    }
   ],
   "source": [
    "    for index in range(len(data) - sequence_length): \n",
    "        result.append(data[index: index + sequence_length]) \n",
    "\n",
    "    result = np.array(result)\n",
    "    row = round(split * result.shape[0]) \n",
    "    print (\"Amount of training data = {}\".format(split * result.shape[0]))\n",
    "    print (\"Amount of testing data = {}\".format((1-split) * result.shape[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    train = result[:int(row), :] \n",
    "    X_train = train[:, :-1] # all data until day m\n",
    "    y_train = train[:, -1][:,-1] # day m + 1 adjusted close price\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.78544061, 0.79961089, 0.78998073, 0.18487646, 0.73263144,\n",
       "        0.80178064, 0.85258554, 0.87962225, 0.79885057],\n",
       "       [0.79885057, 0.79766537, 0.77842004, 0.20749075, 0.6302588 ,\n",
       "        0.79992581, 0.85168128, 0.8792669 , 0.77777778],\n",
       "       [0.77777778, 0.78210117, 0.77842004, 0.26946511, 0.69849806,\n",
       "        0.79815341, 0.85088217, 0.87889001, 0.77969349],\n",
       "       [0.77969349, 0.79182879, 0.78034682, 0.19646395, 0.72694989,\n",
       "        0.79691686, 0.85020924, 0.87857774, 0.79118774],\n",
       "       [0.79118774, 0.78793774, 0.79190751, 0.14544163, 0.68143137,\n",
       "        0.79568031, 0.84941013, 0.87825469, 0.78735632],\n",
       "       [0.78735632, 0.79182879, 0.76300578, 0.23328225, 0.61885336,\n",
       "        0.79419645, 0.84833764, 0.87782396, 0.76245211],\n",
       "       [0.76245211, 0.75680934, 0.74566474, 0.26950249, 0.63586454,\n",
       "        0.79250649, 0.84694972, 0.87728555, 0.74329502],\n",
       "       [0.74329502, 0.75291829, 0.74759152, 0.17295257, 0.73269675,\n",
       "        0.7911875 , 0.84564591, 0.87682252, 0.75670498],\n",
       "       [0.75670498, 0.76070039, 0.75529865, 0.16708407, 0.71558934,\n",
       "        0.79036313, 0.84440519, 0.87640256, 0.76436782],\n",
       "       [0.76436782, 0.76070039, 0.75722543, 0.17867155, 0.69280719,\n",
       "        0.78945633, 0.84314344, 0.87591799, 0.76436782],\n",
       "       [0.76436782, 0.76653696, 0.76685934, 0.15766456, 0.69850103,\n",
       "        0.78879683, 0.84194478, 0.8755088 , 0.76628352],\n",
       "       [0.76628352, 0.76264591, 0.75722543, 0.15680484, 0.68142036,\n",
       "        0.78817856, 0.84074611, 0.87504576, 0.76245211],\n",
       "       [0.76245211, 0.75875486, 0.75337187, 0.23126378, 0.67003013,\n",
       "        0.78723053, 0.83940025, 0.87450735, 0.75478927],\n",
       "       [0.75478927, 0.75291829, 0.74373796, 0.23421672, 0.67002334,\n",
       "        0.78628251, 0.83788615, 0.8739151 , 0.74712644],\n",
       "       [0.74712644, 0.75486381, 0.74759152, 0.20154749, 0.73269081,\n",
       "        0.78582911, 0.83639308, 0.87338746, 0.7605364 ],\n",
       "       [0.7605364 , 0.76070039, 0.76493256, 0.15355287, 0.69280719,\n",
       "        0.78541692, 0.83496309, 0.87284905, 0.7605364 ],\n",
       "       [0.7605364 , 0.76264591, 0.761079  , 0.16263597, 0.70419657,\n",
       "        0.78566423, 0.83355414, 0.87233218, 0.76436782],\n",
       "       [0.76436782, 0.76264591, 0.76685934, 0.16446754, 0.70419487,\n",
       "        0.78578789, 0.83214519, 0.8718153 , 0.76819923],\n",
       "       [0.76819923, 0.76848249, 0.77071291, 0.18999738, 0.70419317,\n",
       "        0.78582911, 0.83050491, 0.87129843, 0.77203065],\n",
       "       [0.77203065, 0.77042802, 0.76493256, 0.16297238, 0.66434647,\n",
       "        0.78566423, 0.82875949, 0.87073848, 0.76245211],\n",
       "       [0.76245211, 0.75875486, 0.73795761, 0.23802938, 0.61308749,\n",
       "        0.78467499, 0.8266776 , 0.87004932, 0.73563218],\n",
       "       [0.73563218, 0.73929961, 0.73795761, 0.18760513, 0.71560802,\n",
       "        0.78397428, 0.82499527, 0.86933862, 0.74329502]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.74329502, 0.74329502, 0.74329502, ..., 0.96934866, 0.96743295,\n",
       "       0.95785441])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], amount_of_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.79501916, 0.79377432, 0.7822736 , 0.16065488, 0.66437187,\n",
       "        0.80309962, 0.85321641, 0.87984838, 0.78544061],\n",
       "       [0.78544061, 0.79961089, 0.78998073, 0.18487646, 0.73263144,\n",
       "        0.80178064, 0.85258554, 0.87962225, 0.79885057],\n",
       "       [0.79885057, 0.79766537, 0.77842004, 0.20749075, 0.6302588 ,\n",
       "        0.79992581, 0.85168128, 0.8792669 , 0.77777778],\n",
       "       [0.77777778, 0.78210117, 0.77842004, 0.26946511, 0.69849806,\n",
       "        0.79815341, 0.85088217, 0.87889001, 0.77969349],\n",
       "       [0.77969349, 0.79182879, 0.78034682, 0.19646395, 0.72694989,\n",
       "        0.79691686, 0.85020924, 0.87857774, 0.79118774],\n",
       "       [0.79118774, 0.78793774, 0.79190751, 0.14544163, 0.68143137,\n",
       "        0.79568031, 0.84941013, 0.87825469, 0.78735632],\n",
       "       [0.78735632, 0.79182879, 0.76300578, 0.23328225, 0.61885336,\n",
       "        0.79419645, 0.84833764, 0.87782396, 0.76245211],\n",
       "       [0.76245211, 0.75680934, 0.74566474, 0.26950249, 0.63586454,\n",
       "        0.79250649, 0.84694972, 0.87728555, 0.74329502],\n",
       "       [0.74329502, 0.75291829, 0.74759152, 0.17295257, 0.73269675,\n",
       "        0.7911875 , 0.84564591, 0.87682252, 0.75670498],\n",
       "       [0.75670498, 0.76070039, 0.75529865, 0.16708407, 0.71558934,\n",
       "        0.79036313, 0.84440519, 0.87640256, 0.76436782],\n",
       "       [0.76436782, 0.76070039, 0.75722543, 0.17867155, 0.69280719,\n",
       "        0.78945633, 0.84314344, 0.87591799, 0.76436782],\n",
       "       [0.76436782, 0.76653696, 0.76685934, 0.15766456, 0.69850103,\n",
       "        0.78879683, 0.84194478, 0.8755088 , 0.76628352],\n",
       "       [0.76628352, 0.76264591, 0.75722543, 0.15680484, 0.68142036,\n",
       "        0.78817856, 0.84074611, 0.87504576, 0.76245211],\n",
       "       [0.76245211, 0.75875486, 0.75337187, 0.23126378, 0.67003013,\n",
       "        0.78723053, 0.83940025, 0.87450735, 0.75478927],\n",
       "       [0.75478927, 0.75291829, 0.74373796, 0.23421672, 0.67002334,\n",
       "        0.78628251, 0.83788615, 0.8739151 , 0.74712644],\n",
       "       [0.74712644, 0.75486381, 0.74759152, 0.20154749, 0.73269081,\n",
       "        0.78582911, 0.83639308, 0.87338746, 0.7605364 ],\n",
       "       [0.7605364 , 0.76070039, 0.76493256, 0.15355287, 0.69280719,\n",
       "        0.78541692, 0.83496309, 0.87284905, 0.7605364 ],\n",
       "       [0.7605364 , 0.76264591, 0.761079  , 0.16263597, 0.70419657,\n",
       "        0.78566423, 0.83355414, 0.87233218, 0.76436782],\n",
       "       [0.76436782, 0.76264591, 0.76685934, 0.16446754, 0.70419487,\n",
       "        0.78578789, 0.83214519, 0.8718153 , 0.76819923],\n",
       "       [0.76819923, 0.76848249, 0.77071291, 0.18999738, 0.70419317,\n",
       "        0.78582911, 0.83050491, 0.87129843, 0.77203065],\n",
       "       [0.77203065, 0.77042802, 0.76493256, 0.16297238, 0.66434647,\n",
       "        0.78566423, 0.82875949, 0.87073848, 0.76245211],\n",
       "       [0.76245211, 0.75875486, 0.73795761, 0.23802938, 0.61308749,\n",
       "        0.78467499, 0.8266776 , 0.87004932, 0.73563218]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.95402299, 0.95785441, 0.95785441, ..., 0.91187739, 0.90996169,\n",
       "       0.91762452])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.95628846],\n",
       "       [0.9523339 ],\n",
       "       [0.94992006],\n",
       "       ...,\n",
       "       [0.9034717 ],\n",
       "       [0.904866  ],\n",
       "       [0.90557444]], dtype=float32)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5992"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This MinMaxScaler instance is not fitted yet. Call 'fit' with appropriate arguments before using this method.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-6dd00522ee6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmin_max_scaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin_max_scaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36minverse_transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    379\u001b[0m             \u001b[0mInput\u001b[0m \u001b[0mdata\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mtransformed\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mIt\u001b[0m \u001b[0mcannot\u001b[0m \u001b[0mbe\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \"\"\"\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'scale_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m    766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall_or_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    769\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFittedError\u001b[0m: This MinMaxScaler instance is not fitted yet. Call 'fit' with appropriate arguments before using this method."
     ]
    }
   ],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "new = min_max_scaler.inverse_transform(p)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sheet1',\n",
       " 'GBP Curncy',\n",
       " 'JPY Curncy',\n",
       " 'USD Curncy',\n",
       " 'EUR Curncy',\n",
       " 'CAD Curncy',\n",
       " 'NZD Curncy',\n",
       " 'SEK Curncy',\n",
       " 'AUD Curncy',\n",
       " 'CHF Curncy',\n",
       " 'NOK Curncy',\n",
       " 'ZAR Curncy']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xl.sheet_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load spreadsheet\n",
    "close = pd.ExcelFile('close.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pandas.io.excel.ExcelFile at 0x1a31953e80>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "close = pd.ExcelFile('close.xlsx')\n",
    "dq=np.array(close.parse(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.3462, 111.72  ,   1.1847, ...,   0.7935,   0.9669,   7.8175],\n",
       "       [  1.3465, 111.72  ,   1.1849, ...,   0.7937,   0.967 ,   7.8172],\n",
       "       [  1.3466, 111.72  ,   1.1848, ...,   0.7937,   0.9669,   7.8169],\n",
       "       ...,\n",
       "       [  1.3522, 112.63  ,   1.1993, ...,   0.7811,   0.9746,   8.1754],\n",
       "       [  1.3525, 112.62  ,   1.1993, ...,   0.7812,   0.9745,   8.1774],\n",
       "       [  1.3522, 112.61  ,   1.1996, ...,   0.7811,   0.9743,   8.1868]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from   mosek.fusion import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.006153636871897642, array([0.05734983, 0.        , 0.        ]))"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n      = 3;\n",
    "w      = 0.06740718306100879;   \n",
    "mu     = [0.1073,0.0737,0.0627]\n",
    "x0     = [0.0,0.0,0.0]\n",
    "gammas = [0.035,0.040,0.050,0.060,0.070,0.080,0.090]\n",
    "GT     = [\n",
    "        [ 0.1566673333200005, 0.0232190712557243 ,  0.0012599496030238 ],\n",
    "        [ 0.0              , 0.102863378954911  , -0.00222873156550421],\n",
    "        [ 0.0              , 0.0                ,  0.0338148677744977 ]]\n",
    "f = n*[0.01]\n",
    "g = n*[0.001]\n",
    "MarkowitzWithTransactionsCost(n,mu,GT,x0,w,gammas[0],f,g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
